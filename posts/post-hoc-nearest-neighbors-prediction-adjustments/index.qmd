---
title: '_Post Hoc_ Nearest Neighbors Prediction Adjustments'
author: 'Max Kuhn'
date: '2024-04-07'
categories:
  - post-processing
  - nearest neighbors
  - regression
---

<hr>
 
Quinlan (1993) describes a post-processing technique used for numeric predictions that takes the model's predictions and adjusts it using information from the training set. 

Let's say you have some model with outcome $y$ and a vector of predictors $\boldsymbol{x}$. We've fit some model to the training set and we have a new observation with predictors $\boldsymbol{x}_0$; the model's prediction is $\widehat{y}_0$.

This method finds the $K$-nearest neighbors to $\boldsymbol{x}_0$ from the training set (denotes as $\boldsymbol{x}_1\ldots \boldsymbol{x}_K$) and their corresponding predictions $\widehat{y}_i$. The distances from the new sample to the training set points are $d_i$.

For the new data point, the $K$ adjusted predictions are: 

$$
\widehat{a}_i = y_i + (\widehat{y}_0 - \widehat{y}_i)
$$

for $i=1\ldots K$.  The final prediction is the weighted average the $\widehat{a}_i$ where the weights are $w_i = 1 / (d_i + \epsilon)$. $\epsilon$ is there to prevent division by zero and Quinlan defaults this to 0.5. 

This adjustment is an intregal part of the Cubist rule-based model ensemble that we talk about in _APM_. 

To do this in general, I've started a small R package called [adjusted])https://topepo.github.io/adjusted/). It works off if tidymodels workflow objects and uses Gower distance for calculations. 


Here's an example using MARS:

```{r}
#| label: startup
library(tidymodels)
library(adjusted)
# Also requires the earth package
tidymodels_prefer()
theme_set(theme_bw())
```


```{r}
#| label: data

data(deliveries, package = "modeldata")

set.seed(991)
delivery_split <- initial_validation_split(deliveries, prop = c(0.6, 0.2), strata = time_to_delivery)
delivery_train <- training(delivery_split)
delivery_test  <- testing(delivery_split)
delivery_val   <- validation(delivery_split)
```


```{r}
#| label: fit
mars_spec <-  
  mars(prod_degree = 2) %>% 
  set_mode("regression")

mars_fit <- 
  workflow() %>% 
  add_formula(time_to_delivery ~ .) %>% 
  add_model( mars_spec) %>% 
  fit(data = delivery_train)
```


```{r}
#| label: eqn
mars_fit %>%
  extract_fit_engine() %>% 
  format(digits = 3) %>% 
  cat()
```


```{r}
#| label: obj

adj_obj <- nn_adjust(mars_fit, delivery_train)
```


```{r}
#| label: preds

val_pred <- 
  tibble(neighbors = 0:20) %>% 
  mutate(
    .pred = map(neighbors, 
                ~ predict(adj_obj, new_data = delivery_val, neighbors = .x) %>% bind_cols(delivery_val)),
    rmse = map_dbl(.pred, ~ rmse_vec(.x$time_to_delivery, .x$.pred))
  )
```


```{r}
#| label: rmse
val_pred %>% 
  ggplot(aes(neighbors, rmse)) +
  geom_point() + 
  geom_line()
```

show bl res and compare

```{r}
#| label: res
val_pred_0 <- val_pred %>% filter(neighbors == 0) %>% pluck("rmse")

val_pred %>% 
  mutate(pct_imp = (val_pred_0 - rmse) / val_pred_0) %>% 
  ggplot(aes(neighbors, pct_imp)) +
  geom_point() + 
  geom_line() + 
  scale_y_continuous(labels = scales::percent)
```


```{r}
predict(adj_obj, new_data = delivery_test, neighbors = 0) %>% 
  bind_cols(delivery_test) %>% 
  rmse(time_to_delivery, .pred)

predict(adj_obj, new_data = delivery_test, neighbors = 1) %>% 
  bind_cols(delivery_test) %>% 
  rmse(time_to_delivery, .pred)
```
