---
title: '_Post Hoc_ Nearest Neighbors Prediction Adjustments'
author: 'Max Kuhn'
date: '2024-04-10'
categories:
  - post-processing
  - nearest neighbors
  - regression
---

<hr>
 
[Quinlan (1993)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Combining+instance-based+and+model-based+learning&btnG=) describes a post-processing technique used for numeric predictions that adjusts them using information from the training set. 

Let's say you have some model with a numeric outcome $y$ and a vector of predictors $\boldsymbol{x}$. We've fit some model to the training set and we have a new observation with predictors $\boldsymbol{x}_0$; the model's prediction is $\widehat{y}_0$.

This method finds the $K$-nearest neighbors to $\boldsymbol{x}_0$ from the training set (denotes as $\boldsymbol{x}_1\ldots \boldsymbol{x}_K$) and their corresponding predictions $\widehat{y}_i$. The distances from the new sample to the training set points are $d_i$.

For the new data point, the $K$ adjusted predictions are: 

$$
\widehat{a}_i = y_i + (\widehat{y}_i - \widehat{y}_0)
$$

for $i=1\ldots K$.  The final prediction is the weighted average the $\widehat{a}_i$ where the weights are $w_i = 1 / (d_i + \epsilon)$. $\epsilon$ is there to prevent division by zero and Quinlan defaults this to 0.5. 

This adjustment is an integral part of the Cubist rule-based model ensemble that we talk about in [_APM_](http://appliedpredictivemodeling.com/). 

To do this in general, I've started a small R package called [adjusted](https://topepo.github.io/adjusted/). It requires a fitted tidymodels workflow object and uses [Gower distance](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Some+distance+properties+of+latent+root+and+vector+methods+used+in+multivariate+analysis&btnG=) for calculations. 


Here's an example using MARS on the [food delivery data](https://aml4td.org/chapters/whole-game.html#sec-delivery-times):

```{r}
#| label: startup
library(tidymodels)
# Found at https://github.com/topepo/adjusted
library(adjusted)
# Also requires the earth package

tidymodels_prefer()
theme_set(theme_bw())
```

The data are in the modeldata package. We'll do the same split as the book (training/validation/testing): 

```{r}
#| label: data

data(deliveries, package = "modeldata")

set.seed(991)
delivery_split <- initial_validation_split(deliveries, prop = c(0.6, 0.2),
                                           strata = time_to_delivery)
delivery_train <- training(delivery_split)
delivery_test  <- testing(delivery_split)
delivery_val   <- validation(delivery_split)
```

Let's specify a [multivariate adaptive regression spline](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22multivariate+adaptive+regression+spline%22+Friedman&btnG=) model that can utilize two-factor interaction terms. The initial code is pretty simple: 

```{r}
#| label: fit
#| results: hide
mars_spec <-  
  mars(prod_degree = 2) %>% 
  set_mode("regression")

mars_fit <- 
  workflow() %>% 
  add_formula(time_to_delivery ~ .) %>% 
  add_model( mars_spec) %>% 
  fit(data = delivery_train)
```

We can use the earth package's `format()` function to see the model terms/splits. It's fairly long though: 

<details>

```{r}
#| label: eqn
mars_fit %>%
  extract_fit_engine() %>% 
  format(digits = 3) %>% 
  cat()
```

</details>

**Warning** this syntax may change...

The adjusted package requires the use of the fitted model as well as the initial training set. The main function `nn_adjust()` has those two arguments: 

```{r}
#| label: obj

adj_obj <- nn_adjust(mars_fit, training = delivery_train)
```

and pretty much does _nothing_ at this point. We don't even need to specifiy the number of neighbors until prediction/adjustment time: 

```{r}
#| label: example

# Predict the first validation row: 
x_0 <- delivery_val %>% slice(1)

# Original prediction:
predict(mars_fit, x_0)

# Same:
predict(adj_obj, x_0, neighbors = 0)

# Adjust using 3 most similar samples
predict(adj_obj, x_0, neighbors = 3)
```

So how many neighbors should we use? Let's try different values and compute the RMSE for the validation set: 

```{r}
#| label: preds

val_pred <- 
  tibble(neighbors = 0:20) %>% 
  mutate(
    .pred = map(neighbors, 
                ~ predict(adj_obj, new_data = delivery_val, neighbors = .x) %>%
                  bind_cols(delivery_val)),
    rmse = map_dbl(.pred, ~ rmse_vec(.x$time_to_delivery, .x$.pred))
  )
```

The RMSE profile looks fairly common (based on our experiences with Cubist). The 1-NN model is _awful_ since it is over-fitting to a single data point. As we increase the number of neighbors the RMSE drops and eventually surpasses the results without any adjustment: 

```{r}
#| label: rmse
val_pred %>% 
  ggplot(aes(neighbors, rmse)) +
  geom_point() + 
  geom_line() +
  labs(y = "RMSE (validation)")
```

Let's compute the percent improvement relative to the no-adjustment case: 

```{r}
#| label: res
val_pred_0 <- val_pred %>% filter(neighbors == 0) %>% pluck("rmse")

val_pred %>% 
  mutate(pct_imp = (val_pred_0 - rmse) / val_pred_0) %>% 
  ggplot(aes(neighbors, pct_imp)) +
  geom_point() + 
  geom_line() + 
  scale_y_continuous(labels = scales::percent) +
  labs(y = "Percent Imprvement (validation)")
```

I made this since the tidymodels group is finally focusing on _post-processing_: things that we can do to the model predictions to make them better. Another example is model calibration methods. 

Our goal is to let you add post-processing steps to the workflow and tune/optimize these in the same way as pre-processing parameters or model hyperparameters. 
