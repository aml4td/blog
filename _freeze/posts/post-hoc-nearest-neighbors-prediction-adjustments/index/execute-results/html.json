{
  "hash": "180b4ca2ff1abbc77ecdf831cce9a066",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: '_Post Hoc_ Nearest Neighbors Prediction Adjustments'\nauthor: 'Max Kuhn'\ndate: '2024-04-10'\ncategories:\n  - post-processing\n  - nearest neighbors\n  - regression\n  - tidymodels\n---\n\n\n<hr>\n \n[Quinlan (1993)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Combining+instance-based+and+model-based+learning&btnG=) describes a post-processing technique used for numeric predictions that adjusts them using information from the training set. \n\nLet's say you have some model with a numeric outcome $y$ and a vector of predictors $\\boldsymbol{x}$. We've fit some model to the training set and we have a new observation with predictors $\\boldsymbol{x}_0$; the model's prediction is $\\widehat{y}_0$.\n\nThis method finds the $K$-nearest neighbors to $\\boldsymbol{x}_0$ from the training set (denotes as $\\boldsymbol{x}_1\\ldots \\boldsymbol{x}_K$) and their corresponding predictions $\\widehat{y}_i$. The distances from the new sample to the training set points are $d_i$.\n\nFor the new data point, the $K$ adjusted predictions are: \n\n$$\n\\widehat{a}_i = y_i + (\\widehat{y}_0 - \\widehat{y}_i)\n$$\n\nfor $i=1\\ldots K$.  The final prediction is the weighted average the $\\widehat{a}_i$ where the weights are $w_i = 1 / (d_i + \\epsilon)$. $\\epsilon$ is there to prevent division by zero and Quinlan defaults this to 0.5. \n\nSuppose the true value of the closest neighbor is 10 and its prediction is 11. If our new value $\\boldsymbol{x}_0$ is over-predicted with a value of 15, we end up adjusting the prediction down to 14 (i.e., 10 + (15 - 11)).\n\nThis adjustment is an integral part of the Cubist rule-based model ensemble that we discuss in [_APM_](http://appliedpredictivemodeling.com/) (and will later in this book).  We'd like to apply it to any regression model. \n\nTo do this in general, I've started a small R package called [adjusted](https://topepo.github.io/adjusted/). It requires a fitted tidymodels workflow object and uses [Gower distance](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Some+distance+properties+of+latent+root+and+vector+methods+used+in+multivariate+analysis&btnG=) for calculations. \n\n\nHere's an example using MARS on the [food delivery data](https://aml4td.org/chapters/whole-game.html#sec-delivery-times):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.5      ✔ recipes      1.0.10\n✔ dials        1.2.1      ✔ rsample      1.2.1 \n✔ dplyr        1.1.4      ✔ tibble       3.2.1 \n✔ ggplot2      3.5.0      ✔ tidyr        1.3.1 \n✔ infer        1.0.6      ✔ tune         1.2.0 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n✔ purrr        1.0.2      ✔ yardstick    1.3.1 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n```\n\n\n:::\n\n```{.r .cell-code}\n# Found at https://github.com/topepo/adjusted\nlibrary(adjusted)\n# Also requires the earth package\n\ntidymodels_prefer()\ntheme_set(theme_bw())\n```\n:::\n\n\nThe data are in the modeldata package. We'll do the same split as the book (training/validation/testing): \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(deliveries, package = \"modeldata\")\n\nset.seed(991)\ndelivery_split <- initial_validation_split(deliveries, prop = c(0.6, 0.2),\n                                           strata = time_to_delivery)\ndelivery_train <- training(delivery_split)\ndelivery_test  <- testing(delivery_split)\ndelivery_val   <- validation(delivery_split)\n```\n:::\n\n\nLet's specify a [multivariate adaptive regression spline](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22multivariate+adaptive+regression+spline%22+Friedman&btnG=) model that can utilize two-factor interaction terms. The initial code is pretty simple: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmars_spec <-  \n  mars(prod_degree = 2) %>% \n  set_mode(\"regression\")\n\nmars_fit <- \n  workflow() %>% \n  add_formula(time_to_delivery ~ .) %>% \n  add_model( mars_spec) %>% \n  fit(data = delivery_train)\n```\n:::\n\n\nWe can use the earth package's `format()` function to see the model terms/splits. It's fairly long though: \n\n<details>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmars_fit %>%\n  extract_fit_engine() %>% \n  format(digits = 3) %>% \n  cat()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  53\n  -  0.456 * dayTue\n  +   3.48 * dayWed\n  +   5.97 * dayThu\n  +   10.6 * dayFri\n  +   15.1 * daySat\n  +   3.68 * daySun\n  -   1.13 * h(hour-16.233)\n  -   2.35 * h(17.319-hour)\n  +  0.466 * h(hour-17.319)\n  -   2.57 * h(3.84-distance)\n  +   3.99 * h(distance-3.84)\n  -  0.563 * h(2-item_02)\n  +   4.22 * h(item_02-2)\n  -  0.554 * h(1-item_03)\n  +  0.851 * h(item_03-1)\n  -   11.6 * h(2-item_10)\n  +   10.4 * h(item_10-2)\n  -  0.807 * h(1-item_24)\n  +  0.581 * h(item_24-1)\n  +  0.869 * h(17.319-hour)*dayTue\n  -  0.656 * h(17.319-hour)*dayThu\n  -   1.57 * h(18.023-hour)*dayFri\n  -   1.39 * h(hour-18.023)*dayFri\n  -    1.7 * h(18.629-hour)*daySat\n  -   2.86 * h(hour-18.629)*daySat\n  +   1.16 * h(distance-3.84)*dayFri\n  -  0.561 * h(5.02-distance)*daySat\n  +    2.1 * h(distance-5.02)*daySat\n  -   11.3 * h(1-item_01)*item_10\n  +   9.95 * h(item_01-1)*item_10\n  -   1.43 * h(1-item_10)*daySat\n  -   2.55 * h(item_10-1)*daySat\n  -  0.578 * h(17.319-hour)*h(distance-3.54)\n  +  0.324 * h(17.319-hour)*h(3.54-distance)\n  +   1.05 * h(1-item_01)*h(item_09-1)\n  -  0.546 * h(1-item_01)*h(1-item_09)\n```\n\n\n:::\n:::\n\n\n</details>\n\n\n::: {.callout-warning}\nI just made the package; it would probably be called “experimental.” The syntax below may change in the future. \n:::\n\n\nThe adjusted package requires the use of the fitted model as well as the initial training set. The main function `nn_adjust()` has those two arguments: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nadj_obj <- nn_adjust(mars_fit, training = delivery_train)\n```\n:::\n\n\nand pretty much does _nothing_ at this point. We don't even need to specifiy the number of neighbors until prediction/adjustment time: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict the first validation row: \nx_0 <- delivery_val %>% slice(1)\n\n# Original prediction:\npredict(mars_fit, x_0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred\n  <dbl>\n1  29.1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Same:\npredict(adj_obj, x_0, neighbors = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred\n  <dbl>\n1  29.1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Adjust using 3 most similar samples\npredict(adj_obj, x_0, neighbors = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred\n  <dbl>\n1  29.5\n```\n\n\n:::\n:::\n\n\nSo how many neighbors should we use? Let's try different values and compute the RMSE for the validation set: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nval_pred <- \n  tibble(neighbors = 0:20) %>% \n  mutate(\n    .pred = map(neighbors, \n                ~ augment(adj_obj, new_data = delivery_val, neighbors = .x)),\n    rmse = map_dbl(.pred, ~ rmse_vec(.x$time_to_delivery, .x$.pred))\n  )\n```\n:::\n\n\nThe RMSE profile looks fairly common (based on our experiences with Cubist). The 1-NN model is _awful_ since it is over-fitting to a single data point. As we increase the number of neighbors the RMSE drops and eventually surpasses the results without any adjustment: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nval_pred %>% \n  ggplot(aes(neighbors, rmse)) +\n  geom_point() + \n  geom_line() +\n  labs(y = \"RMSE (validation)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rmse-1.png){width=576}\n:::\n:::\n\n\nLet's compute the percent improvement relative to the no-adjustment case: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nval_pred_0 <- val_pred %>% filter(neighbors == 0) %>% pluck(\"rmse\")\n\nval_pred %>% \n  mutate(pct_imp = (val_pred_0 - rmse) / val_pred_0) %>% \n  ggplot(aes(neighbors, pct_imp)) +\n  geom_point() + \n  geom_line() + \n  scale_y_continuous(labels = scales::percent) +\n  labs(y = \"Percent Imprvement (validation)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/res-1.png){width=576}\n:::\n:::\n\n\nFor these data, the best case is a 1.9% improvement in the RMSE. That's not a game changer, but it is certainly helpful if every little bit of performance is important.  \n\nI made this package because the tidymodels group is finally focusing on _post-processing_: things that we can do to the model predictions to make them better. Another example is model calibration methods. \n\nOur goal is to let you add post-processing steps to the workflow and tune/optimize these in the same way as pre-processing parameters or model hyperparameters. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}