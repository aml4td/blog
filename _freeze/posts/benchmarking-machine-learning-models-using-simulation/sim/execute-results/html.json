{
  "hash": "b23b8d7bb19c5a41ed28c943b3a28dd8",
  "result": {
    "engine": "knitr",
    "markdown": "::: {.cell}\n\n:::\n\n\n\"If you torture the data long enough, it will confess\" - source unknown\n\nWhat is the objective of most data analysis? One way I think about it is that we are trying to discover or approximate what is really going on in our data (and in general, nature). However, I occasionally run into people think that if one model fulfills our expectations (e.g. higher number of significant p-values or accuracy) than it must be better than any other model that does not. For most data sets, we don't know what the truth is, so this attitude can be problematic. \n\nComputational biology/bioinformatics are particularly bad in this way. In many cases, the cost, time and complexity of high dimensional biology experiments means that there will be a solid, methodical validation of analysis of the initial data set. This is has be verified by at least one publication. \n\nI was talking to someone recently who was describing their research with ~150 samples and ~50,000 predictors. They used the same sample set to do feature selection and then build predictive models. The results was a random forest model based on about 150 predictors. The validation was based on running some of the same samples using a different technology. When I asked if there there would be any external validation, their response was \"we're never going to get a 5,000 sample clinical trial to check the results.\" While true (and a bit dramatic), it is not an excuse to throw out good methodology.  In fact, you would think that a lack of a clear path to validation would make people be more dogmatic about methodology, but that's not the case sometimes. \n\nSo when I'm trying to evaluate any sort of statistic method, I almost always find a good simulation system so that I can produce results where I know the truth. A good example of this are the \"Friedman\" simulations systems for regression modeling. For example, the '[Friedman 3](http://scholar.google.com/scholar?hl=en&q=Multivariate+adaptive+regression+splines&btnG=&as_sdt=1%2C7&as_sdtp=)' simulation system is a non-linear function of four predictors:\n\n<pre>\ny = atan ((x2 x3 - (1/(x2 x4)))/x1) + error\n</pre> \n\nThe the [mlbench](http://cran.r-project.org/web/packages/mlbench/index.html) package has this in R code as well as other simulation systems.\n\nI've been looking for a system that can be used to test a few different aspects of classification models:\n\n* class imbalances\n* non-informative predictors\n* correlation amoung the predictors\n* linear and nonlinear signals\n\nI spent a few hours developing up with one. It models the log-odds of a binary event as a function of informative and non-informative predictors. The true signals are additive \"sets\" of a few different types. First, there are two main effects and an interaction:\n\n<pre>\nintercept - 4A + 4B + 2AB \n</pre>\n\nThe intercept is a parameter for the simulation and can be used to control the amount of class imbalance. The second set of effects are linear with coefficients that alternate signs and have values between 2.5 and 0.025. For example, if there were six predictors in this set, the contribution to the log-odds would be \n\n<pre>\n-2.50C + 2.05D -1.60E + 1.15F -0.70G + 0.25H\n</pre>\n\nThe third set is a nonlinear function of a single predictor ranging between [0, 1] called J here:\n<pre>\n(J^3) + 2exp(-6(J-0.3)^2) \n</pre>\nI saw this in one of [Radford Neal](http://www.cs.utoronto.ca/~radford/)'s presentations but I couldn't find an exact reference for it. The equation produces an interesting trend:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](sim_files/figure-html/nonlin-1.png){width=576}\n:::\n:::\n\n\nThe fourth set of informative predictors are copied from one of Friedman's systems and use a set of two (`K` and `L`):\n<pre>\n2sin(KL)\n</pre>\n\nAll of these effects are added up to model the log-odds. This is used to calculate the probability of a sample being in the first class and a random uniform number is used to actually make the assignment of the actual class. \n\nWe can also add non-informative predictors to the data. These are random standard normal predictors and can be optionally added to the data in two ways: a specified number of independent predictors or a set number of predictors that follow a particular correlation structure. The only two correlation structure that I've implemented are \n\n* compound-symmetry (aka exchangeable) where there is a constant correlation between all the predictors\n\n* auto-regressive 1 [AR(1)]. While there is no time component to these data, we can use this structure to add predictors of varying levels of correlation. For example, simulating ten predictors with a correlation parameter of 0.75 yields the following between-predictor correlation structure:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sim_files/figure-html/corrplot-1.png){width=480}\n:::\n:::\n\n\nTo demonstrate, let's take a set of data and see how a support vector machine performs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(468)\ntraining <- twoClassSim(  300, noiseVars = 100, corrVar = 100, corrValue = .75)\ntesting  <- twoClassSim(  300, noiseVars = 100, corrVar = 100, corrValue = .75)\nlarge    <- twoClassSim(10000, noiseVars = 100, corrVar = 100, corrValue = .75)\n```\n:::\n\n\nThe default for the number of informative linear predictors is 10 and the default intercept of -5 makes the class frequencies fairly balanced:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(large$Class)/nrow(large)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClass1 Class2 \n0.5457 0.4543 \n```\n\n\n:::\n:::\n\n\nWe'll use the <pre>train</pre> function to tune and train the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n\nctrl <- trainControl(method = \"repeatedcv\",\n                     repeats = 3,\n                     classProbs = TRUE,\n                     summaryFunction = twoClassSummary)\n\nset.seed(1254)\nfullModel <- train(Class ~ .,\n                   data = training,\n                   method = \"svmRadial\",\n                   preProc = c(\"center\", \"scale\"),\n                   tuneLength = 8,\n                   metric = \"ROC\",\n                   trControl = ctrl)\nprint(fullModel, digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSupport Vector Machines with Radial Basis Function Kernel \n\n300 samples\n215 predictors\n  2 classes: 'Class1', 'Class2' \n\nPre-processing: centered (215), scaled (215) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 270, 270, 270, 270, 270, 270, ... \nResampling results across tuning parameters:\n\n  C      ROC    Sens   Spec \n   0.25  0.626  0.646  0.498\n   0.50  0.626  0.654  0.510\n   1.00  0.622  0.681  0.464\n   2.00  0.667  0.708  0.524\n   4.00  0.703  0.717  0.555\n   8.00  0.708  0.731  0.555\n  16.00  0.708  0.715  0.571\n  32.00  0.708  0.733  0.562\n\nTuning parameter 'sigma' was held constant at a value of 0.002389\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.00239 and C = 8.\n```\n\n\n:::\n:::\n\n\nCross-validation estimates the best area under the ROC curve to be 0.708. Is this an accurate estimate? The test set has:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfullTest <- roc(testing$Class, \n                predict(fullModel, testing, type = \"prob\")[,1], \n                levels = rev(levels(testing$Class)))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nfullTest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nroc.default(response = testing$Class, predictor = predict(fullModel,     testing, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n\nData: predict(fullModel, testing, type = \"prob\")[, 1] in 140 controls (testing$Class Class2) < 160 cases (testing$Class Class1).\nArea under the curve: 0.795\n```\n\n\n:::\n:::\n\n\nFor this small test set, the estimate is 0.086 larger than the resampled version. How do both of these compare to our approximation of the truth?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfullLarge <- roc(large$Class, \n                 predict(fullModel, large, type = \"prob\")[,1], \n                 levels = rev(levels(testing$Class)))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nfullLarge\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nroc.default(response = large$Class, predictor = predict(fullModel,     large, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n\nData: predict(fullModel, large, type = \"prob\")[, 1] in 4543 controls (large$Class Class2) < 5457 cases (large$Class Class1).\nArea under the curve: 0.747\n```\n\n\n:::\n:::\n\n\nHow much did the presence of the non-informative predictors affect this model? We know the true model, so we can fit that and evaluate it in the same way:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrealVars <- names(training)\nrealVars <- realVars[!grepl(\"(Corr)|(Noise)\", realVars)]\n\nset.seed(1254)\ntrueModel <- train(Class ~ .,\n                   data = training[, realVars],\n                   method = \"svmRadial\",\n                   preProc = c(\"center\", \"scale\"),\n                   tuneLength = 8,\n                   metric = \"ROC\",\n                   trControl = ctrl)\nprint(trueModel, digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSupport Vector Machines with Radial Basis Function Kernel \n\n300 samples\n 15 predictor\n  2 classes: 'Class1', 'Class2' \n\nPre-processing: centered (15), scaled (15) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 270, 270, 270, 270, 270, 270, ... \nResampling results across tuning parameters:\n\n  C      ROC    Sens   Spec \n   0.25  0.892  0.779  0.826\n   0.50  0.920  0.856  0.819\n   1.00  0.934  0.873  0.843\n   2.00  0.939  0.869  0.860\n   4.00  0.942  0.896  0.864\n   8.00  0.934  0.869  0.857\n  16.00  0.925  0.848  0.845\n  32.00  0.916  0.835  0.805\n\nTuning parameter 'sigma' was held constant at a value of 0.0391\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.0391 and C = 4.\n```\n\n\n:::\n:::\n\n\nMuch higher! Is this verified by the other estimates?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrueTest <- roc(testing$Class, \n                predict(trueModel, testing, type = \"prob\")[,1], \n                levels = rev(levels(testing$Class)))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\ntrueTest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nroc.default(response = testing$Class, predictor = predict(trueModel,     testing, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n\nData: predict(trueModel, testing, type = \"prob\")[, 1] in 140 controls (testing$Class Class2) < 160 cases (testing$Class Class1).\nArea under the curve: 0.929\n```\n\n\n:::\n\n```{.r .cell-code}\ntrueLarge <- roc(large$Class, \n                 predict(trueModel, large, type = \"prob\")[,1], \n                 levels = rev(levels(testing$Class)))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\ntrueLarge\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nroc.default(response = large$Class, predictor = predict(trueModel,     large, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n\nData: predict(trueModel, large, type = \"prob\")[, 1] in 4543 controls (large$Class Class2) < 5457 cases (large$Class Class1).\nArea under the curve: 0.929\n```\n\n\n:::\n:::\n\n\nAt this point, we might want to look and see what would happen if all 200 non-informative predictors were uncorrelated etc. At least we have a testing tool to make objective statements. \n\nCode to create this can be found here and will end up making its way into the [caret](http://cran.r-project.org/web/packages/caret/index.html) package.\n\nAny suggestions for simulation systems? \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}