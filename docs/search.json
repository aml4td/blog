[
  {
    "objectID": "posts/wtf-article/index.html",
    "href": "posts/wtf-article/index.html",
    "title": "WTF Article",
    "section": "",
    "text": "Kjell and I have a new paper called “What They Forgot to Tell You about Machine Learning with an Application to Pharmaceutical Manufacturing.”\nIt is in the Pharmaceutical Statistics journal, but you can read the preprint on the GitHub page.\nI co-opted the “WTF” aspect of it from a workshop that our friends called “What They Forgot to Teach You About R”. I thought a reviewer or two might balk, but all we got was a comment about the novel use of that acronym.\nThe article was written while we were gearing up for the aml4td book, and you can get a sampling of many of the points we’ll be making there. We’ll also use a similar data set in a few chapters."
  },
  {
    "objectID": "posts/while-you-wait-for-that-to-finish-can-i-interest-you-in-parallel-processing/index.html",
    "href": "posts/while-you-wait-for-that-to-finish-can-i-interest-you-in-parallel-processing/index.html",
    "title": "While you wait for that to finish, can I interest you in parallel processing?",
    "section": "",
    "text": "caret has been able to utilize parallel processing for some time (before it was on CRAN in October 2007) using slightly different versions of the package. Around September of 2011, caret started using the foreach package was used to “harmonize” the parallel processing technologies thanks to a super smart guy named Steve Weston.\nI’ve done a few benchmarks to quantify the benefits to parallel processing over the years. I just did another set since I just got a new iMacPro. This time, I put the code and results in a GitHub repo. The main questions that I wanted to answer were:\n\nIs the benefit the same across computers and operating systems?\nIs there any performance difference between parallel processing technologies (or packages)?\nFor my test cases, is it better to parallelize resampling loops or the individual model fit?\n\nThe task I used was to tune a boosted classification tree model (via the xgboost) package over 25 random values of the seven tuning parameters. The data were simulated and contained 2,000 training set points and 115 numeric predictors. Five repeats of 10-fold cross-validation was used so that 1,250 separate models could be run in parallel. An example of the code is here.\nI tested this using three machines:\n\nA 2015 MacBookPro (version 11,4) with a 2.5 GHz Intel Core i7 processor. It has 4 physical cores.\nA DIY machine built in January of 2016 using a 3.4 GHz Intel Core i7 processor (6 cores).\nA 2017 iMacPro (1,1) with a 3 GHz Intel Xeon W processor. There are 10 cores.\n\nAll the computers have &gt;=8GB of memory. The DIY machine was running Ubuntu 16.04 as well as Windows 10. All of the builds used R 3.4.2 or higher.\nThe methods for parallelization were:\n\nforking uses the fork system call to create new instances of the session (i.e. workers) that run parts of the computations. This is available on MacOS, unix, and linux. It is available to foreach via the doMC and doParallel packages. Historically, I have used doMC for my work but both were tested here.\nsockets via the doParallel package. I don’t know a whole lot about socket connections but they have been around a long time via the snow package and can run on all major operating systems.\nthreading: xgboost has the option to use openMP to parallelize the individual model fits.\n\nThis JSS paper from 2009 might be helpful.\nOnce nice thing about parallelism in caret is that you do not have to modify the package code. With foreach, you simply “register” a parallel backend and the caret functions automatically utilize the workers.\nTo be clear, the forking/socket approaches split the 1,250 model fits across the workers while the threading affects each individual model fit (one-at-a-time).\nIn each case, different number of workers were applied to the problems and the execution time was measured. Since these are all recent Intel processors, hyper-threading can be used. This means that, even though you have X physical cores, there are 2X “logical cores” that can perform independent operations. For that reason, we can add more workers than physical cores. The order of all runs were randomized and run in clear R sessions.\nFirst, let’s look at the non-threading results. The raw execution times were:\n\n\n\n\n\nWhen converted to speed-ups, we get:\n\n\n\n\n\nThe speed-up is the sequential time divided by the parallel time; a speed-up of 3 means that the parallel execution took a third less time than the sequential code.\nMy observations:\n\nAs expected, the desktops were substantially faster than the laptop. The speeds also rank-ordered with the age of the computer.\nOn the same machine, linux ran faster than windows by slightly less than a fold.\nIt appears that the speed-ups are linear up until we go beyond the number of physical cores. There is incremental benefit to using logical cores but it is small.\nI’ve never seen a case where performance get’s substantially worse as you increase the number of workers. If the model is already fast (e.g. PLS) or your workers exceed system memory, this isn’t an issue.\nThere was no real difference in performance between forking and sockets (at least using doParallel).\nThe new iMacPro does really well. The speed-ups are linear up until about 8 or 9 cores. This means that an execution time of 38 minutes can be cut down to 4 or 5 minutes.\n\nThe doMC results are strange. When the number of workers is a multiple of 5, there is a performance hit. At first I thought this was an aberration until it was apparent that it occurred over two operating systems and three computers. Also, it is specific to doMC; forking using doParallel does not have the same issue. I’ve notified the doMC package maintainer about it.\nWhat about threading? These runs had foreach run sequentially and allowed xgb.train to run in parallel by adjusting the nthread argument. The results:\n\n\n\n\n\nThis isn’t so great and really surprising (or fair). We are starting and stopping threads 1,250 times across the model tuning process. In general, we want to parallelize the longest running part of the algorithm. I have no doubt of the utility of threading the computations for a single model but it can be counterproductive when model is called repeatedly.\nThe code and results can be found here if you are interested in more details.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/what-the-market-will-bear/index.html",
    "href": "posts/what-the-market-will-bear/index.html",
    "title": "What the Market Will Bear",
    "section": "",
    "text": "I’m not sure what the third one is about, but save your money…\n\n\n\n\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/user-2014-highlights/index.html",
    "href": "posts/user-2014-highlights/index.html",
    "title": "useR! 2014 Highlights",
    "section": "",
    "text": "My talk went well; here are the slides and a link to the paper pre-print.\nHadley Wickham gave an excellent tutorial on dplyr.\nBased on the talk I saw, I think I will take the data sets from the book and make some public visualizations on the Plotly website.\nThere were a few presentations on interactive graphics that were very good (here, here and here).\nTal Galili gave an excellent talk on visualizing and analyzing hierarchical clustering algorithms.\nThe HeR session was great. I also learned about R-ladies.\nRevolution Analytics and RStudio proposed two similar techniques for preserving versions for better reproducibility.\nThe subsemble package by Erin LeDell was impressive.\nI had dinner with John Chambers, which was pretty cool too.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/type--what/index.html",
    "href": "posts/type--what/index.html",
    "title": "type = “what”?",
    "section": "",
    "text": "One great thing about R is that has a wide diversity of packages written by many different people of many different viewpoints on how software should be designed. However, this does tend to bite us periodically.  \nWhen I teach newcomers about R and predictive modeling, I have a slide that illustrates one of the weaknesses of this system: heterogeneous interfaces. If you are building a classification model and want to generate class probabilities for new samples, the syntax can be… diverse. Here is a sample of syntax for different models:\n\n\n\n\n\nThat’s a lot of minutia to remember. I did a quick and dirty census of all the classification models used by caret to quantify the variability in this particular syntax. The train utilizes 64 different models that can produce class probabilities. Of these, many were from the same package. For example, both nnet and multinom are in the nnet package and probably should not count twice since the latter is a wrapper for the former. As another example, the RWeka packages has at least six functions that all use probability as the value for type.\nFor this reason, I cooked the numbers down to one value of type per package (using majority vote if there was more than one). There were 40 different packages once these redundancies were eliminated. Here is a histogram of the type values for calculating probabilities:\n The most frequent situation is no type value at all. For example, the lda package automatically generated predicted classes and posterior probabilities without requiring the user to specify anything. There were a handful of cases where the class did not have a predict method to generate class probabilities (e.g. party and pamr) and these also counted as “none”.\nFor those of us that use R to create predictive models on a day-to-day basis, this is a lot of detail to remember (especially if we want to try different models). This is one of the reasons I created caret; it has a unified interface to models that eliminates the need to remember the name of the function, the value of type and any other arguments. In case you are wondering, I chose `type = “prob”’.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/tidymodels-updates-and-voting/index.html",
    "href": "posts/tidymodels-updates-and-voting/index.html",
    "title": "tidymodels updates and voting!",
    "section": "",
    "text": "While I’m still supporting caret, the majority of my development effort has gone into the tidyverse modeling packages (called tidymodels).\nIf you’ve never heard of this, we have just made an excellent learning resources at tidymodels.org. You might consider focusing on the Get Started pages.\nAnother item of note: help us guide development of this ecosystem by voting for new features! Vote here.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/the-basics-of-encoding-categorical-data-for-predictive-models/index.html",
    "href": "posts/the-basics-of-encoding-categorical-data-for-predictive-models/index.html",
    "title": "The Basics of Encoding Categorical Data for Predictive Models",
    "section": "",
    "text": "Thomas Yokota asked a very straight-forward question about encodings for categorical predictors: “Is it bad to feed it non-numerical data such as factors?” As usual, I will try to make my answer as complex as possible.\n(I’ve heard the old wives tale that eskimos have 180 different words in their language for snow. I’m starting to think that statisticians have at least as many ways of saying “it depends”)\nBTW, we cover this in Sections 3.6, 14.1 and 14.7 of the book.\nMy answer: it depends on the model. Some models can work with categorical predictors in their nature, non-numeric encodings. Trees, for example, can usually partition the predictor into distinct groups. For a predictor X with levels a through e, a split might look like\nif X in {a, c, d} the class = 1\nelse class = 2\nRule-based models operate similarly.\nNaive Bayes models are another method that does not need to re-encode the data. In the above example, the frequency distribution of the predictor is computed overall as well as within each of the classes (a good example of this is in Table 13.1 for those of you that are following along).\nHowever, these are the exceptions; most models require the predictors to be in some sort of numeric encoding to be used. For example, linear regression required numbers so that it can assign slopes to each of the predictors.\nThe most common encoding is to make simple dummy variables. The there are C distinct values of the predictor (or levels of the factor in R terminology), a set of C - 1 numeric predictors are created that identify which value that each data point had.\nThese are called dummy variables. Why C - 1 and not C? First, if you know the values of the first C - 1 dummy variables, you know the last one too. It is more economical to use C - 1. Secondly, if the model has slopes and intercepts (e.g. linear regression), the sum of all of the dummy variables wil add up to the intercept (usually encoded as a “1”) and that is bad for the math involved.\nIn R, a simple demonstration for the example above is:\n&gt; pred1 &lt;- factor(letters[1:5])\n&gt; pred1\n\n[1] a b c d e\nLevels: a b c d e\nThe R function model.matrix is a good way to show the encodings:\n&gt; model.matrix(~pred1)\n\n  (Intercept) pred1b pred1c pred1d pred1e\n1           1      0      0      0      0\n2           1      1      0      0      0\n3           1      0      1      0      0\n4           1      0      0      1      0\n5           1      0      0      0      1\nattr(,\"assign\")\n[1] 0 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$pred1\n[1] \"contr.treatment\"\nA column for the factor level a is removed (since it excludes the first level of the factor). This approach goes by the name of “full-rank” encoding since the dummy variables do not always add up to 1.\nWe discuss different encodings for predictors in a few places but fairly extensively in Section 12.1. In that example, we have a predictor that is a date. Do we encode that as the day or the year (1 to 365) and include it as a numeric predictor? We could also add in predictors for the day of the week, the month, the season etc. There are a lot of options. This question of feature engineering is important. You want to find the encoding that captures the important patterns in the data. If there is a seasonal effect, the encoding should capture that information. Exploratory visualizations (perhaps with lattice or ggplot2) can go a long way to figuring out good ways to represent these data.\nSome of these options result in ordered encodings, such as the day of the week. It is possible that the trends in the data are best exposed if the ordering is preserved. R does have a way for dealing with this:\n&gt; pred2 &lt;- ordered(letters[1:5])\n&gt; pred2\n\n[1] a b c d e\nLevels: a &lt; b &lt; c &lt; d &lt; e\nSimple enough, right? Maybe not. If we need a numeric encoding here, what do we do?\nThere are a few options. One simple way is to assign “scores” to each of the levels. We might assign a value of 1 to a and think that b should be twice that and c should be four times that and so on. It is arbitrary but there are whole branches of statistics dedicated to modeling data with (made up) scores. Trend tests are one example.\nIf the data are ordered, one technique is to create a set of new variables similar to dummy variables. However, their values are not 0/1 but are created to reflect the possible trends that can be estimated.\nFor example, if the predictor has two ordered levels, we can’t fit anything more sophisticated to a straight line. However, if there are three ordered levels, we could fit a linear effect as well as a quadratic effect and so on. There are some smart ways to do this (google “orthogonal polynomials” if you are bored).\nFor each ordered factor in a model, R will create a set of polynomial scores for each (we could use the fancy label of “a set of basis functions” here). For example:\n&gt; model.matrix(~pred2)\n\n  (Intercept) pred2.L pred2.Q    pred2.C pred2^4\n1           1 -0.6325  0.5345 -3.162e-01  0.1195\n2           1 -0.3162 -0.2673  6.325e-01 -0.4781\n3           1  0.0000 -0.5345 -4.096e-16  0.7171\n4           1  0.3162 -0.2673 -6.325e-01 -0.4781\n5           1  0.6325  0.5345  3.162e-01  0.1195\nattr(,\"assign\")\n[1] 0 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$pred2\n[1] \"contr.poly\"\nHere, “L” is for linear, “Q” is quadratic and “C” is cubic and so on. There are five levels of this factor and we can create four new encodings. Here is a plot of what those encodings look like:\n\n\n\n\n\nThe nice thing here is that, if the underlying relationship between the ordered factor and the outcome is cubic, we have a feature in the data that can capture that trend.\nOne other way of encoding ordered factors is to treat them as unordered. Again, depending on the model and the data, this could work just as well.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/some-thoughts-on-do-we-need-hundreds-of-classifiers-to-solve-real-world-classification-problems/index.html",
    "href": "posts/some-thoughts-on-do-we-need-hundreds-of-classifiers-to-solve-real-world-classification-problems/index.html",
    "title": "Some Thoughts on “Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?”",
    "section": "",
    "text": "Sorry for the blogging break. I’ve got a few planned for the next few weeks based on some work I’ve been doing.\nIn the meantime, you should check out “Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?” by Manuel Fernandez-Delgado at JMLR. They took a large number of classifiers and ran them against a large number of data sets from UCI. I was a reviewer on this paper (it heavily relies on caret) and have been interested in seeing peoples reaction when it was made public.\nMy thoughts:\n\nBefore reading this manuscript, take a few minutes and read this oldie by David Hand.\nObviously, it pays to tune your model. That is not the most profound statement to make but the paper does a good job of quantifying the impact of tuning the hyper-parameters.\nRandom forest takes the top spot on a regular basis. I was surprised by this since, in my experience, boosting does better and bagging does almost as well.\nThe authors believe that the parallel version of random forest in caret had an edge in performance. That’s hard to believe since it does’t do anything more than split the forest across different cores. That’s it. I took it out of the package for a bit because, if you are tuning the model, parallelizing the cross-validation is faster. I put it back in a few verisons ago since I knew people would want it after reading this manuscript.\nI was hoping that the authors would take a better graphical and analytical approach to comparing the methods. Table after table numbs my soul.\nDespite the number of models used, it would have been nice to see more emphasis on recent deep-learning models and boosting via the gbm package.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/slides-from-rpharma/index.html",
    "href": "posts/slides-from-rpharma/index.html",
    "title": "Slides from R/Pharma",
    "section": "",
    "text": "My slides from the R/Pharma conference on “Modeling in the Tidyverse” are in pdf format as well as the HTML version.\n(Joe Cheng just killed it in his shiny presentation - see this repo)\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/simulated-annealing-feature-selection/index.html",
    "href": "posts/simulated-annealing-feature-selection/index.html",
    "title": "Simulated Annealing Feature Selection",
    "section": "",
    "text": "As previously mentioned, caret has two new feature selection routines based on genetic algorithms (GA) and simulated annealing (SA). The help pages for the two new functions give a detailed account of the options, syntax etc.\nThe package already has functions to conduct feature selection using simple filters as well as recursive feature elimination (RFE). RFE can be very effective. It initially ranks the features, then removes them in sequence, starting with the least importance variables. It is a greedy method since it never backtracks to reevaluate a subset. Basically, it points itself in a single direction and proceeds. The main question is how far to go in that direction.\nThese two new functions, gafs and safs, conduct a global search over the feature set. This means that the direction of the search is constantly changing and may re-evaluate solutions that are similar to (or the same as) previously evaluated feature sets. This is good and bad. If a feature set is not easily “rankable” then it should do better than RFE. However, it will require a larger number of model fits and has a greater chance of overfitting the features to the training set.\nI won’t go over the details of GAs or SA. There are a lot of references on these and you can always check out Chapter 19 of our book. Also, since there were two previous blog posts about genetic algorithms, I’ll focus on simulated annealing in this one.\nTo demonstrate I’ll use the Alzheimer’s disease data from Section 19.6 (page 502). It contains biomarker and patient demographic data that are being used to predict who will eventually have cognitive impairment. The original data contained instances for 333 subjects and information on 133 predictors.\nIn the book, we fit a number of different models to these data and conducted feature selection. Using RFE, linear discriminant analysis, generalized linear models and naive Bayes seemed to benefit from removing non-informative predictors. However, K-nearest neighbors and support vector machines had worse performance as RFE proceeded to remove predictors.\nWith KNN, there is no model-based method for measuring predictor importance. In these cases, the rfe will use the area under the ROC curve for each individual predictor for ranking. Using that approach, here is the plot of KNN performance over subset sizes:\n\n\n\n\n\nThe values shown on the plot are the average area under the ROC curve measured using 5 repeats of 10-fold cross-validation. This implies that the full set of predictors is required. I should also note that, even with the entire predictor set, the KNN model did worse than LDA and random forests.\nWhile RFE couldn’t find a better subset of predictors, that does not mean that one doesn’t exist. Can simulated annealing do better?\nThe code to load and split the data are in the AppliedPredictiveModeling package and you can find the markdown for this blog post linked at the bottom of this post. We have a data frame called training that has all the data used to fit the models. The outcome is a factor called Class and the predictor names are in a character vector called predVars. First, let’s define the resampling folds.\nLet’s define the control object that will specify many of the important options for the search. There are a lot of details to spell out, such as how to train the model, how new samples are predicted etc. There is a pre-made list called caretSA that contains a starting template.\nnames(caretSA)\n## [1] \"fit\"            \"pred\"           \"fitness_intern\" \"fitness_extern\"\n## [5] \"initial\"        \"perturb\"        \"prob\"           \"selectIter\"    \nThese functions are detailed on the packages web page. We will make a copy of this and change the method of measuring performance using hold-out samples:\nknnSA &lt;- caretSA\nknnSA$fitness_extern &lt;- twoClassSummary\nThis will compute the area under the ROC curve and the sensitivity and specificity using the default 50% probability cutoff. These functions will be passed into the safs function (see below)\nsafs will conduct the SA search inside a resampling wrapper as defined by the index object that we created above (50 total times). For each subset, it computed and out-of-sample (i.e. “external”) performance estimate to make sure that we are not overfitting to the feature set. This code should reproduce the same folds as those used in the book:\nlibrary(caret)\nset.seed(104)\nindex &lt;- createMultiFolds(training$Class, times = 5)\nHere is the control object that we will use:\nctrl &lt;- safsControl(functions = knnSA,\n                    method = \"repeatedcv\",\n                    repeats = 5,\n                    ## Here are the exact folds to used:\n                    index = index,\n                    ## What should we optimize? \n                    metric = c(internal = \"ROC\",\n                               external = \"ROC\"),\n                    maximize = c(internal = TRUE,\n                                 external = TRUE),\n                    improve = 25,\n                    allowParallel = TRUE)\nThe option improve specifies how many SA iterations can occur without improvement. Here, the search restarts at the last known improvement after 25 iterations where the internal fitness value has not improved. The allowParallel options tells the package that it can parallelize the external resampling loops (if parallel processing is available).\nThe metric argument above is a little different than it would for train or rfe. SA needs an internal estimate of performance to help guide the search. To that end, we will also use cross-validation to tune the KNN model and measure performance. Normally, we would use the train function to do this. For example, using the full set of predictors, this could might work to tune the model:\ntrain(x = training[, predVars],\n      y = training$Class,\n      method = \"knn\",\n      metric = \"ROC\",\n      tuneLength = 20,\n      preProc = c(\"center\", \"scale\"),\n      trControl = trainControl(method = \"repeatedcv\",\n                               ## Produce class prob predictions\n                               classProbs = TRUE,\n                               summaryFunction = twoClassSummary))\nThe beauty of using the caretSA functions is that the ... are available.\nA short explanation of why you should care…\nR has this great feature where you can seamlessly pass arguments between functions. Suppose we have this function that computes the means of the columns in a matrix or data frame:\nmean_by_column &lt;- function(x, ...) {\n  results &lt;- rep(NA, ncol(x))\n  for(i in 1:ncol(x)) results[i] &lt;- mean(x[, i], ...)\n  results\n  }\n(aside: don’t loop over columns with for. See ?apply, or ?colMeans, instead)\nThere might be some options to the mean function that we want to pass in but those options might change for different applications. Rather than making different versions of the function for different option combinations, any argument that we pass to mean_by_column that is not one it its arguments (x, in this case) is passed to wherever the three dots appear inside the function. For the function above, they go to mean. Suppose there are missing values in x:\nexample &lt;- matrix(runif(100), ncol = 5)\nexample[1, 5] &lt;- NA\nexample[16, 1] &lt;- NA\nmean_by_column(example)\n## [1]     NA 0.4922 0.4704 0.5381     NA\nmean has an option called na.rm that will compute the mean on the complete data. Now, we can pass this into mean_by_column even though this is not one of its options.\nmean_by_column(example, na.rm = TRUE)\n## [1] 0.5584 0.4922 0.4704 0.5381 0.4658\nHere’s why this is relevant. caretSA$fit uses train to fit and possibly tune the model.\ncaretSA$fit\n## function (x, y, lev = NULL, last = FALSE, ...) \n## train(x, y, ...)\nAny options that we pass to safs that are not x, y, iters, differences, or safsControl will be passed to train.\n(Even further, any option passed to safs that isn’t an option to train gets passed down one more time to the underlying fit function. A good example of this is using importance = TRUE with random forest models. )\nSo, putting it all together:\nset.seed(721)\nknn_sa &lt;- safs(x = training[, predVars],\n               y = training$Class,\n               iters = 500,\n               safsControl = ctrl,\n\n               ## Now we pass options to `train` via \"knnSA\":               \n               method = \"knn\",\n               metric = \"ROC\",\n               tuneLength = 20,\n               preProc = c(\"center\", \"scale\"),\n               trControl = trainControl(method = \"repeatedcv\",\n                                        repeats = 2,\n                                        classProbs = TRUE,\n                                        summaryFunction = twoClassSummary,\n                                        allowParallel = FALSE))\nTo recap:\n\nthe SA is conducted many times inside of resampling to get an external estimate of performance.\ninside of this external resampling loop, the KNN model is tuned using another, internal resampling procedure.\nthe area under the ROC curve is used to guide the search (internally) and to know if the SA has overfit to the features (externally)\nin the code above, when safs is called with other options (e.g. method = \"knn\"),\n\nsafs passes the method, metric, tuneLength, preProc, and tuneLength, trControl options to caretSA$fit\n\ncaretSA$fit passes these options to train\n\n\n\nAfter external resampling, the optimal number of search iterations is determined and one last SA is run using all of the training data.\nNeedless to say, this executes a lot of KNN models. When I ran this, I used parallel processing to speed things up using the doMC package.\nHere are the results of the search:\nknn_sa\n## Simulated Annealing Feature Selection\n## \n## 267 samples\n## 132 predictors\n## 2 classes: 'Impaired', 'Control' \n## \n## Maximum search iterations: 500 \n## Restart after 25 iterations without improvement (15.6 restarts on average)\n## \n## Internal performance values: ROC, Sens, Spec\n## Subset selection driven to maximize internal ROC \n## \n## External performance values: ROC, Sens, Spec\n## Best iteration chose by maximizing external ROC \n## External resampling method: Cross-Validated (10 fold, repeated 5 times) \n## \n## During resampling:\n##   * the top 5 selected variables (out of a possible 132):\n##     Ab_42 (96%), tau (92%), Cystatin_C (82%), NT_proBNP (82%), VEGF (82%)\n##   * on average, 60 variables were selected (min = 45, max = 75)\n## \n## In the final search using the entire training set:\n##    * 59 features selected at iteration 488 including:\n##      Alpha_1_Antitrypsin, Alpha_1_Microglobulin, Alpha_2_Macroglobulin, Angiopoietin_2_ANG_2, Apolipoprotein_E ... \n##    * external performance at this iteration is\n## \n##        ROC       Sens       Spec \n##      0.852      0.198      0.987 \nThe algorithm automatically chose the subset created at iteration 488 of the SA (based on the external ROC) which contained 59 out of 133 predictors.\nWe can also plot the performance values over iterations using the plot function. By default, this uses the ggplot2 package, so we can add a theme at the end of the call:\n\n\n\n\n\nEach of the data points for the external fitness is a average of the 50 resampled ROC values. The most improvement was found in the first 200 iterations. The internal estimate is generally more optimistic than the external estimate. It also tends to increase while the external estimate is relatively flat, indicating some overfitting. The plot above indicates that less iterations might probably give us equivalent performance. Instead of repeating the SA with fewer iterations, the update function can be used to pick a different subset size.\nLet’s compare the RFE and SA profiles. For RFE and SA, the ROC values are averaged over the 50 resamples. In the case of SA, the number of predictors is also an average. For example, at iteration 100 of the SA, here is the subset size distribution across the 50 resamples:\n\n\n\n\n\nHere are superimposed smoothed trend lines for the resampling profiles of each search method:\n\n\n\n\n\nRecall that the same cross-validation folds were used for SA and RFE, so this is an apples-to-apples comparison. SA searched a smaller range of subset sizes over the iterations in comparison to RFE. The code here starts the initial subset with a random 20% of the possible features and tended to increase as the search continued and then stabilized at a size of about 55.\nHow does this predictor subset perform on the test data?\nlibrary(pROC)\nroc(testing$Class,\n    predict(knn_sa, testing)$Impaired,\n    levels = rev(levels(testing$Class)))\n## Call:\n## roc.default(response = testing$Class, \n##             predictor = predict(knn_sa, testing)$Impaired, \n##             levels = rev(levels(testing$Class)))\n## \n## Data: predict(knn_sa, testing)$Impaired in 48 controls \n##       (testing$Class Control) &lt;  18 cases (testing$Class \n##       Impaired).\n##       \n## Area under the curve: 0.848\nThis is better than the test set results for the RFE procedure. Note that the test set AUC is much more in-line with the external estimate of the area under the ROC curve. The bad new is that we evaluated many more models than the RFE procedure and the SA process was slightly more than 11-fold slower than RFE to complete. Good things take time. Here is a parallel-coordinate plot of the individual resampling results, match by fold:\n\n\n\n\n\nThe difference is statistically signficant too:\nsummary(diff(rs, metric = \"ROC\"))\n## Call:\n## summary.diff.resamples(object = diff(rs, metric = \"ROC\"))\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## ROC \n##     RFE      SA     \n## RFE          -0.0504\n## SA  8.02e-06        \nThe genetic algorithm code in gafs has very similar syntax to safs and also has pre-made functions.\nThe knitr file for these analyses can be found here (in Rhtml).\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/rstudio-2018-conference-presentation-and-materials/index.html",
    "href": "posts/rstudio-2018-conference-presentation-and-materials/index.html",
    "title": "RStudio 2018 Conference Presentation and Materials",
    "section": "",
    "text": "We’ve released our videos of the talks at the 2018 RStudio conference. My talk was Modeling in the Tidyverse (video) and I was also in the Tidyverse fireside chat (video). There are a lot of great talks on the conference website.\nI also conducted a two day workshop on caret and the new modeling packages (recipes, rsample, tidyposterior and others) at the conference. The was no video of that, but all of the materials can be found in the course GitHub repository.\nThere will be at least one other workshop for this year on these topics. When that is finalized, I’ll post the details here and on on twitter.\n(the image above is from the workshop and is from tidyposterior)\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/reproducible-research-at-enar/index.html",
    "href": "posts/reproducible-research-at-enar/index.html",
    "title": "Reproducible Research at ENAR",
    "section": "",
    "text": "I gave a talk at the Spring ENAR meetings this morning on some of the technical aspects of creating the book. The session was on reproducible research and the slides are here.\nI was dinged for not using git for version control (we used dropbox for simplicity) but overall the comments were good. There was a small panel at the end for answering questions, which were mostly related to proprietary systems (e.g. SAS).\nI was also approached by an editor for Computational Statistics in regards to writing all of this up, which I will when I get a free moment.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/recent-changes-to-caret/index.html",
    "href": "posts/recent-changes-to-caret/index.html",
    "title": "Recent Changes to caret",
    "section": "",
    "text": "Here is a summary of some recent changes to caret.\nFeature Updates:\n\ntrain was updated to utilize recent changes in the gbm package that allow for boosting with three or more classes (via the multinomial distribution)\nThe Yeo-Johnson power transformation was added. This is very similar to the Box-Cox transformation, but it does not require the data to be greater than zero.\n\nNew models referenced by train:\n\nMaximum uncertainty linear discriminant analysis (Mlda) and factor-based linear discriminant analysis (RFlda) from the HiDimDA package were added.\nThe kknn.train model in the kknn package was added. This is basically a more intelligent K-nearest neighbors model that can use distance weighting, non-Euclidean distances (via the o Minkowski distance) and a few other features.\nThe extraTrees function in the package of the same name was added. This generalizes the random forest model by adding randomness to the predictors and the split values that are evaluated at each split point.\n\nNumerous bugs were also fixed in the last few releases.\nThe new version is 5.16-04. Feel free to email me at mxkuhn@gmail.com if you have any feature requests or questions.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/podcast-on-nonclinical-statistics/index.html",
    "href": "posts/podcast-on-nonclinical-statistics/index.html",
    "title": "Podcast on Nonclinical Statistics",
    "section": "",
    "text": "Hugo Bowne-Anderson and I spoke about about data science in pharmaceuticals, the tidyverse, and more for the excellent DataFramed podcast from DataCamp. Listen to it here or through your favorite blogging app.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/one-statisticians-view-of-big-data/index.html",
    "href": "posts/one-statisticians-view-of-big-data/index.html",
    "title": "One Statistician’s View of Big Data",
    "section": "",
    "text": "Recently I’ve had several questions about using machine learning models with large data sets. Here is a talk I gave at Yale’s Big Data Symposium on the subject.\nI believe that, with a few exceptions, less data is more. Once you get beyond some “large enough” number of samples, most models don’t really change that much and the additional computation burden is likely to cause practical problems with model fitting.\nOff the top of my head, the exceptions that I can think of are:\n\nclass imbalances\npoor variability in measured predictors\nexploring new “spaces” or customer segments\n\nBig Data may be great as long as you are adding something of value (instead of more of what you already have). The last bullet above is a good example. I work a lot with computational chemistry and we are constantly moving into new areas of “chemical space” making new compounds that have qualities that had not been previously investigated. Models that ignore this space are not as good as ones that do include them.\nAlso, new measurements or characteristic of your samples can make all the difference. Anthony Goldbloom of Kaggle has a great example from a competition for predicting the value of used cars:\n\nThe results included for instance that orange cars were generally more reliable - and that colour was a very significant predictor of the reliability of a used car.\n\n\n“The intuition here is that if you are the first buyer of an orange car, orange is an unusual colour you’re probably going to be someone who really cares about the car and so you looked after it better than somebody who bought a silver car,” said Goldbloom.\n\n\n“The data doesn’t lie - the data unearthed that correlation. It was something that they had not taken into account before when purchasing vehicles.”\n\nMy presentation has other examples of adding new information to increase the dimensionality of the data. The final quote sums it up:\n\nThe availability of Big Data should be a trigger to really re-evaluate what we are trying to solve and why this will help.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/nonclinical-statistics-book/index.html",
    "href": "posts/nonclinical-statistics-book/index.html",
    "title": "Nonclinical Statistics Book",
    "section": "",
    "text": "Springer has a new book (Amazon) edited by Lanju Zhang that captures the breadth of problems for statistics in the pharmaceutical industry including: compound optimization, genetic testing, high-throughput screening, safety testing, and manufacturing. From the first chapter:\n\n‘We define “Nonclinical Statistics” as statistics applied to areas other than clinical trials in pharmaceutical/biotechnology industries’\n\nIt’s a big book (~700 pages) and has a lot of great content. I was a section editor for the drug discovery chapters:\n\nStatistical Methods for Drug Discovery (Max Kuhn, Phillip Yates, and Craig Hyde)\nHigh-Throughput Screening Data Analysis (Hanspeter Gubler)\nQuantitative-Structure Activity Relationship Modeling and Cheminformatics (Max Kuhn)\nGWAS for Drug Discovery (Yang Lu, Katherine Perez-Morera and Rita M. Cantor)\nStatistical Applications in Design and Analysis of In Vitro Safety Screening Assays (Lei Shu, Gary Gintant and Lanju Zhang)\n\nI particularly like the chapter by Bill Pikounis and Luc Bijnens (“How To Be a Good Nonclinical Statistician”) which has a lot of excellent general advice.\nThe back cover blurb is:\n\n‘This book serves as a reference text for regulatory, industry and academic statisticians and also a handy manual for entry level Statisticians. Additionally it aims to stimulate academic interest in the field of Nonclinical Statistics and promote this as an important discipline in its own right. This text brings together for the first time in a single volume a comprehensive survey of methods important to the nonclinical science areas within the pharmaceutical and biotechnology industries. Specifically the Discovery and Translational sciences, the Safety/Toxiology sciences, and the Chemistry, Manufacturing and Controls sciences. Drug discovery and development is a long and costly process. Most decisions in the drug development process are made with incomplete information. The data is rife with uncertainties and hence risky by nature. This is therefore the purview of Statistics. As such, this book aims to introduce readers to important statistical thinking and its application in these nonclinical areas. The chapters provide as appropriate, a scientific background to the topic, relevant regulatory guidance, current statistical practice, and further research directions.’\n\nThe hardcopy format will be released on February 14, 2016. I couldn’t say whether you should gift the important person in your life with nonclinical statistics…\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/new-workshop-in-washington-dc-august/index.html",
    "href": "posts/new-workshop-in-washington-dc-august/index.html",
    "title": "New Workshop in Washington DC (August)",
    "section": "",
    "text": "I’ll be conducting a workshop called “Applied Machine Learning” in Washington DC on August 15 and 16. The last one, at the RStudio conference, sold out quickly.\nThe 2 day course is a blend of caret and the newer tidy modeling pacakges (recipes, rsample, etc):\n\nMachine learning is the study and application of algorithms that learn from and make predictions on data. From search results to self-driving cars, it has manifested itself in all areas of our lives and is one of the most exciting and fast-growing fields of research in the world of data science.\n\n\nThis two-day course will provide an overview of using R for supervised learning. The session will step through the process of building, visualizing, testing, and comparing models that are focused on prediction. The goal of the course is to provide a thorough workflow in R that can be used with many different regression or classification techniques. Case studies on real data will be used to illustrate the functionality and several different predictive models are illustrated.\n\n\nThe course focuses on both high-level approaches to modeling (e.g., the caret package) and newer modeling packages in the tidyverse: recipes, rsample, yardstick, and tidyposterior. Basic familiarity with R and the tidyverse is required.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/new-location-same-content/index.html",
    "href": "posts/new-location-same-content/index.html",
    "title": "New Location, Same Content",
    "section": "",
    "text": "This is the new home for the Applied Predictive Modeling blog.\nWe’ve moved it here, along with the previous blog posts, because we’re now using a format that is much easier to work with (Quarto). This will make it a lot easier to create posts1.\nThe blog is starting back up since we are actively developing our new (third) book called Applied Machine Learning for Tabular Data (aml4td). We’ll post progress updates, requests for community help, discussions of technical matters, and other topics.\nWe kept the old blog name, which might be a little confusing since we are focusing on a different book. First, we wanted our posts in the same location; adding content in a new place was awkward. Second, the new book has similar topics and I’d expect we’d put similar content here.\nInstead of having discussions inside the blog posts, we’ll create GitHub issues when we ask for community feedback and folks can respond there.\n(The banner image is by Dmytro Tolokonov)"
  },
  {
    "objectID": "posts/new-location-same-content/index.html#footnotes",
    "href": "posts/new-location-same-content/index.html#footnotes",
    "title": "New Location, Same Content",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOur old site was started around 2010. GitHub was barely a thing and we were years away from R’s markdown/rmarkdown/blogdown packages. We used Squarespace to host the blog, and it was fine but unwieldy for what we were doing.↩︎"
  },
  {
    "objectID": "posts/new-caret-version-6052/index.html",
    "href": "posts/new-caret-version-6052/index.html",
    "title": "New caret Version (6.0-52)",
    "section": "",
    "text": "A new version of caret (6.0-52) is on CRAN.\nHere is the news file but the Cliff Notes are:\n\nsub-sampling for class imbalances is now integrated with train and is used inside of standard resampling. There are four methods available right now: up- and down-sampling, SMOTE, and ROSE. The help page has detailed information.\nNine additional models were added, bringing the total up to 192.\nMore error traps were added for common mistakes (e.g. bad factor levels in classification).\nVarious bug fixes and snake enhancements\n\nOn-deck for upcoming versions:\n\nAn expanded interface for preprocessing. You might want to process some predictors one way and others differently (or not at all). A new interface will allow for this but should maintain backwards compatibility (I hope)\nCensored data models. Right now we are spec’ing out how this will work but the plan is to have models for predicting the outcome directly as well as models that predict survivor function probabilities. Email me (max.kuhn@pfizer.com) if you have an interest in this.\nEnabling prediction intervals (for models that support this) using predict.train. To be clear, caret isn’t generating these but if you fit an lm model, you will be able to generate intervals from predict.lm and pass them through predict.train.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/my-research-tools/index.html",
    "href": "posts/my-research-tools/index.html",
    "title": "My Research Tools",
    "section": "",
    "text": "Pfizer has an excellent group of librarians and they recently contacted people, including a few statisticians, about how we find and organize article. I’ve spent considerable time thinking about this over the years. I’ve wanted to start a discussion about this topic for a while since I can’t believe that someone isn’t doing this better. Comments here or via email are enthusiastically welcome.\nFor finding journal articles, I do a few different things.\n\nRSS feeds for journals.\nRSS feeds are pretty straightforward to use. Most journals have RSS feeds of various types for journals (e.g. current issue, just accepted, articles ASAP etc.) In some cases, like PLOSone, you can create RSS feeds for specific search terms within that journal (see the examples at the bottom of this post). I haven’t figured out how to filter RSS feeds based on whether the manuscript has supplemental materials (e.g. data).\nRSS isn’t perfect. For example, some of the ASA journals have mucked up their XML and I see a lot of repeats of articles on the same day. An edited list of what I keep tabs on is at the end of this post.\n(As an aside, RSS feeds are also great for monitoring specific topics on Stack Overflow and Crossvalidated)\nI have tried myriad RSS readers to aggregate and monitor my feeds. I’m currently using Feedly.\nAlso, this is only for content that you have identified as interesting. There could be something else out there that you have missed completely. That leads me to…\n\n\nGoogle Alerts\nI have about 30 different alerts. Some are related to general topics (e.g. [\"training set\" \"test set\" -microarray -SNP -QSAR -proteom -RNA -biomarker -biomarkers]) and others look for anything citing specific manuscripts (e.g. [Documents citing \"The design and analysis of benchmark experiments\"]). See this page for examples of how to create effective alerts. There are other uses for alerts too.\nAlerts are very effective. I usually get emails with the alerts in batches of 20 or so at a time. I haven’t quite figured out what the trigger is; in some cases I get two batches in a single day.\nOne thing I would put on the wish list is to so some sort of smart aggregation. If have alerts for [ \"simulated annealing\" \"feature selection\" ]; Articles excluding patents and [ \"genetic algorithm\" \"feature selection\" ]; Articles excluding patents, this results in abundant redundancy since many feature selection articles mention both search algorithms.\nKeep in mind that the alerts may not be new articles but items that are new to Google. This isn’t really an issue for me but it is worth mentioning.\n\n\nGoogle Scholar\nI love Google Scholar. Search on a title and you always be able to find the manuscript, links to different sources for obtaining it, plus a list of articles which reference it. Subject-based search are just as effective.\n(Our librarians were surprised to find that we could get access to articles that our institution did not have licenses for via Google. For example, the scholar page for an article will list multiple versions of the reference. Some of these may correspond to the home page of one of the authors where he/she has a local copy of the PDF)\nGoogle has good tips on searching. This presentation is excellent with some tricks that I didn’t know.\nSo once I’ve found articles, how do I manage them?\n\n\nPapers\nPapers… I have equal parts love and hate for this program. I’ll list the pros and cons below. I should say that I have been using this since the original version and have become increasingly frustrated . I’m not using the most recent version and I have tried a lot of different alternatives (e.g. Mendeley, BibDesk, Bookends, Endnote, Sente, Zotero). Unfortunately, for someone with thousands of PDFs, Papers (version 2) has some features that the others haven’t mastered yet. I would love to move away from Papers.\nWhat is good:\n\nImporting articles is easy. In many cases, just dropping them into the window will find the metadata and automatically annotate the reference. Weirdly, drag-and-drop works better than the “Match” feature in the article window. There are Open In Papers bookmarks for most browsers. Once you find a journal article, use this link to start Papers and open the link. Often, the application automatically reads the citation information from the webpage and imports it. Clicking on the PDF link within the article’s web page imports that file.\nArticles within Papers can collect supplementary files easily. One minor issue is that plain text files are not automatically imported as PDF, CSV for other file formats are.\nPapers does a great job or organizing the PDFs locally. I sync to Dropbox and have the same repository across different computers.\nThe bibtex export works well. This was invaluable when we were writing the book.\nTheir apps for tablets/mobile are easy to use and low maintenance. Syncing has not been an issue for me so far.\n\nThe bad news\n\nSlooooow. It is really slow.\nThe search facilities for your PDF repository are not very powerful. This seems like it is a pretty low bar to jump over.\nKeywords work but are manually added and the interface is pretty kludgy. I would love for this feature to work better. Hell, it wouldn’t be difficult to automatically figure this out based on content (I wrote some rudimentary R/SQL code to do it on an really long plane flight once).\nI have a small percentage of papers whose PDFs have gone missing.\nI might accidentally import an article twice. In most cases, Papers doesn’t tell me that I’m doing it until it is too late. Although they have a method for merging entries, I’d like to avoid this process beforehand.\nThey release versions with little to no testing. This is amazing but basic functionality in new major versions simply does not work. It is remarkable in the worst way.\nWhile they did win the Apple Design Award for the first version, the interface seems to be getting worse with every new release. The color scheme for Papers 3 makes me depressed. Literal 50 shades of gray.\n\nThe last two issues have driven me crazy. I don’t see myself upgrading any time soon.\n\n\nTypesetting\nI use LaTeX for almost all articles that I write. It is a pain when working with others who have never used it (or heard of it) but it is worth it. Also, the power you get when using LaTeX with Sweave or knitr simply cannot be underestimated. Apart from exporting bibtex from Papers, the other tools I use are:\n\nSublime Text: This is a great, lightweight editor that has some great add-ons for typesetting that integrate with Skim. Skim is pretty nice, but I would really like Sublime to work with OS X’s Preview.\n\ntexpad is another good editor for OS X but, given the price, it might be difficult to argue that it is better than Sublime. It does hide a lot of the LaTeX junk that goes into typesetting a tex file but this is really a minor perk.\n\nI gave a talk at ENAR last year related to this. We’ve since moved the book version control to github and have translated all of our Sweave code to knitr.\n\n\nMy Journal Feeds\nIn no particular order:\n\nBMC Bioinformatics - Latest Articles\nComputational Biology and Chemistry\nBioinformatics - current issue\nBioinformatics\nJournal of Biomolecular Screening current issue\nJournal of Computational Biology - Table of Contents\nRSC - Med. Chem. Commun. latest articles\nChemometrics and Intelligent Laboratory Systems\nJournal of Chemical Information and Modeling: Latest Articles (ACS Publications)\nJournal of Computational Chemistry\nMolecular Informatics\nJournal of Medicinal Chemistry: Latest Articles (ACS Publications)\nJournal of Chemometrics\nJournal of Cheminformatics - Latest Articles\nPLOS ONE ML\nLatest Issue of Combinatorial Chemistry & High Throughput Screening\nBMC Cell Biology - Latest Articles\nNature reviews. Molecular cell biology[TA]\nNature Reviews Drug Discovery - Issue - nature.com science feeds\nNature Reviews Genetics - Issue - nature.com science feeds\nGenome Biology - Latest Articles\nGenome Research current issue\nDrug Discovery Today\nNature Biological Sciences Research\nJMLR\nMachine Learning (Online First)\nstat.ML updates on arXiv.org\nPLOS ONE feature selection\nMachine Learning\nNature - Issue\nJournal of Statistical Software\nProceedings of the National Academy of Sciences Statistics\nJournal of the American Statistical Association: Table of Contents\nThe American Statistician: Table of Contents\nWiley Interdisciplinary Reviews: Computational Statistics\nComputational Statistics & Data Analysis\nphysics.data-an updates on arXiv.or\nElectronic Journal of Statistics\nPharmaceutical Statistics\nJournal of the Royal Statistical Society: Series C (Applied Statistics)\nBiostatistics - current issue\nJournal of the Royal Statistical Society: Series B (Statistical Methodology)\nNature Reviews Drug Discovery - Issue - nature.com science feeds\nDrug Discovery Today\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/lots-of-package-news/index.html",
    "href": "posts/lots-of-package-news/index.html",
    "title": "Lots of Package News",
    "section": "",
    "text": "I’ve sent a slew of packages to CRAN recently (thanks to Swetlana and Uwe). There are updates to:\n\ncaret was primarily updated to deal with an issue introduced in the last version. It is a good idea to avoid fully loading the underlying modeling packages to avoid name conflicts. We made that change in the last version and it ended up being more complex than thought. A quirk in the regression tests missed it too but the whole thing can be avoided by loading the modeling package. news file\nrsample had some modest additions including bridges to caret and recipes. The website added more application examples (times series and survival analysis). news file\nrecipes had more substantial enhancments including six new steps, a better interface for creating interactions (using selectors), and the ability to save the processed data in a sparse format. news file\nCubist and C50 have been updated and brought into the age of roxygen and pkgdown. C5.0 now has a nice plot method a la partykit and now has a vignette. I’ll be adding a few features to each over time.\n\nTwo new packages:\n\nyardstick contains many of the performance measurement methods in caret but in a format that is easier to use with dplyr syntax and functional programming.\ntidyposterior is a Bayesian version of caret’s resamples function. It can be used to take the resampling results from multiple models and do more formal statistical comparisons. It is similar in spirit to Benavoli et al (2017). We are looking for nominations for the hex logo so please offer your suggestions (but keep it clean).\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/feature-selection-strikes-back-part-1/index.html",
    "href": "posts/feature-selection-strikes-back-part-1/index.html",
    "title": "Feature Selection Strikes Back (Part 1)",
    "section": "",
    "text": "In the feature selection chapter, we describe several search procedures (“wrappers”) that can be used to optimize the number of predictors. Some techniques were described in more detail than others. Although we do describe genetic algorithms and how they can be used for reducing the dimensions of the data, this is the first of series of blog posts that look at them in practice using simulated data described in a previous post.\nGenetic algorithms are optimization tools that search for the best solution by mimicking the evolution of a population. A set of predictor subsets are evaluated in terms of their model performance and the best sets combine randomly to form new subsets. In the GA lingo, each iteration has a collection of subsets (i.e. population of chromosomes) with a corresponding model performance value (i.e. their fitness values). At each step of reproduction, there is some probability of random mutations. This has the effect of randomly turning some predictors off or on in each subset. The algorithm continues for a set number of generations.\nOne question is how to evaluate the fitness function. There are a few options:\n\nFor each subset, employ the same cross-validation approach used with the full data set. We know from the literature that this will not be a good estimate of future performance because of over-fitting to the predictor set. However, can it be used to differentiate good subsets from bad subsets?\nUse the test set. This is a poor choice since the data can no longer provide an unbiased view of model performance. Single test sets usually have more noise than resampled estimates.\nSet aside some data from the training set for calculating model performance for a given subset. We may eventually end up over-fitting to these data, so should we randomly select a different hold-out set each time? This will add some noise to the fitness values.\n\nIn the literature, how is this usually handled? From what I’ve found, internal cross-validated accuracy is used, meaning that the model is cross-validated within the feature selection. For this reason, there is a high likelihood that the estimate of the model’s performance will be optimistic (since it does not reflect the uncertainty induced by the search procedure).\nFor this example, we’ll simulate 500 training set samples and add a total of 200 non-informative predictors. For the extra predictors, 100 will be a set of uncorrelated standard normal random variables while 100 will be multivariate normal with a pre-defined correlation structure. The correlated set will have variances of 1 and an auto-regressive structure (AR1). While the is no time component to this model, using this structure will simulate predictors with various levels of correlation. To do this, a function called twoClassSim is used. The code for this document is here, so you can see and use this function yourself.\nThree sets of data were simulated: a training set of 500 samples, a test set of 200 samples and a very large set that will help us approximate the true error rate.\nset.seed(468)\ntraining &lt;- twoClassSim(500, noiseVars = 100, \n                        corrVar = 100, corrValue = 0.75)\ntesting &lt;- twoClassSim(200, noiseVars = 100, \n                       corrVar = 100, corrValue = 0.75)\nlarge &lt;- twoClassSim(10000, noiseVars = 100, \n                     corrVar = 100, corrValue = 0.75)\n                     \n## Get the names of the truly active predictors\nrealVars &lt;- names(training)\nrealVars &lt;- realVars[!grepl(\"(Corr)|(Noise)\", realVars)]\n\n## We will use cross-validation later, so we setup the folds here so we\n## can make sure all of the model fits use the same data (not really\n## required, but helpful)\ncvIndex &lt;- createMultiFolds(training$Class, times = 2)\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     repeats = 2, \n                     classProbs = TRUE, \n                     summaryFunction = twoClassSummary, \n                     allowParallel = FALSE, \n                     index = cvIndex)\nFor these data, one model that has fairly good performance is quadratic discriminant analysis (QDA). This model can generate non-linear class boundaries (i.e. quadratic patterns) and models the covariance matrix of the predictors differently for each class. This means that for two classes and p predictors, a total of p (p+1) parameters are estimated (and these are just the variance parameters). With a large number of non-informative predictors, this can negatively affect model performance.\nIf we knew the true predictor set, what would we expect in terms of performance? Although QDA has no tuning parameters, I’ll use two repeats of 10-fold cross-validation to estimate the area under the ROC curve. Additionally, the AUC will also be derived from the test set and using the large sample set too.\nThe resampled area under the curve was the area under the ROC curve was 0.91 and the test set estimate was 0.946. There some difference there and the large sample estimate of the AUC is 0.928. Overall, performance is pretty good.\nNow, what happens when you use all the available predictors?\nThe estimates of performance for the AUC were: 0.505 (resampling), 0.492 (test set) and 0.515 (large-sample). All of these indicate that QDA tanked because of the excess variables.\nThe ROC curves for the large-sample predictions are:\n Another nasty side-effect when you have a large number of predictors (215) with a relatively small set of training set data (500) with several of the classical discriminant models is that the class probabilities become extremely polarized. We demonstrate this in the book and the same issue occurs here. The plot below shows histograms of the Class 1 probability for the large-sample set for each model. There are panels for each of the true classes.\n\n\n\n\n\nThe model with the true predictors has fairly well-calibrated class probabilities while the probabilities for the model with all the predictors are concentrated near zero and one. This can occur for both linear and quadratic discriminant analysis as well as naive Bayes models. In the book, we also show that this is independent of the amount of signal in the data.\nClearly, there is a need for some feature selection. To start, I used a genetic algorithm to maximize performance of the QDA model. The search procedure for these data used most of the defaults from the GA package:\n\n400 generations\nan initial population of 50 chromosomes\na mutation probability of 10%\na cross-over probability of 80%\nelitism of 2\nthe area under the ROC curve was estimated using two repeats of 10-fold cross-validation was used\n\nIn total, about 400 * 50 * 20 = 400,000 QDA models were evaluated. I also ran the models within each generation in parallel, which makes a good dent in the computation time.\nTo do this, I used a slightly modified version of the GA R package. Our changes enabled us to save the chromosome value for the best result per generation (for further analysis) and use parallel processing. The package maintainer (Luca Scrucca) has been testing these additions.\nI have to define a fitness function that defines what should be maximized. I’ll use caret to tune the model and return the resampled estimate of the area under the ROC curve:\n## 'ind' will be a vector of 0/1 data denoting which predictors are being\n## evaluated.\nROCcv &lt;- function(ind, x, y, cntrl) {\n    library(caret)\n    library(MASS)\n    ind &lt;- which(ind == 1)\n    ## In case no predictors are selected:\n    if (length(ind) == 0) return(0)\n    out &lt;- train(x[, ind, drop = FALSE], y, \n                 method = \"qda\", \n                 metric = \"ROC\", \n                 trControl = cntrl)\n    ## this function returns the resampled ROC estimate\n    caret:::getTrainPerf(out)[, \"TrainROC\"]\nNow, to run the algorithm with the GA package:\nlibrary(GA)\nset.seed(137)\nga_resampled &lt;- ga(type = \"binary\", \n                   fitness = ROCcv, \n                   min = 0, \n                   max = 1, \n                   maxiter = 400, \n                   nBits = ncol(training) - 1, \n                   names = names(training)[-ncol(training)], \n                   ## These options are passed through the ga funciton\n                   ## and into the ROCcv function\n                   x = training[, -ncol(training)], \n                   y = training$Class, \n                   cntrl = ctrl, \n                   ## These two options are not yet in the GA package.\n                   keepBest = TRUE, \n                   parallel = TRUE)\nThe results are summarized in the image below, where the three estimates of the area under the ROC curve is shown. The size of the points is indicative of the number of predictors used in the best chromosome of each generation.\n\n\n\n\n\nThe resampled estimate has no idea that it is embedded inside of a feature selection routine, so it does not factor in that variability. The search steadily increases the AUC until it converges. However, the test set AUC (as well as the large-sample version) initially increase but then converge to a much smaller value that the cross-validation results would lead one to believe. These two pessimistic estimates of the AUC appear to be in-line with one another although the large-sample results are slightly lower in many generations. It looks at though the model is over-fitting to the predictors and the resampling procedure is not picking up on this.\nAs previously mentioned, another tactic is to utilize a separate test set to measure the area under the ROC curve. If we have a lot of data, it may be a good idea to have an “evaluation” set of samples to measure performance during feature selection and keep a different (true) test set to only use at the end.\nLet’s sacrifice our test set for the genetic algorithm and see if this helps. The new objective funciotn is:\n## An added 'test' argument...\nROCtest &lt;- function(ind, x, y, cntrl, test) {\n    library(MASS)\n    ind &lt;- which(ind == 1)\n    if (length(ind) == 0) return(0)\n    modFit &lt;- qda(x[, ind], y)\n    testROC &lt;- roc(test$Class, \n                   predict(modFit, \n                           test[, ind, drop = FALSE])$posterior[, 1],\n                   levels = rev(levels(y)))\n    as.vector(auc(testROC))\n}\nThe updated call to the ga function is:\nset.seed(137)\nga_test &lt;- ga(type = \"binary\", \n              fitness = ROCtest, \n              min = 0, \n              max = 1, \n              maxiter = 1000, \n              nBits = ncol(training) - 1, \n              names = names(training)[-ncol(training)], \n              x = training[,-ncol(training)], \n              y = training$Class, \n              cntrl = ctrl, \n              test = testing, \n              keepBest = TRUE, \n              parallel = TRUE)\nHere are the results:\n\n\n\n\n\nIf this were not a simulated data set, we would only see the green curve. There is a similar pattern between the resampled ROC and the results from our evaluation set (formerly known as the test set). However, the GA is still selecting too many predictors and, as a consequence, the true performance is still pretty poor. Basically, the evaluation set is not showing the degradation of performance due to the non-informative predictors (i.e. we are over-fitting to the evaluation set).\nThe genetic algorithm converged on a subset size of 97 predictors. This included 7 of the 10 linear predictors, 1 of the non-linear terms and both of the terms that have an interaction effect in the model. Looking across the generations, we can see the frequency that each type of non-informative predictor was retained:\n\n\n\n\n\nLet’s now fit a QDA model based on these predictors and see what the large-sample ROC curve looks like:\n## The bestBinary item is a list of the best chromosomes from\n## each generation. We will look at the last one and fit a QDA\n## model.\nfinalVars &lt;- ga_test@bestBinary[[length(ga_test@bestBinary)]]\nfinalFit &lt;- qda(training[, finalVars], training$Class)\n## Get the large sample results:\nfinalLarge &lt;- roc(large$Class, \n                  predict(finalFit, \n                          large[, finalVars])$posterior[, 1], \n                  levels = rev(levels(large$Class)))\nfinalLarge\n\n## \n## Call:\n## roc.default(response = large$Class, predictor = predict(finalFit,     large[, finalVars])$posterior[, 1], levels = rev(levels(large$Class)))\n## \n## Data: predict(finalFit, large[, finalVars])$posterior[, 1] in 4684 controls (large$Class Class2) &lt; 5316 cases (large$Class Class1).\n## Area under the curve: 0.622\nThe large-sample estimate of the area under the ROC curve is 0.622, which is not as good as the true model (0.928) but better than the worst-case scenario (0.515). The ROC curves are:\n\n\n\n\n\nIn the next blog post, I’ll look at other ways of improving the genetic algorithm. Before we do that though, let’s make a comparison to another feature selection procedure: recursive feature elimination (RFE). RFE is basically a backwards selection procedure that uses a some variable importance metric to rank the predictors. If will use the area under the ROC curve for each predictor to quantify its relative importance.\nHere, all subset sizes were evaluated and the procedure was cross-validated using the same two repeats of ten-fold cross-validation. The QDA models were trained in the same manner:\n## caret includes some pre-defined code for RFE, including code to do\n## linear discriminant analysis (LDA). The code for LDA and QDA are\n## almost identical, so we can recycle the LDA code and only change\n## the model fit function (to use QDA) and the function that computes\n## the model summary statistics (so that we can get the area under the \n## ROC curve):\nqdaFuncs &lt;- ldaFuncs\nqdaFuncs$fit &lt;- function(x, y, first, last, ...) {\n    library(MASS)\n    qda(x, y, ...)\n}\nqdaFuncs$summary &lt;- twoClassSummary\n\nqdaRfe &lt;- rfe(x = training[, -ncol(training)], \n              y = training$Class, \n              sizes = 1:(ncol(training) - 1), \n              metric = \"ROC\", \n              rfeControl = rfeControl(method = \"repeatedcv\", \n                                      repeats = 2, \n                                      index = cvIndex, \n                                      functions = qdaFuncs))\nHere, there are a maximum of 215 * 20 = 4,300 models being evaluated. In comparison to the genetic algorithm, not only is this fewer models, but many of them have smaller subset sizes than those shown in the figures above.\nThe potential down-side to this feature selection technique is that it is greedy; once a predictor is discarded, it is never considered again in a subset. For this reason, RFE may achieve a local optimum where as the genetic algorithm has the ability to find a global optimum.\nThis approach filtered far more predictors. The profile of the area under the ROC curve (with all three estimates of the area under the ROC curve):\n\n\n\n\n\nThe RFE algorithm fit the final QDA model to 11 predictors, including 5 linear effects and both predictors associated with the interaction. However, it did not capture any of the non-linear terms and picked up 4 non-informative predictors. Of the noise predictors, it was not biased towards the correlated set (only 1 of the 4 were correlated). The estimates were consistent with one another and the area under the ROC curve was estimated as 0.907 using resampling, 0.893 using the test set and 0.885 using the large-sample holdout. The consistency of these values is probably due to the RFE procedure constructing external resampling, meaning that each cross-validation did a separate feature elimination sequence and used held-out data to estimate performance. This prevented the type of overly optimistic estimates that were seen earlier.\nThe large-sample ROC curves are:\n\n\n\n\n\nSo far, RFE is more effective than the genetic algorithm at sub-setting features for these data.\nThe next blog post will look at modifications of the genetic algorithm that will improve performance.\nThe R packages loaded at the time of the analysis were: base (2.15.2), caret (5.15-87), cluster (1.14.3), datasets (2.15.2), doMC (1.2.5), foreach (1.4.0), GA (1.3), ggplot2 (0.9.2.1), graphics (2.15.2), grDevices (2.15.2), iterators (1.0.6), knitr (0.8), lattice (0.20-10), latticeExtra (0.6-24), MASS (7.3-22), methods (2.15.2), multicore (0.1-7), nlme (3.1-105), plyr (1.7.1), pROC (1.5.4), RColorBrewer (1.0-5), reshape2 (1.2.1), stats (2.15.2) and utils (2.15.2)\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/feature-selection-2--genetic-boogaloo/index.html",
    "href": "posts/feature-selection-2--genetic-boogaloo/index.html",
    "title": "Feature Selection 2 - Genetic Boogaloo",
    "section": "",
    "text": "Previously, I talked about genetic algorithms (GA) for feature selection and illustrated the algorithm using a modified version of the GA R package and simulated data. The data were simulated with 200 non-informative predictors and 12 linear effects and three non-linear effects. Quadratic discriminant analysis (QDA) was used to model the data. The last set of analyses showed, for these data, that:\n\nThe performance of QDA suffered from including the irrelevant terms.\nUsing a resampled estimate area under the ROC curve resulted in selection bias and severely over-fit to the predictor set.\nUsing a single test set to estimate the fitness value did marginally better model with an area under the ROC curve of 0.622.\nRecursive feature elimination (RFE) was also used and found a much better predictor subset with good performance (an AUC of 0.885 using the large-sample data set).\n\nFor the genetic algorithm, I used the default parameters (e.g. crossover rate, mutation probability etc). Can we do better with GA’s?\nOne characteristic seen in the last set of analyses is that, as the number of irrelevant predictors increases, there is a gradual decrease in the fitness value (although the true performance gets worse). The initial population of chromosomes for the GA is based on simple random sampling, which means that each predictor has about a 50% chance of being included. Since our true subset is much smaller, the GA should favor smaller sets and move towards cases with fewer predictors… except that it didn’t. I think that this didn’t happen because the of two reasons:\n\nThe increase in performance caused by removing a single predictor is small. Since there is no “performance cliff” for this model, the GA isn’t able to see improvements in performance unless a subset is tested with a very low number of predictors.\nClearly, using the evaluation set and resampling to measure the fitness value did not penalize larger models enough. The plots of these estimates versus the large sample estimates showed that they were not effective measurements. Note that the RFE procedure did not have this issue since the feature selection was conducted within the resampling process (and reduced the effect of selection bias).\n\nWould resampling the entire GA process help? It might but I doubt it. It only affects how performance is measured and does not help the selection of features, which is the underlying issue. I think it would result in another over-fit model and all that the resampling would do would be to accurately tell when the model begins to over-fit. I might test this hypothesis in another post.\nThere are two approaches that I’ll try here to improve the effectiveness of the GA. The first is to modify the initial population. If we have a large number of irrelevant predictors in the model, the GA has difficultly driving the number of predictors down. However, would the converse be true? We are doing feature selection. Why not start the GA with a set of chromosomes with small predictor sets. Would the algorithm drive up the number of predictors or would it see the loss of performance and keep the number small?\nTo test this, I simulated the same data sets:\nset.seed(468)\ntraining &lt;- twoClassSim(500, noiseVars = 100, \n                        corrVar = 100, corrValue = 0.75)\ntesting &lt;- twoClassSim(500, noiseVars = 100, \n                       corrVar = 100, corrValue = 0.75)\nlarge &lt;- twoClassSim(10000, noiseVars = 100, \n                     corrVar = 100, corrValue = 0.75)\nrealVars &lt;- names(training)\nrealVars &lt;- realVars[!grepl(\"(Corr)|(Noise)\", realVars)]\ncvIndex &lt;- createMultiFolds(training$Class, times = 2)\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     repeats = 2, \n                     classProbs = TRUE, \n                     summaryFunction = twoClassSummary,\n                     allowParallel = FALSE, \n                     index = cvIndex)\nThe ga function has a parameter for creating the initial population. I’ll use one that produces chromosomes with a random 10% of the predictors.\ninitialSmall &lt;- function(object, ...) \n{\n    population &lt;- sample(0:1, \n                         replace = TRUE, \n                         size = object@nBits * object@popSize, \n                         prob = c(0.9, 0.1))\n    population &lt;- matrix(population, \n                         nrow = object@popSize, \n                         ncol = object@nBits)\n    return(population)\n}\nThen I’ll run the GA again (see the last post for more explanation of this code).\nROCcv &lt;- function(ind, x, y, cntrl) \n{\n    library(caret)\n    library(MASS)\n    ind &lt;- which(ind == 1)\n    ## In case no predictors are selected:\n    if (length(ind) == 0)  return(0)\n    out &lt;- train(x[, ind], y, \n                 method = \"qda\", \n                 metric = \"ROC\", \n                 trControl = cntrl)\n    ## Return the resampled ROC value\n    caret:::getTrainPerf(out)[, \"TrainROC\"]\n}\n\nset.seed(137)\nga_small_cv &lt;- ga(type = \"binary\", \n                  fitness = ROCcv, \n                  min = 0, max = 1, \n                  maxiter = 400, \n                  population = initialSmall, \n                  nBits = ncol(training) - 1, \n                  names = names(training)[-ncol(training)],\n                  x = training[, -ncol(training)], \n                  y = training$Class, \n                  cntrl = ctrl, \n                  keepBest = TRUE, \n                  parallel = FALSE)\nThe genetic algorithm converged on a subset size of 14 predictors. This included 5 of the 10 linear predictors, 0 of the non-linear terms and both of the terms that have an interaction effect in the model. The trends were:\n\n\n\n\n\nThe resampling and the large sample results have the same pattern until about 75 iterations, after which point they agree in some areas and disagree in others. However the genetic algorithm begins to over-fit to the predictors. The resampling results do not reflect this but the test set would have picked up the issue.\nApplying the resulting model to the large-sample set, the ROC curve is shown below along with the curves from the previous analysis.\n\n\n\n\n\nStarting from small subset sizes appeared to have helped. Would using the evaluation set to estimate the fitness have had the same results?\nROCtest &lt;- function(ind, x, y, cntrl, test) \n{\n    library(MASS)\n    ind &lt;- which(ind == 1)\n    if (length(ind) == 0) \n        return(0)\n    modFit &lt;- qda(x[, ind], y)\n    testROC &lt;- roc(test$Class, \n                   predict(modFit, \n                           test[, ind, drop = FALSE])$posterior[,1], \n                   levels = rev(levels(y)))\n    as.vector(auc(testROC))\n}\n\nset.seed(137)\nga_small &lt;- ga(type = \"binary\", \n               fitness = ROCtest, \n               min = 0, max = 1, \n               maxiter = 400, \n               population = initialSmall, \n               nBits = ncol(training) - 1, \n               names = names(training)[-ncol(training)], \n               x = training[, -ncol(training)], \n               y = training$Class, \n               cntrl = ctrl, \n               test = testing, \n               keepBest = TRUE, \n               parallel = FALSE)\nThis GA showed:\n\n\n\n\n\nThe pattern is very similar to the previous GA run (where resampling was used).\nGiven that the resampling process is susceptible to over-fitting, how can we penalize the results based on the size of the subset? The idea of penalization/regularization is pretty common in statistics and machine learning. The most commonly known measure is the Akaike information criterion (AIC) which takes the objective function that is being optimized (e.g. RMSE or likelihood) and penalizes it based on the sample size and number of parameters. That’s not very straight-forward here. First, it is very model dependent. In many cases, the number of parameters is not a useful concept. What would be use for tree-based models? In ensemble models, there may be more parameters than data points. What is the objective function? I’ve been trying to optimize the area under the ROC curve, but there is not theoretical connection between the ROC curve and QDA. QDA can be motivated by Bayes’ Rule and, if a probability distribution is specified, it is for the predictor data.\nOne less theoretical solution uses desirability functions. This technique is used to blend several outcomes into a single measure of desirability. In our case, I want to maximize the area under the ROC curve but minimize the number of predictors in the model. The desirability function first defines curves that translates both of these characteristics to a [0, 1] scale where zero is unacceptable and one is desirable. Any curve can do. I’ll use the parameterization created by Derringer and Suich that defines high and low thresholds for each characteristic and linearizes the desirability in-between those values:\nlibrary(desirability)\n## The best ROC value is one, 0.5 is the worst\ndROC &lt;- dMax(low = 0.5, high = 1)\n## The 'best' possible model would have a single \n## predictor and the worst would have everything\ndPreds &lt;- dMin(low = 1, high = ncol(training) - 1)\n## Show them:\npar(mfrow = c(1, 2))\nplot(dROC, nonInform = FALSE)\ntitle(\"Area Under the ROC Curve\")\nplot(dPreds, nonInform = FALSE)\ntitle(\"Subset Size\")\n\n\n\n\n\nThe ROC desirability curve is maximized when the area under the curve is high while the subset size curve is most desirable for small values. I can adjust the thresholds a several different ways. For example, I may only want subset sizes less than 100. If I move the high value to 100, any solution with less than 100 predictors would be equally acceptable. Another modification is to allow the curves to bend to make each characteristic easier or more difficult to satisfy. The SAS application JMP has a nice tool for configuring desirability functions (obviously, I used an R package above to create them).\nThe overall desirability is the geometric mean of the individual desirabilites. For two inputs, I multiply them together and take the square root. Since I’ve multiplied the values together, one consequence is that the overall desirability is unacceptable if any one of the individual values is unacceptable (i.e. a value of zero). This can be avoided by adjusting the individual curve to always be higher than a desirability of zero.\nWe can then maximize the overall desirability and hope that the GA can find a balance between performance and sparsity. I’ll define a new fitness function that uses overall desirability as measured with the resampled estimate of the area under the ROC curve:\nDcv &lt;- function(ind, x, y, cntrl) \n{\n    library(caret)\n    library(MASS)\n    library(desirability)\n    ind &lt;- which(ind == 1)\n    if (length(ind) == 0) return(0)\n    out &lt;- train(x[, ind], y, \n                 method = \"qda\", \n                 metric = \"ROC\", \n                 trControl = cntrl)\n    rocVal &lt;- caret:::getTrainPerf(out)[, \"TrainROC\"]\n    dROC &lt;- dMax(0.5, 1)\n    dPreds &lt;- dMin(1, ncol(x))\n    ## Comnined the two with a geometirc mean\n    allD &lt;- dOverall(dROC, dPreds)\n    ## Calculate the overall desirability value\n    predict(allD, data.frame(ROC = rocVal, NumPred = length(ind)))\n}\nAny I will once again use the GA package to search the predictor space:\nset.seed(137)\nga_D &lt;- ga(type = \"binary\", \n           fitness = Dcv, \n           min = 0, max = 1, \n           maxiter = 500, \n           population = initialSmall, \n           nBits = ncol(training) - 1, \n           names = names(training)[-ncol(training)], \n           x = training[, -ncol(training)], \n           y = training$Class, \n           cntrl = ctrl, \n           keepBest = TRUE, \n           parallel = FALSE)\nHere are the profiles for the three estimates of desirability (symbols sizes again indicate the subset size):\n\n\n\n\n\nThe first pattern to note is that all three estimates are strongly correlated. There seems to be negligable evidence of selection bias creeping in as before. The GA converged to a fairly small subset size. The genetic algorithm converged on a subset size of 8 predictors. This included 5 of the 10 linear predictors, none of the non-linear terms, both of the terms that have an interaction effect in the model and 1 irrelavant predictor.\n\n\n\n\n\nn terms of the area under the ROC curve, the GA was able to produce pretty competitive performance:\nfinalDVars &lt;- ga_D@bestBinary[[length(ga_D@bestBinary)]]\nfinalDFit &lt;- qda(training[, finalDVars], training$Class)\nfinalDLarge &lt;- roc(large$Class, \n                   predict(finalDFit, \n                           large[, finalDVars])$posterior[, 1],\n                   levels = rev(levels(large$Class)))\nfinalDLarge\n\n## \n## Call:\n## roc.default(response = large$Class, predictor = predict(finalDFit,\n##     large[, finalDVars])$posterior[, 1], levels = rev(levels(large$Class)))\n## \n## Data: predict(finalDFit, large[, finalDVars])$posterior[, 1] in 4640 controls (large$Class Class2) &lt; 5360 cases (large$Class Class1).\n## Area under the curve: 0.93\nThe resampling results are slightly optimistic and the test set is slightly pessimistic. The large-sample estimate of the area under the ROC curve is 0.755, which is not as good as the true model (0.931) but better than the worst-case scenario (0.52). The ROC curves are:\n\n\n\n\n\nSo this approach is yielding near-optimal results.\nAs I did in the last post, I’m compelled to throw a wet blanket on all of this. Last time, I showed that, for these data, the RFE procedure was more effective than the GA. With the two adjustments I made to the GA, it has the performance edge. What if I were to use a classification model with built-in feature selection? One such approach is the Flexible Discriminant Model (FDA). FDA is a generalizes of linear discriminant analysis that can produce non-linear class boundaries. It does this using a framework that generalizing the basis functions that can be used. One approach is to use Multivariate Adaptive Regression Splines (MARS) to fit the model. MARS is a regression model that has one quality in common with tree-based models; MARS chooses one or more predictor to “split” on. Multiple splits are used to model different predictors, but if a predictor was never used in a split, the class boundaries are functionally independent of that predictor. So, FDA simultaneously builds a classification model while conducting feature selection. This is computationally advantageous (less models are being fit), less likely to over-fit and ties the feature selection process directly to model performance.\nI fit an FDA model with MARS basis functions. There are two tuning parameters. First, the degree of the model indicates the maximum number of predictors that can be used in a split. I’ll evaluate only first or second degree models. The other parameter is the number of retained terms. MARS does a forward stage of splitting then, like tree models, prunes the model terms. The nprune parameter controls how far MARS can reduce the complexity of the model.\nI’ll use caret package’s train function again with the same cross-validation scheme:\nfdaModel &lt;- train(Class ~ ., \n                  data = training, \n                  method = \"fda\", \n                  tuneGrid = expand.grid(.nprune = 2:30,.degree = 1:2), \n                  metric = \"ROC\", \n                  trControl = ctrl)\nfdaTest &lt;- roc(testing$Class, \n               predict(fdaModel, testing, type = \"prob\")[, 1], \n               levels = rev(levels(testing$Class)))\nfdaTest\n\n## \n## Call:\n## roc.default(response = testing$Class, predictor = predict(fdaModel,\n##     testing, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n## \n## Data: predict(fdaModel, testing, type = \"prob\")[, 1] in 219 controls (testing$Class Class2) &lt; 281 cases (testing$Class Class1).\n## Area under the curve: 0.947\n\nfdaLarge &lt;- roc(large$Class, \n                predict(fdaModel, large, type = \"prob\")[, 1],\n                levels = rev(levels(testing$Class)))\nfdaLarge\n\n## \n## Call:\n## roc.default(response = large$Class, predictor = predict(fdaModel,\n##     large, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n## \n## Data: predict(fdaModel, large, type = \"prob\")[, 1] in 4640 controls (large$Class Class2) &lt; 5360 cases (large$Class Class1).\n## Area under the curve: 0.945\nThe resampling profile for this model was:\n\n\n\n\n\nThe model used additive functions of the predictor data (which may result in an interpretable model). The ROC curves:\n\n\n\n\n\nThe results indicate that a single FDA model does better than the best possible QDA model and the model fitting process was much model simplistic and straight-forward for these data. This may not always be true. This simulation system has non-linear terms that QDA should not be able to model (and FDA/MARS can), so it is not a completely fair comparison.\nThe code for these analyses can be found here.\nThe next post in this series looks at another wrapper-based feature selection algorithm: particle swarm optimization.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/exercises-and-solutions/index.html",
    "href": "posts/exercises-and-solutions/index.html",
    "title": "Exercises and Solutions",
    "section": "",
    "text": "Kjell and I are putting together solutions to the exercise sections. It is a lot of work, so it may take some time.\nIf you are teaching a class and are interested in using the book for a class, please drop me an email (mxkuhn@gmail.com). We would like to hear more about possible specifics. For example, we would like feedback on the format (e.g. pdf or web page), availability (e.g. password protected website?) or any other aspect.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/equivocal-zones/index.html",
    "href": "posts/equivocal-zones/index.html",
    "title": "Equivocal Zones",
    "section": "",
    "text": "In Chapter 11, equivocal zones were briefly discussed. The idea is that some classification errors are close to the probability boundary (i.e. 50% for two class outcomes). If this is the case, we can create a zone where we the samples are predicted as “equivocal” or “indeterminate” instead of one of the class levels. This only works if the model does not incorrectly classify samples with complete (but wrong) confidence.\nIn molecular diagnostics, many assays are required to have these zones for FDA approval and great care goes into their determination. If the assay returns an equivocal result, a recommendation would be made to repeat the assay or to obtain another sample.\nDoes this actually work with a classification model? How would we do it in R?\nTo illustrate this, I will use the two-class simulation system outlined [here](../benchmarking-machine-learning-models-using-simulation/.\nFor example:\nlibrary(caret)\n \nset.seed(933)\ntraining &lt;- twoClassSim(n = 1000)\ntesting  &lt;- twoClassSim(n = 1000)\nLet’s fit a random forest model using the default tuning parameter value to get a sense of the baseline performance. I’ll calculate a set of different classification metrics: the area under the ROC curve, accuracy, Kappa, sensitivity and specificity.\np &lt;- ncol(training) - 1\n \nfiveStats &lt;- function(...) c(twoClassSummary(...), defaultSummary(...))\n \nctrl &lt;- trainControl(method = \"cv\",\n                     summaryFunction = fiveStats,\n                     classProbs = TRUE)\n \nset.seed(721)\nrfFit &lt;- train(Class ~ ., data = training,\n                method = \"rf\",\n                metric = \"ROC\",\n                tuneGrid = data.frame(.mtry = floor(sqrt(p))),\n                ntree = 1000,\n                trControl = ctrl)\n\nrfFit\n## 1000 samples\n##   15 predictors\n##    2 classes: 'Class1', 'Class2' \n## \n## No pre-processing\n## Resampling: Cross-Validation (10 fold) \n## \n## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... \n## \n## Resampling results\n## \n##   ROC    Sens   Spec   Accuracy  Kappa  ROC SD  Sens SD  Spec SD  Accuracy SD\n##   0.924  0.856  0.817  0.837     0.673  0.0329  0.0731   0.0561   0.0536     \n##   Kappa SD\n##   0.107   \n## \n## Tuning parameter 'mtry' was held constant at a value of 3                \nWe will fit the same model but performance is only measured using samples outside the zone:\neZoned &lt;- function(...)\n{\n  ## Find points within 0.5 +/- zone\n  buffer &lt;- .10\n  preds &lt;- list(...)[[1]]\n  inZone &lt;- preds$Class1 &gt; (.5 - buffer) & preds$Class1 &lt; (.5 + buffer)\n  preds2 &lt;- preds[!inZone,]\n  c(twoClassSummary(preds2, lev = levels(preds2$obs)), \n    defaultSummary(preds2),\n    ## We should measure the rate in which we do not make \n    ## a prediction. \n    Reportable = mean(!inZone))\n}\n \nctrlWithZone &lt;- trainControl(method = \"cv\",\n                             summaryFunction = eZoned,\n                             classProbs = TRUE)\nset.seed(721)\nrfEZ &lt;- train(Class ~ ., data = training,\n              method = \"rf\",\n              metric = \"ROC\",\n              tuneGrid = data.frame(.mtry = floor(sqrt(p))),\n              ntree = 1000,\n              trControl = ctrlWithZone)\n     \nrfEZ     \n## 1000 samples\n##   15 predictors\n##    2 classes: 'Class1', 'Class2' \n## \n## No pre-processing\n## Resampling: Cross-Validation (10 fold) \n## \n## Summary of sample sizes: 900, 900, 900, 900, 900, 900, ... \n## \n## Resampling results\n## \n##   ROC   Sens   Spec   Accuracy  Kappa  Reportable  ROC SD  Sens SD  Spec SD\n##   0.96  0.917  0.891  0.905     0.808  0.767       0.024   0.0483   0.0668 \n##   Accuracy SD  Kappa SD  Reportable SD\n##   0.0507       0.102     0.0655       \n## \n## Tuning parameter 'mtry' was held constant at a value of 3\nSo by failing to predict about 23% of the samples, we were able to achieve a good boost in performance. What would happen if we change the zone size? The same procedure what used with zones up to +/- 0.14 and here are the results for the various metrics:\n\n\n\n\n\nThere is an improvement in each of the measures as long as we are willing to accept an increasing proportion of indeterminate results. Does this replicate in the test set?\nrfPred &lt;- predict(rfFit, testing)\nrfProb &lt;- predict(rfFit, testing, type = \"prob\")[, \"Class1\"]\nrfPred &lt;- data.frame(obs = testing$Class,\n                     pred = rfPred,\n                     Class1 = rfProb)\n \nfiveStats(rfPred, lev = levels(rfPred$obs))\neZoned(rfPred, lev = levels(rfPred$obs))\nfiveStats(rfPred, lev = levels(rfPred$obs))\n##       ROC      Sens      Spec  Accuracy     Kappa \n## 0.9378583 0.8518519 0.8543478 0.8530000 0.7047244 \neZoned(rfPred, lev = levels(rfPred$obs))\n##        ROC       Sens       Spec   Accuracy      Kappa Reportable \n##  0.9678054  0.9361702  0.9239437  0.9305913  0.8601139  0.7780000 \n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/downsampling-using-random-forests/index.html",
    "href": "posts/downsampling-using-random-forests/index.html",
    "title": "Down-Sampling Using Random Forests",
    "section": "",
    "text": "We discuss dealing with large class imbalances in Chapter 16. One approach is to sample the training set to coerce a more balanced class distribution. We discuss\n\ndown-sampling: sample the majority class to make their frequencies closer to the rarest class.\nup-sampling: the minority class is resampled to increase the corresponding frequencies\nhybrid approaches: some methodologies do a little of both and possibly impute synthetic data for the minority class. One such example is the SMOTE procedure.\n\nHere is an image from the book that shows the results of sampling a simulated data set:\n\n\n\n\n\nThe down-side to down-sampling is that information in the majority classes is being thrown away and this situation becomes more acute as the class imbalance becomes more severe.\nRandom forest models have the ability to use down-sampling without data loss. Recall that random forests is a tree ensemble method. A large number of bootstrap samples are taken form the training data and a separate unpruned tree is created for each data set. This model contains another feature that randomly samples a subset of predictors at each split to encourage diversity of the resulting trees. When predicting a new sample, a prediction is produced by every tree in the forest and these results are combined to generate a single prediction for an individual sample.\nRandom forests (and bagging) use bootstrap sampling. This means that if there are n training set instances, the resulting sample will select n samples with replacement. As a consequence, some training set samples will be selected more than once.\nTo incorporate down-sampling, random forest can take a random sample of size c*nmin, where c is the number of classes and nmin is the number of samples in the minority class. Since we usually take a large number of samples (at least 1000) to create the random forest model, we get many looks at the data in the majority class. This can be very effective.\nThe R package for the book contains scripts to reproduce almost of the analyses in the text. We mistakenly left out the code to down-sample random forests. I’ll demonstrate it here with a simulated data set and then show code for the caravan policy data use din the chapter.\nLet’s create simulated training and test sets using this method:\n\n\n\n\n\n## Simulate data sets with a small event rate\nset.seed(1)\ntraining &lt;- twoClassSim(500, intercept = -13)\ntesting &lt;- twoClassSim(5000, intercept = -13)\n \ntable(training$Class)\n##  \n##  Class1 Class2 \n##     428     72 \n \nnmin &lt;- sum(training$Class == \"Class2\")\nnmin\n##  [1] 72\nNow we will train two random forest models: one using down-sampling and another with the standard sampling procedure. The area under the ROC curve will be used to quantify the effectiveness of each procedure for these data.\nctrl &lt;- trainControl(method = \"cv\",\n                     classProbs = TRUE,\n                     summaryFunction = twoClassSummary)\n\nset.seed(2)\nrfDownsampled &lt;- train(Class ~ ., data = training,\n                       method = \"rf\",\n                       ntree = 1500,\n                       tuneLength = 5,\n                       metric = \"ROC\",\n                       trControl = ctrl,\n                       ## Tell randomForest to sample by strata. Here, \n                       ## that means within each class\n                       strata = training$Class,\n                       ## Now specify that the number of samples selected\n                       ## within each class should be the same\n                       sampsize = rep(nmin, 2))\n\n\nset.seed(2)\nrfUnbalanced &lt;- train(Class ~ ., data = training,\n                      method = \"rf\",\n                      ntree = 1500,\n                      tuneLength = 5,\n                      metric = \"ROC\",\n                      trControl = ctrl)\nNow we can compute the test set ROC curves for both procedures:\ndownProbs &lt;- predict(rfDownsampled, testing, type = \"prob\")[,1]\ndownsampledROC &lt;- roc(response = testing$Class, \n                      predictor = downProbs,\n                      levels = rev(levels(testing$Class)))\n\nunbalProbs &lt;- predict(rfUnbalanced, testing, type = \"prob\")[,1]\nunbalROC &lt;- roc(response = testing$Class, \n                predictor = unbalProbs,\n                levels = rev(levels(testing$Class)))\nAnd finally, we can plot the curves and determine the area under each curve:\nplot(downsampledROC, col = rgb(1, 0, 0, .5), lwd = 2)\n## Call:\n## roc.default(response = testing$Class, predictor = downProbs, \n##    levels = rev(levels(testing$Class)))\n## \n## Data: downProbs in 701 controls (testing$Class Class2) &lt; 4299 cases (testing$Class Class1).\n## Area under the curve: 0.9503\n## \n\nplot(unbalROC, col = rgb(0, 0, 1, .5), lwd = 2, add = TRUE)\n## Call:\n## roc.default(response = testing$Class, predictor = unbalProbs, levels = rev(levels(testing$Class)))\n## \n## Data: unbalProbs in 701 controls (testing$Class Class2) &lt; 4299 cases (testing$Class Class1).\n## Area under the curve: 0.9242\n\nlegend(.4, .4,\n       c(\"Down-Sampled\", \"Normal\"),\n       lwd = rep(2, 1), \n       col = c(rgb(1, 0, 0, .5), rgb(0, 0, 1, .5)))\nThis demonstrates an improvement using the alternative sampling procedure.\nOne last note about this analysis. The cross-validation procedure used to tune the down-sampled random forest model is likely to give biased results. If a single down-sampled data set is fed to the cross-validation procedure, the resampled performance estimates will probably be optimistic (since the unbalance was not present). In the analysis shown here, the resampled area under the ROC curve was overly pessimistic:\ngetTrainPerf(rfDownsampled)\n##    TrainROC TrainSens  TrainSpec method\n## 1 0.8984348         1 0.07142857     rf\n\nauc(downsampledROC)\n## Area under the curve: 0.9503\nFor the caravan data in Chapter 16, this code can be used to fit the same model:\nset.seed(1401)\nrfDownInt &lt;- train(CARAVAN ~ ., data = trainingInd,\n                   method = \"rf\",\n                   ntree = 1500,\n                   tuneLength = 5,\n                   strata = training$CARAVAN,\n                   sampsize = rep(sum(training$CARAVAN == \"insurance\"), 2),\n                   metric = \"ROC\",\n                   trControl = ctrl)\nevalResults$RFdownInt &lt;- predict(rfDownInt, evaluationInd, type = \"prob\")[,1]\ntestResults$RFdownInt &lt;- predict(rfDownInt, testingInd, type = \"prob\")[,1]\nrfDownIntRoc &lt;- roc(evalResults$CARAVAN,\n                    evalResults$RFdownInt,\n                    levels = rev(levels(training$CARAVAN)))\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/datacamp-course-update/index.html",
    "href": "posts/datacamp-course-update/index.html",
    "title": "DataCamp Course [UPDATE]",
    "section": "",
    "text": "Zachary Deane-Mayer, who collaborates on caret, has put together a DataCamp course on Machine Learning in R.\nZach and DataCamp did a great job of developing a course that is just right for people who are relatively new to R.\nThe really cool thing about the course is that their system lets you execute the R code as the instructors walk you through it (on their system). If you are taking the class on your work machine and can’t easily get R, this takes all the burden of getting an install together. You can just focus on the code and the reasons why you might approach a problem in that way.\n(Disclosure: I contributed some videos to the course but I don’t make any money from it or DataCamp)\n[update] As much as I’d love recommend Zach’s work, please avoid using DataCamp because of the sexual assult issues and cover up.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/confidence-in-prediction/index.html",
    "href": "posts/confidence-in-prediction/index.html",
    "title": "Confidence in Prediction",
    "section": "",
    "text": "A few colleagues have just published a paper on measuring the confidence in prediction in regression models (“Interpretable, Probability-Based Confidence Metric for Continuous Quantitative Structure-Activity Relationship Models”). The idea is related to applicability domains: the region of the predictor space were the model can create reliable predictions.\nHistorically, the primary method for computing the applicability domain was to judge the similarity of new samples to the training set to characterize if the model would need to extrapolate for these samples. That doesn’t take into account the training set outcomes or, more importantly, any regions inside the training set space where the model does not fit the data well.\nThe approach that this paper takes is to create a local root mean squared error that is weighted by the distance of the new sample to the nearest training set neighbors (inspired by the approach taken by Quinlan (1993) for instance-based corrections in Cubist).\nWe examine confidence in prediction methods in our book in the section “When Should You Trust Your Model’s Prediction?”\nSource: http://pubs.acs.org/doi/abs/10.1021/ci300554t\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/comparing-different-species-of-crossvalidation/index.html",
    "href": "posts/comparing-different-species-of-crossvalidation/index.html",
    "title": "Comparing Different Species of Cross-Validation",
    "section": "",
    "text": "This is the first of two posts about the performance characteristics of resampling methods. I just had major shoulder surgery, but I’ve pre-seeded a few blog posts. More will come as I get better at one-handed typing.\nFirst, a review:\nThere are quite a few methods for resampling. Here is a short summary (more in Chapter 4 of the book):\nWhich one should you use? It depends on the data set size and a few other factors. We statisticians tend to think about the operating characteristics of these procedures. For example, each of the methods above can be characterized in terms of their bias and precision.\nSuppose that you have a regression problem and you are interested in measuring RMSE. Imagine that, for your data, there is some “true” RMSE value that a particular model could achieve. The bias is the difference between what the resampling procedure estimates your RMSE to be for that model and the true RMSE. Basically, you can think of it as accuracy of estimation. The precision measures how variable the result is. Some types of resampling have higher bias than others and the same is true for precision.\nImagine that the true RMSE is the target we are trying to hit and suppose that we have four different types of resampling. This graphic is typically used when we discuss accuracy versus precision.\nClearly we want to be in the lower right.\nGenerally speaking, the bias of a resampling procedure is thought to be related to how much data is held out. If you hold-out 50% of your data using 2-fold CV, the thinking is that your final RMSE estimate will be more biased than one that held out 10%. On the other hand, the conventional wisdom is that holding less data out decreases precision since each hold-out sample has less data to get a stable estimate of performance (i.e. RMSE).\nI ran some simulations to evaluate the precision and bias of these methods. I simulated some regression data (so that I know the real answers and compute the true estimate of RMSE). The model that I used was random forest with 1000 trees in the forest and the default value of the tuning parameter. I simulated 100 different data sets with 500 training set instances. For each data set, I also used each of the resampling methods listed above 25 times using different random number seeds. In the end, we can compute the precision and average bias of each of these resampling methods.\nI won’t show the distributions of the precision and bias values across the simulations but use the median of these values. The median represents the distributions well and are simpler to visualize."
  },
  {
    "objectID": "posts/comparing-different-species-of-crossvalidation/index.html#question-1a-and-1b-how-do-the-variance-and-bias-change-in-basic-cv-also-is-it-worth-repeating-cv",
    "href": "posts/comparing-different-species-of-crossvalidation/index.html#question-1a-and-1b-how-do-the-variance-and-bias-change-in-basic-cv-also-is-it-worth-repeating-cv",
    "title": "Comparing Different Species of Cross-Validation",
    "section": "Question 1a and 1b: How do the variance and bias change in basic CV? Also, Is it worth repeating CV?",
    "text": "Question 1a and 1b: How do the variance and bias change in basic CV? Also, Is it worth repeating CV?\nFirst, let’s look at how the precision changes over the amount of data held-out and the training set size. We use the variance of the resampling estimates to measure precision.\nFirst, a value of 5 on the x-axis is 5-fold CV and 10 is 10-fold CV. Values greater than 10 are repeated 10-fold (i.e. a 60 is six repeats of 10-fold CV). For on the left-hand side of the graph (i.e. 5-fold CV), the median variance is 0.019. This measures how variable 5-fold CV is across all the simulated data sets.\n There probably isn’t any surprise here: if your measure additional replicates, the measured variance goes down. At some point the variance will level off but we are still gaining precision by repeating 10-fold CV more than once. Looking at the first two data points on the (single 5-fold and 10-fold CV), the reduction in variance is probably due to how much is being left out (10% versus 80%) as well as the number of resamples (5 versus 10).\nWhat about bias? The conventional wisdom is that the bias should be better for the 10-fold CV replicates since less is being left out in those cases. Here are the results:\n\n\n\n\n\nFrom this, 5-fold CV is pessimistically biased and that bias is reduced by moving to 10-fold CV. Perhaps it is within the noise, but it would also appear that repeating 10-fold CV a few times can also marginally reduce the bias."
  },
  {
    "objectID": "posts/comparing-different-species-of-crossvalidation/index.html#question-2a-and-2b-how-does-the-amount-held-back-affect-lgocv-is-it-better-than-basic-cv",
    "href": "posts/comparing-different-species-of-crossvalidation/index.html#question-2a-and-2b-how-does-the-amount-held-back-affect-lgocv-is-it-better-than-basic-cv",
    "title": "Comparing Different Species of Cross-Validation",
    "section": "Question 2a and 2b: How does the amount held back affect LGOCV? Is it better than basic CV?",
    "text": "Question 2a and 2b: How does the amount held back affect LGOCV? Is it better than basic CV?\nLooking at the leave-group-out CV results, the variance analysis shows an interesting pattern:\n\n\n\n\n\nVisually at least, it appears that the amount held-out has a slightly a bigger influence on the variance of the results than the number of times that the process is repeated. Leaving more out buys you better individual resampled RMSE values (i.e. more precision).\nThat’s one side of the coin. What about the bias?\n From this, LGOCV is overly pessimistic as you increase the amount held out. This could be because, with less data used to training the model, the less substrate random forest has to create an accurate model. It is hard to say why the bias didn’t flatten out towards zero when small amounts of data are left out.\nAlso, the number of held-out data sets doesn’t appear to reduce the bias.\nOne these results alone, if you use LGOCV try to leave a small amount out (say 10%) and do a lot of replicates to control the variance. But… why not just do repeated 10-fold CV?\nWe have simulations where both LGOCV and 10-fold CV left out 10%. We can do a head-to-head comparison of these results to see which procedure seems to work better. Recall that the main difference between these two procedures is that repeated 10-fold CV splits the hold-out data points evenly within a fold. LGOCV just randomly selects samples each time. In ease case, the same training set sample will show up in more than one of the hold-out data sets so the difference is more about configuration of samples.\nHere are the variance curves:\n\n\n\n\n\nThat seems pretty definitive: all other things being equal, you gain about a log unit of precision using repeated 10-fold CV instead of LGOCV with a 10% hold-out.\nThe bias curves show a fair amount of noise (keeping in mind the scale of this graph compared to the other bias images above):\n\n\n\n\n\nI would say that there is no real difference in bias and expected this prior to seeing the results. We are always leaving 10% behind and, if this is what drives bias, the two procedures should be about the same.\nSo my overall conclusion, so far, is that repeated 10-fold CV is the best in terms of variance and bias. As always, caveats apply. For example, if you have a ton of data, the precision and bias of 10- or even 5-fold CV may be acceptable. Your mileage may vary.\nThe next post will look at:\n\nthe variance and bias of the nominal bootstrap estimate\na comparison of repeated 10-fold CV to the bootstrap\nthe out-of-bag estimate of RMSE from the individual random forest model and how it compares to the other procedures.\n\nEDIT: based on the comments, here is one of the simulation files. I broke them up to run in parallel on our grid but they are all the same (except the seeds). Here is the markdown file for the post if you want the plot code or are interested to see how I summarized the results.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/caret-webinar-on-feb-25/index.html",
    "href": "posts/caret-webinar-on-feb-25/index.html",
    "title": "caret webinar on Feb 25",
    "section": "",
    "text": "I”ll be doing a webinar with the Orange County R User Group on the caret package on Tue, Feb 25, 2014 1:00 PM - 2:00 PM EST.\nHere is the url in case you are interested: https://www3.gotomeeting.com/register/673845982\nThanks to Ray DiGiacomo for setting this up.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/caret-package-plans/index.html",
    "href": "posts/caret-package-plans/index.html",
    "title": "caret package plans",
    "section": "",
    "text": "A few people have asked if anything is going to happen to caret now that I’m working at RStudio.\nThe answer is “very little will be different”.\nMore specifically:\n\ncaret will be around for a long time. I’m still using it for things too!\nThere will still be hills and valleys for development/support. That pattern already exists and will continue since I still have a “day job”.\nAs for new things coming out of Rstudio, there is likely to be a fair amount of overlap with caret (in terms of functionality). caret may be the most untidy package out there; just take a look at any call to train or the other modeling functions. There wont be a caret2 but probably a set of packages that overlap with caret in different ways (I thought about trolling and calling the first package carrot).\ncaret is fairly focused (myopic?) since it concentrates on prediction. There are similar things that people might want to do where the quantities of interest might not be prediction of a number or class. For example, while you might want to make inferences on your linear mixed model, you might also want to report your resampled RMSE to prove that your model has some fidelity to the actual data.\nThere are some features that I want(ed) for caret that would be invasive changes or might break backwards compatibility. Those are more likely to end up in a new package.\nWhile you might now want a tidyverse solution to modeling (and that’s fine), there will be some features in new pacakges that will be incentives. It is easier to add new stuff in a new system than to redefine the old. Hopefully, I’ve learned my lessons in regards to extensibility; while the user may not realize it, the innards of caret have been refactored more than once.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/calibration-affirmation/index.html",
    "href": "posts/calibration-affirmation/index.html",
    "title": "Calibration Affirmation",
    "section": "",
    "text": "In the book, we discuss the notion of a probability model being “well calibrated”. There are many different mathematical techniques that classification models use to produce class probabilities. Some of values are “probability-like” in that they are between zero and one and sum to one. This doesn’t necessarily mean that the probability estimates are consistent with the true event rate seen with similar samples. Suppose a doctor told you that you have a model predicts that you have a 90% chance of having cancer. Is that number consistent with the actual likelihood of you being sick?\nAs an example, we can look at the cell image segmentation data use in the text (from this paper). The data are in the caret package. The outcome is whether or not a cell in an image is well segmented (WS) or poorly segments (PS). We try to predict this using measured characteristics of the supposed cell.\nWe can fit a partial least squares discriminant analysis model to these data. This model can use the basic PLS model for regression (based on binary dummy variables for the classes) and we can post-process the data into class probabilities and class predictions. One method to do this is the softmax function. This simple mathematical formula can used but, in my experience, doesn’t lead to very realistic predictions. Let’s fit that model:\nlibrary(caret)\ndata(segmentationData)\n \n## Retain the original training set\nsegTrain &lt;- subset(segmentationData, Case == \"Train\")\n## Remove the first three columns (identifier columns)\nsegTrainX &lt;- segTrain[, -(1:3)]\nsegTrainClass &lt;- segTrain$Class\n \n## Test Data\nsegTest &lt;- subset(segmentationData, Case != \"Train\")\nsegTestX &lt;- segTest[, -(1:3)]\nsegTestClass &lt;- segTest$Class\n\nset.seed(1)\nuseSoftmax &lt;- train(segTrainX, segTrainClass, method = \"pls\",\n                    preProc = c(\"center\", \"scale\"),\n                    ## Tune over 15 values of ncomp using 10 fold-CV\n                    tuneLength = 15,\n                    trControl = trainControl(method = \"cv\"),\n                    probMethod = \"softmax\")\n \n## Based on cross-validation, how well does the model fit?\ngetTrainPerf(useSoftmax)\n##    TrainAccuracy TrainKappa method\n##  1        0.8108      0.592    pls\n\n## Predict the test set probabilities\nSoftmaxProbs &lt;- predict(useSoftmax, segTestX, type = \"prob\")[,\"PS\",]\n \n## Put these into a data frame:\ntestProbs &lt;- data.frame(Class = segTestClass,\n                        Softmax = SoftmaxProbs)\nThe accuracy and Kappa statistics reflect a reasonable model.\nWe are interested in knowing if cells are poorly segmented. What is the distribution of the test set class probabilities for the two classes?\nlibrary(ggplot2)\nsoftmaxHist &lt;- ggplot(testProbs, aes(x = Softmax))\nsoftmaxHist &lt;- softmaxHist + geom_histogram(binwidth = 0.02)\nsoftmaxHist+ facet_grid(Class ~ .) + xlab(\"Pr[Poorly Segmented]\")\n\n\n\n\n\nLooking at the bottom panel, the mode of the distribution is around 40%. Also, very little of the truly well segmented samples are not confidently predicted as such since most probabilities are greater than 30%. The same is true for the poorly segmented cells. Very few values are greater than 80%.\nWhat does the calibration plot look like?\nplot(calibration(Class ~ Softmax, data = testProbs), type = \"l\")\n\n\n\n\n\nThis isn’t very close to the 45 degree reference line so we shouldn’t expect the probabilities to be very realistic.\nAnother approach for PLS models is to use Bayes’ rule to estimate the class probabilities. This is a little more complicated but at least it is based on probability theory. We can fit the model this way using the option probMethod = \"Bayes\":\nset.seed(1)\nuseBayes &lt;- train(segTrainX, segTrainClass, method = \"pls\",\n                  preProc = c(\"center\", \"scale\"),\n                  tuneLength = 15,\n                  trControl = trainControl(method = \"cv\"),\n                  probMethod = \"Bayes\")\n \n## Compare models:\ngetTrainPerf(useBayes)\n##   TrainAccuracy TrainKappa method\n## 1        0.8108     0.5984    pls\ngetTrainPerf(useSoftmax)\n##   TrainAccuracy TrainKappa method\n## 1        0.8108      0.592    pls\nBayesProbs &lt;- predict(useBayes, segTestX, type = \"prob\")[,\"PS\",]\n \n## Put these into a data frame as before:\ntestProbs$Bayes &lt;- BayesProbs\nPerformance is about the same. The test set ROC curves both have AUC values of 0.87.\nWhat do these probabilities look like?\nBayesHist &lt;- ggplot(testProbs, aes(x = Bayes))\nBayesHist &lt;- BayesHist + geom_histogram(binwidth = 0.02)\nBayesHist + facet_grid(Class ~ .) + xlab(\"Pr[Poorly Segmented]\")\n\n\n\n\n\nEven though the accuracy and ROC curve results show the models to be very similar, the class probabilities are very different. Here, the most confidently predicted samples have probabilities closer to zero or one. Based on the calibration plot, is this model any better?\n\n\n\n\n\nMuch better! Looking at the right-hand side of the plot, the event rate of samples with Bayesian generated probabilities is about 97% for class probabilities between 91% and 100%. The softmax model has an event rate of zero.\nThis is a good example because, by most measures,the model performance is almost identical. However, one generates much more realistic values than the other.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/boston-r-user-group-talk-update/index.html",
    "href": "posts/boston-r-user-group-talk-update/index.html",
    "title": "Boston R User Group Talk [UPDATE]",
    "section": "",
    "text": "I’ll be giving a talk on Boston R user Group on Thursday March 10th at 6:00 PM. The talk will be on rule-based regression models.\nThe image above is the training/test set split for the data that I’ll be using the illustrate the models.\nSlides can be found here. Someone took video and I will link to that if it is posted soemwhere.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/bay-area-rug-talk-on-317-updated/index.html",
    "href": "posts/bay-area-rug-talk-on-317-updated/index.html",
    "title": "Bay Area RUG Talk on 3/17 (updated)",
    "section": "",
    "text": "I’m making my yearly pilgrimage to San Fransico to teach at PAW.\nI’ll also be giving a short talk at the Bay Area R Users Group on model tags in the caret package and the code that produced this interactive plot.\nIt is at 7:00 PM on Monday March 17th at San Francisco Marriott Marquis\nThe slide deck is here.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/asa-talk-in-new-york-this-thursday-1114/index.html",
    "href": "posts/asa-talk-in-new-york-this-thursday-1114/index.html",
    "title": "ASA Talk in New York This Thursday (11/14)",
    "section": "",
    "text": "I’ll be giving a talk on predictive modeling for the American Statistical Association next Thursday (the 14th) :\n\nPredictive Modeling: An Introduction and a Disquisition in Three Parts\nThe primary goal of predictive modeling (aka machine learning) (aka pattern recognition) is to produce the most accurate prediction for some quantity of interest. In this talk, I will give a brief introduction as well as as a discussion on three topics: the friction between interpretability and accuracy, the role of Big Data and the current unmet needs.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/a-talk-and-course-in-nyc-next-week/index.html",
    "href": "posts/a-talk-and-course-in-nyc-next-week/index.html",
    "title": "A Talk and Course in NYC Next Week",
    "section": "",
    "text": "I’ll be giving talk on Tuesday February 17 (7:00PM-9:00PM) that will be an overview of predictive modeling. It will not be highly technical and here is the current outline:\n\n“Predictive modeling” definition\nSome example applications\nA short overview and example\n\nHow is this different from what statisticians already do?\nWhat can drive choice of methodology?\nWhere should we focus our efforts?\n\nThe location is Thoughtworks NYC (99 Madison Avenue, 15th Floor).\nThe next day (Wednesday Feb 18th) I will be teaching Applied Predictive Modeling for the NYC Data Science Academy from 9:00am – 4:30pm at 205 E 42nd Street, New York, NY 10017. This will focus on R.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/2024-tidymodels-user-survey/index.html",
    "href": "posts/2024-tidymodels-user-survey/index.html",
    "title": "2024 Tidymodels User Survey",
    "section": "",
    "text": "In the tidymodels group, we re-evaluate our development roadmap after a major development epoch. In this case, we are finishing up extended support for censored data models (more on that later).\nWe always want community input into our direction. So please take a look at our tidymodels survey for 2024 priorities blog post and then take the survey.\nWhile it’s not a guarantee, we do take it into account. For example, I was not really interested in creating a stacking ensemble package. However, it was rated very high in past surveys (here’s an example of some of those results). That helped lead one of our former interns group members to make the stacks package.\nA few things are somewhat in process now, so we left those off the list. Our original list of possibilities had almost two dozen entries; we’ve whittled it down to less than ten for the survey.\n(The photo is a NASA/JPL/SSI image of the hexagon on Saturn’s north pole)"
  },
  {
    "objectID": "posts/2022-tidymodels-user-survey/index.html",
    "href": "posts/2022-tidymodels-user-survey/index.html",
    "title": "2022 tidymodels user survey",
    "section": "",
    "text": "We are conducting another survey to see where users would like us to spend our development time.\nHere’s a link to the survey!\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Predictive Modeling Blog",
    "section": "",
    "text": "This is a continuation of original blog that we made for our previous book. New posts, as of 2024, can be found here. The posts here are not associated with our employers; opinions are our own.\nOther places that we create content: the Tidyverse blog and tidymodels.org’s Learn pages.\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nData Usage with Postprocessing\n\n\n\n\n\n\n\n\n\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nPost Hoc Nearest Neighbors Prediction Adjustments\n\n\n\n\n\n\n\n\n\n\n\n2024-04-10\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary 2024 Talks\n\n\n\n\n\n\n\n\n\n\n\n2024-04-09\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Survival Analysis\n\n\n\n\n\nPredictive survival models come to tidymodels.\n\n\n\n\n\n2024-04-07\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nTwo New Preprocessing Chapters\n\n\n\n\n\n\n\n\n\n\n\n2024-03-18\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\n2024 Tidymodels User Survey\n\n\n\n\n\nTell us which features are most important to you.\n\n\n\n\n\n2024-03-04\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nWTF Article\n\n\n\n\n\n\n\n\n\n\n\n2024-03-01\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nProgress Update (February 2024)\n\n\n\n\n\n\n\n\n\n\n\n2024-02-27\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nNew Location, Same Content\n\n\n\n\n\n\n\n\n\n\n\n2024-02-26\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\n2022 tidymodels user survey\n\n\n\n\n\n\n\n\n\n\n\n2021-10-07\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels updates and voting!\n\n\n\n\n\n\n\n\n\n\n\n2020-04-27\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nSlides from R/Pharma\n\n\n\n\n\n\n\n\n\n\n\n2018-08-16\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nR/Medicine conference\n\n\n\n\n\n\n\n\n\n\n\n2018-08-15\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nPodcast on Nonclinical Statistics\n\n\n\n\n\n\n\n\n\n\n\n2018-06-30\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nEarly draft of our “Feature Engineering and Selection” book\n\n\n\n\n\n\n\n\n\n\n\n2018-05-14\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\ntidyposterior slides\n\n\n\n\n\n\n\n\n\n\n\n2018-05-04\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nNew Workshop in Washington DC (August)\n\n\n\n\n\n\n\n\n\n\n\n2018-04-10\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Resampling Redux with Agricultural Economics Data\n\n\n\n\n\n\n\n\n\n\n\n2018-03-12\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio 2018 Conference Presentation and Materials\n\n\n\n\n\n\n\n\n\n\n\n2018-03-04\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nWhile you wait for that to finish, can I interest you in parallel processing?\n\n\n\n\n\n\n\n\n\n\n\n2018-01-17\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nLots of Package News\n\n\n\n\n\n\n\n\n\n\n\n2017-12-11\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\ncaret Cheatsheet\n\n\n\n\n\n\n\n\n\n\n\n2017-09-12\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nNested Resampling with rsample\n\n\n\n\n\n\n\n\n\n\n\n2017-09-04\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nNonclinical Statistics Position in New England\n\n\n\n\n\n\n\n\n\n\n\n2017-07-27\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nDo Resampling Estimates Have Low Correlation to the Truth? The Answer May Shock You.\n\n\n\n\n\n\n\n\n\n\n\n2017-04-24\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\ncaret package plans\n\n\n\n\n\n\n\n\n\n\n\n2017-02-02\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nWorking at RStudio\n\n\n\n\n\n\n\n\n\n\n\n2016-11-28\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\n2016 UK Tour\n\n\n\n\n\n\n\n\n\n\n\n2016-09-26\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nDataCamp Course [UPDATE]\n\n\n\n\n\n\n\n\n\n\n\n2016-09-26\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nBoston R User Group Talk [UPDATE]\n\n\n\n\n\n\n\n\n\n\n\n2016-03-04\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nNonclinical Statistics Book\n\n\n\n\n\n\n\n\n\n\n\n2016-02-08\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Iowa R User Group Talk [Updated]\n\n\n\n\n\n\n\n\n\n\n\n2016-01-18\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nNonclinical Statistician Position at Pfizer\n\n\n\n\n\n\n\n\n\n\n\n2015-12-14\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nIn Search Of…\n\n\n\n\n\n\n\n\n\n\n\n2015-12-13\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nC5.0 Class Probability Shrinkage\n\n\n\n\n\n\n\n\n\n\n\n2015-09-14\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nThe 2014 Ziegel Award\n\n\n\n\n\n\n\n\n\n\n\n2015-08-12\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Engineering versus Feature Extraction: Game On!\n\n\n\n\n\n\n\n\n\n\n\n2015-08-03\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nNew caret Version (6.0-52)\n\n\n\n\n\n\n\n\n\n\n\n2015-07-22\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nSlides from recent talks\n\n\n\n\n\n\n\n\n\n\n\n2015-04-21\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nA Talk and Course in NYC Next Week\n\n\n\n\n\n\n\n\n\n\n\n2015-03-13\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nSimulated Annealing Feature Selection\n\n\n\n\n\n\n\n\n\n\n\n2015-01-12\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Solutions Available\n\n\n\n\n\n\n\n\n\n\n\n2015-01-08\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nNew Version of caret on CRAN\n\n\n\n\n\n\n\n\n\n\n\n2015-01-05\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nMy Research Tools\n\n\n\n\n\n\n\n\n\n\n\n2014-12-15\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the Bootstrap and Cross-Validation\n\n\n\n\n\n\n\n\n\n\n\n2014-12-08\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nComparing Different Species of Cross-Validation\n\n\n\n\n\n\n\n\n\n\n\n2014-12-02\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nSolutions on github\n\n\n\n\n\n\n\n\n\n\n\n2014-11-12\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nSome Thoughts on “Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?”\n\n\n\n\n\n\n\n\n\n\n\n2014-11-11\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nExercise Solutions\n\n\n\n\n\n\n\n\n\n\n\n2014-10-01\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nuseR! 2014 Highlights\n\n\n\n\n\n\n\n\n\n\n\n2014-07-03\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nNew caret version with adaptive resampling\n\n\n\n\n\n\n\n\n\n\n\n2014-05-28\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nA Tutorial and Talk at useR! 2014 [Important Update]\n\n\n\n\n\n\n\n\n\n\n\n2014-05-07\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nCross-validation pitfalls when selecting and assessing regression and classification models\n\n\n\n\n\n\n\n\n\n\n\n2014-04-10\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nBay Area RUG Talk on 3/17 (updated)\n\n\n\n\n\n\n\n\n\n\n\n2014-03-09\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\ncaret webinar materials\n\n\n\n\n\n\n\n\n\n\n\n2014-02-28\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nSample Mislabeling and Boosted Trees\n\n\n\n\n\n\n\n\n\n\n\n2014-02-18\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing Probability Thresholds for Class Imbalances\n\n\n\n\n\n\n\n\n\n\n\n2014-02-06\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\ncaret webinar on Feb 25\n\n\n\n\n\n\n\n\n\n\n\n2014-02-02\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration Affirmation\n\n\n \n\n\n\n\n\n`2014-01-04`{=html}\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nDown-Sampling Using Random Forests\n\n\n\n\n\n\n\n\n\n\n\n2013-12-08\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nASA Talk in New York This Thursday (11/14)\n\n\n\n\n\n\n\n\n\n\n\n2013-11-09\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nThe Basics of Encoding Categorical Data for Predictive Models\n\n\n\n\n\n\n\n\n\n\n\n2013-10-23\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nExercises and Solutions\n\n\n\n\n\n\n\n\n\n\n\n2013-08-30\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nAvailability\n\n\n\n\n\n\n\n\n\n\n\n2013-08-16\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nEquivocal Zones\n\n\n\n\n\n\n\n\n\n\n\n2013-08-16\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nUseR! Slides for “Classification Using C5.0”\n\n\n\n\n\n\n\n\n\n\n\n2013-07-17\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nUseR! 2013 Highlights\n\n\n\n\n\n\n\n\n\n\n\n2013-07-13\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Associations\n\n\n\n\n\n\n\n\n\n\n\n2013-06-21\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\ntype = “what”?\n\n\n\n\n\n\n\n\n\n\n\n2013-06-13\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Selection 3 - Swarm Mentality\n\n\n\n\n\n\n\n\n\n\n\n2013-06-06\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\n33 Months Later\n\n\n\n\n\n\n\n\n\n\n\n2013-05-29\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nOne Statistician’s View of Big Data\n\n\n\n\n\n\n\n\n\n\n\n2013-05-20\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nRecent Changes to caret\n\n\n\n\n\n\n\n\n\n\n\n2013-05-18\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nProjection Pursuit Classification Trees\n\n\n\n\n\n\n\n\n\n\n\n2013-05-14\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Selection 2 - Genetic Boogaloo\n\n\n\n\n\n\n\n\n\n\n\n2013-05-08\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Selection Strikes Back (Part 1)\n\n\n\n\n\n\n\n\n\n\n\n2013-04-29\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking Machine Learning Models Using Simulation\n\n\n\n\n\n\n\n\n\n\n\n2013-04-13\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nWhat the Market Will Bear\n\n\n\n\n\n\n\n\n\n\n\n2013-03-29\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research at ENAR\n\n\n\n\n\n\n\n\n\n\n\n2013-03-11\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence in Prediction\n\n\n\n\n\n\n\n\n\n\n\n2013-02-12\n\n\nMax Kuhn\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Was Left Out\n\n\n\n\n\n\n\n\n\n\n\n2013-02-10\n\n\nMax Kuhn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The main contributors to this blog are Max Kuhn and Kjell Johnson. Together, they wrote the book Applied Predictive Modeling, which won the Ziegel award from the American Statistical Association, recognizing the best book reviewed in Technometrics in 2014. They have also co-written Feature Engineering and Selection.\n\nMax Kuhn\nMax Kuhn is a software engineer at Posit PBC (formerly known as RStudio Inc), where he is working on improving R’s modeling capabilities and maintaining about 30 packages, including caret and tidymodels. He has a Ph.D. in Biostatistics.\nMax was a Senior Director of Nonclinical Statistics at Pfizer Global R&D in Connecticut. He has applied models in the pharmaceutical and diagnostic industries for over 18 years.\nHe is also the main author of Tidy Modeling with R and was the Drug Discovery Section Editor for Nonclinical Statistics for Pharmaceutical and Biotechnology Industries.\nMax can be found at:\n\nGitHub\nTwitter\nMastodon\nLinkedIn\nGoogle Scholar\n\n\n\nKjell Johnson\nKjell Johnson owns and founded StatTenacity. He has a Ph.D. in Statistics at the University of Kentucky and has contributed to drug discovery projects since 1999. He also enjoys making statistics accessible to non-statisticians. He teaches courses for scientists and statisticians alike, including short courses at the Society for Biomolecular Screening, the American Chemical Society, and the Deming Conference.\nKjell is actively involved in other statistical service activities, including past activities of chairing the Midwest Biopharmaceutical Statistics Workshop Organizing Committee in 2010 and working on the ASA’s Biopharm Executive Committee."
  },
  {
    "objectID": "posts/2016-uk-tour/index.html",
    "href": "posts/2016-uk-tour/index.html",
    "title": "2016 UK Tour",
    "section": "",
    "text": "I’ll be in the UK next week doing three talks in three days:\n\nFirst, I’ll be giving a talk at the London R-Ladies meetup on Monday October 3rd with perhaps the best title yet: Whose Scat Is That? An ‘Easily Digestible’ Introduction to Predictive Modeling and caret.\nOn Tuesday, October 4th I’m giving a talk at the Cambridge RUG on tuning hyperparameters using optimization algorithms. This is an extension of this blog post.\n\nFinally, on Wednesday (the 5th) at the fantastic Nonclinical Statistics Conference, I’ll be speaking on Statistical Mediation in Early Discovery by Bayesian Analysis and Visualization. The banner image above is from this talk. Lots of priors and shiny apps.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/2024-02-progress-update/index.html",
    "href": "posts/2024-02-progress-update/index.html",
    "title": "Progress Update (February 2024)",
    "section": "",
    "text": "Since the last update on 2023-11-20, we have a few new sections and chapters.\nThe short-term goal is to have good first drafts of all of the “Preparation” chapters. As of 2024-02-26, they are:\n\nInitial Data Splitting (drafted)\nTransforming Numeric Predictors (drafted but not published)\nWorking with Categorical Predictors (waiting for review)\nEmbeddings\n\nLinear methods (PCA, PLS, etc) not drafted\nMDS sections (waiting for review)\nOther methods (nearest shrunken centroids, etc) in progress\n\nInteractions and Nonlinear Features\n\nInteractions (waiting for review)\nSpline section (in progress, almost finished)\nDiscretization (in progress, almost finished)\n\nMissing Data (started)\n\nWe’ll update the website once the first three chapters listed above have good drafts. Another update will follow when the others are done."
  },
  {
    "objectID": "posts/33-months-later/index.html",
    "href": "posts/33-months-later/index.html",
    "title": "33 Months Later",
    "section": "",
    "text": "After starting in September 2010, the book is now available from the publisher. The Kindle edition is available and Amazon will ship hard copies around July 1st.\nOn top of that, I just sent the first version of the AppliedPredictiveModeling package to CRAN.\n&gt; library(lubridate)\n&gt; diff(ymd(c(\"2010-09-01\", \"2013-05-29\")))\n## Time difference of 1001 days\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/a-tutorial-and-talk-at-user-2014-important-update/index.html",
    "href": "posts/a-tutorial-and-talk-at-user-2014-important-update/index.html",
    "title": "A Tutorial and Talk at useR! 2014 [Important Update]",
    "section": "",
    "text": "See the update below\nI’ll be doing a morning tutorial at useR! at the end of June in Los Angeles. I’ve done this same presentation at the last few conferences and this will probably be the last time for this specific workshop.\nThe tutorial outline is:\n\nConventions in R\nData splitting and estimating performance\nData pre-processing\nOver-fitting and resampling\nTraining and tuning tree models\nTraining and tuning a support vector machine\nComparing models (as time allows)\nParallel processing (as time allows)\n\nI’m also giving a talk called “Adaptive Resampling in a Parallel World”:\n\nMany predictive models require parameter tuning. For example, a classification tree requires the user to specify the depth of the tree. This type of “meta parameter” or “tuning parameter” cannot be estimated directly from the training data. Resampling (e.g. cross-validation or the bootstrap) is a common method for finding reasonable values of these parameters (Kuhn and Johnson, 2013) . Suppose B resamples are used with M candidate values of the tuning parameters. This can quickly increase the computational complexity of the task. Some of the M models could be disregarded early in the resampling process due to poor performance. Maron and Moore (1997) and Shen el at (2011) describe methods to adaptively filter which models are evaluated during resampling and reducing the total number of model fits. However, model parameter tuning is an “embarrassingly parallel” task; model fits can be calculated across multiple cores or machines to reduce the total training time. With the availability of parallel processing is it still advantageous to adaptively resample?\n\n\nThis talk will briefly describe adaptive resampling methods and characterize their effectiveness using parallel processing via simulations.\n\nThe conference website has updated their website to say:\n\nThis year there is no separate registration process or extra fee for attending tutorials.\n\nUPDATE! Since this is the case, I won’t be giving out the book to all the attendees as I originally intended. However, the conference does supply tutorial presenters with a stipend so I will be using that to purchase about a dozen copies that I will randomly distribute to whoever attends.\nSorry for the confusion… (This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/availability/index.html",
    "href": "posts/availability/index.html",
    "title": "Availability",
    "section": "",
    "text": "After being on backorder for about 10 weeks, we are told that a larger batch of books have been printed and should be available shortly (despite the Amazon page’s note about availability).\nOn another note, we have had a few people ask about teaching materials. Kjell and I are working on solutions to the exercises and we are talking to the publisher about making more of the figures available. Right now, the majority of them can be reproduced using the code in the AppliedPredictiveModeling R package.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/benchmarking-machine-learning-models-using-simulation/index.html",
    "href": "posts/benchmarking-machine-learning-models-using-simulation/index.html",
    "title": "Benchmarking Machine Learning Models Using Simulation",
    "section": "",
    "text": "What is the objective of most data analysis? One way I think about it is that we are trying to discover or approximate what is really going on in our data (and in general, nature). However, I occasionally run into people think that if one model fulfills our expectations (e.g. higher number of significant p-values or accuracy) than it must be better than any other model that does not. For most data sets, we don’t know what the truth is, so this is a problem.\nComputational biology/bioinformatics are particularly bad in this way. In many cases, the cost, time and complexity of high dimensional biology experiments prevents a solid, methodical validation of analysis of the initial data set. This is has be verified by a number of different publications.\nI was talking to someone recently who was describing their research with ~150 samples and ~50,000 predictors. They used the same sample set to do feature selection and then build predictive models. The results was a random forest model based on about 150 predictors. The validation was based on running some of the same samples using a different technology. When I asked if there there would be any external validation, their response was “we’re never going to get a 5,000 sample clinical trial to check the results.” While true (and a bit dramatic), it is not an excuse to throw out good methodology. In fact, you would think that a lack of a clear path to validation would make people be more dogmatic about methodology…\nWhen I’m trying to evaluate any sort of statistic method, I try to use a good simulation system so that I can produce results where I know the truth. Examples are the “Friedman” simulations systems for regression modeling, such as the ‘Friedman 3’ model. This is a non-linear regression function of four real predictors:\ny = atan ((x2 x3 - (1/(x2 x4)))/x1) + error\nThe the mlbench package has this in R code as well as other simulation systems.\nI’ve been looking for a system that can be used to test a few different aspects of classification models:\n\nclass imbalances\nnon-informative predictors\ncorrelation amoung the predictors\nlinear and nonlinear signals\n\nI spent a few hours developing one. It models the log-odds of a binary event as a function of true signals using several additive “sets” of a few different types. First, there are two main effects and an interaction:\nintercept - 4A + 4B + 2AB \n(A and B are the predictors) The intercept is a parameter for the simulation and can be used to control the amount of class imbalance.\nThe second set of effects are linear with coefficients that alternate signs and have values between 2.5 and 0.025. For example, if there were six predictors in this set, their contribution to the log-odds would be\n-2.50C + 2.05D -1.60E + 1.15F -0.70G + 0.25H\nThe third set is a nonlinear function of a single predictor ranging between [0, 1] called J here:\n(J^3) + 2exp(-6(J-0.3)^2) \nI saw this in one of Radford Neal’s presentations but I couldn’t find an exact reference for it. The equation produces an interesting trend:\n\n\n\n\n\nThe fourth set of informative predictors are copied from one of Friedman’s systems and use two more predictors (K and L):\n2sin(KL)\nAll of these effects are added up to model the log-odds. This is used to calculate the probability of a sample being in the first class and a random uniform number is used to actually make the assignment of the actual class.\nWe can also add non-informative predictors to the data. These are random standard normal predictors and can be optionally added to the data in two ways: a specified number of independent predictors or a set number of predictors that follow a particular correlation structure. The only two correlation structure that I’ve implemented are\n\ncompound-symmetry (aka exchangeable) where there is a constant correlation between all the predictors\nauto-regressive 1 [AR(1)]. While there is no time component to these data, we can use this structure to add predictors of varying levels of correlation. For example, if there were 4 predictors and r (for rho) was the correlation parameter, the between predictor correlaiton matrix would be\n\n      | 1             sym   |\n      | r    1              |\n      | r^2  r    1         |\n      | r^3  r^2  r    1    |\n      | r^4  r^3  r^2  r  1 |\nFor AR(1), correlations decrease as the predictors are “father away” from each other (in order). Simulating ten predictors (named Corr01 - Corr10) with a correlation parameter of 0.75 yields the following between-predictor correlation structure:\n\n\n\n\n\nTo demonstrate, let’s take a set of data and see how a support vector machine performs:\nset.seed(468)\ntraining &lt;- twoClassSim(  300, noiseVars = 100, \n                        corrVar = 100, corrValue = 0.75)\ntesting  &lt;- twoClassSim(  300, noiseVars = 100, \n                        corrVar = 100, corrValue = 0.75)\nlarge    &lt;- twoClassSim(10000, noiseVars = 100, \n                        corrVar = 100, corrValue = 0.75)\nThe default for the number of informative linear predictors is 10 and the default intercept of -5 makes the class frequencies fairly balanced:\ntable(large$Class)/nrow(large)\n\n## \n## Class1 Class2 \n## 0.5457 0.4543\nWe’ll use the train function to tune and train the model:\nlibrary(caret)\n\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     repeats = 3, classProbs = TRUE, \n                     summaryFunction = twoClassSummary)\n\nset.seed(1254)\nfullModel &lt;- train(Class ~ ., data = training, \n                   method = \"svmRadial\", \n                   preProc = c(\"center\", \"scale\"), \n                   tuneLength = 8, \n                   metric = \"ROC\", \n                   trControl = ctrl)\n\nfullModel\n\n## 300 samples\n## 215 predictors\n##   2 classes: 'Class1', 'Class2' \n## \n## Pre-processing: centered, scaled \n## Resampling: Cross-Validation (10 fold, repeated 3 times) \n## \n## Summary of sample sizes: 270, 270, 270, 270, 270, 270, ... \n## \n## Resampling results across tuning parameters:\n## \n##   C     ROC    Sens   Spec     ROC SD  Sens SD  Spec SD\n##   0.25  0.636  1      0        0.0915  0        0      \n##   0.5   0.635  1      0.00238  0.0918  0        0.013  \n##   1     0.644  0.719  0.438    0.0929  0.0981   0.134  \n##   2     0.68   0.671  0.574    0.0863  0.0898   0.118  \n##   4     0.69   0.673  0.579    0.0904  0.0967   0.11   \n##   8     0.69   0.673  0.579    0.0904  0.0967   0.11   \n##   16    0.69   0.673  0.579    0.0904  0.0967   0.11   \n##   32    0.69   0.673  0.579    0.0904  0.0967   0.11   \n## \n## Tuning parameter 'sigma' was held constant at a value of 0.00353\n## ROC was used to select the optimal model using  the largest value.\n## The final values used for the model were C = 4 and sigma = 0.00353.\nCross-validation estimates the best area under the ROC curve to be 0.69. Is this an accurate estimate? The test set has:\nfullTest &lt;- roc(testing$Class, \n                predict(fullModel, testing, type = \"prob\")[,1], \n                levels = rev(levels(testing$Class)))\nfullTest\n\n## \n## Call:\n## roc.default(response = testing$Class, predictor = predict(fullModel,     testing, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n## \n## Data: predict(fullModel, testing, type = \"prob\")[, 1] in 140 controls (testing$Class Class2) &lt; 160 cases (testing$Class Class1).\n## Area under the curve: 0.78\nFor this small test set, the estimate is 0.09 larger than the resampled version. How do both of these compare to our approximation of the truth?\nfullLarge &lt;- roc(large$Class, \n                 predict(fullModel, large, type = \"prob\")[, 1], \n                 levels = rev(levels(testing$Class)))\nfullLarge\n\n## \n## Call:\n## roc.default(response = large$Class, predictor = predict(fullModel,     large, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n## \n## Data: predict(fullModel, large, type = \"prob\")[, 1] in 4543 controls (large$Class Class2) &lt; 5457 cases (large$Class Class1).\n## Area under the curve: 0.733\nHow much did the presence of the non-informative predictors affect this model? We know the true model, so we can fit that and evaluate it in the same way:\nrealVars &lt;- names(training)\nrealVars &lt;- realVars[!grepl(\"(Corr)|(Noise)\", realVars)]\n\nset.seed(1254)\ntrueModel &lt;- train(Class ~ ., \n                   data = training[, realVars], \n                   method = \"svmRadial\", \n                   preProc = c(\"center\", \"scale\"), \n                   tuneLength = 8, \n                   metric = \"ROC\", \n                   trControl = ctrl)\ntrueModel\n\n## 300 samples\n##  15 predictors\n##   2 classes: 'Class1', 'Class2' \n## \n## Pre-processing: centered, scaled \n## Resampling: Cross-Validation (10 fold, repeated 3 times) \n## \n## Summary of sample sizes: 270, 270, 270, 270, 270, 270, ... \n## \n## Resampling results across tuning parameters:\n## \n##   C     ROC    Sens   Spec   ROC SD  Sens SD  Spec SD\n##   0.25  0.901  0.873  0.733  0.0468  0.0876   0.136  \n##   0.5   0.925  0.873  0.8    0.0391  0.0891   0.11   \n##   1     0.936  0.871  0.826  0.0354  0.105    0.104  \n##   2     0.94   0.881  0.852  0.0356  0.0976   0.0918 \n##   4     0.936  0.875  0.857  0.0379  0.0985   0.0796 \n##   8     0.927  0.835  0.852  0.0371  0.0978   0.0858 \n##   16    0.917  0.821  0.843  0.0387  0.11     0.0847 \n##   32    0.915  0.821  0.843  0.0389  0.11     0.0888 \n## \n## Tuning parameter 'sigma' was held constant at a value of 0.0573\n## ROC was used to select the optimal model using  the largest value.\n## The final values used for the model were C = 2 and sigma = 0.0573.\nMuch higher! Is this verified by the other estimates?\ntrueTest &lt;- roc(testing$Class, \n                predict(trueModel, testing, type = \"prob\")[, 1], \n                levels = rev(levels(testing$Class)))\ntrueTest\n\n## \n## Call:\n## roc.default(response = testing$Class, predictor = predict(trueModel,     testing, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n## \n## Data: predict(trueModel, testing, type = \"prob\")[, 1] in 140 controls (testing$Class Class2) &lt; 160 cases (testing$Class Class1).\n## Area under the curve: 0.923\n\ntrueLarge &lt;- roc(large$Class, \n                 predict(trueModel, large, type = \"prob\")[, 1], \n                 levels = rev(levels(testing$Class)))\ntrueLarge\n\n## \n## Call:\n## roc.default(response = large$Class, predictor = predict(trueModel,     large, type = \"prob\")[, 1], levels = rev(levels(testing$Class)))\n## \n## Data: predict(trueModel, large, type = \"prob\")[, 1] in 4543 controls (large$Class Class2) &lt; 5457 cases (large$Class Class1).\n## Area under the curve: 0.926\nAt this point, we might want to look and see what would happen if all 200 non-informative predictors were uncorrelated etc. At least we have a testing tool to make objective statements.\nCode to create this can be found here and will end up making its way into the caret package.\nAny suggestions for simulation systems?\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/c50-class-probability-shrinkage/index.html",
    "href": "posts/c50-class-probability-shrinkage/index.html",
    "title": "C5.0 Class Probability Shrinkage",
    "section": "",
    "text": "(The image above has nothing do to with this post. It does, however, show the prize that my son won during a recent vacation to Virginia and how I got it back home).\nI was recently asked to explain a potential disconnect in C5.0 between the class probabilities shown in the terminal nodes and the values generated by the prediction code.\nHere is an example using the iris data:\nlibrary(C50)\nmod &lt;- C5.0(Species ~ ., data = iris)\nsummary(mod)\n## Call:\n## C5.0.formula(formula = Species ~ ., data = iris)\n## \n## \n## C5.0 [Release 2.07 GPL Edition]      Tue Sep  8 12:49:43 2015\n## -------------------------------\n## \n## Class specified by attribute `outcome'\n## \n## Read 150 cases (5 attributes) from undefined.data\n## \n## Decision tree:\n## \n## Petal.Length &lt;= 1.9: setosa (50)\n## Petal.Length &gt; 1.9:\n## :...Petal.Width &gt; 1.7: virginica (46/1)\n##     Petal.Width &lt;= 1.7:\n##     :...Petal.Length &lt;= 4.9: versicolor (48/1)\n##         Petal.Length &gt; 4.9: virginica (6/2)\n## \n## \n## Evaluation on training data (150 cases):\n## \n##      Decision Tree   \n##    ----------------  \n##    Size      Errors  \n## \n##       4    4( 2.7%)   &lt;&lt;\n## \n## \n##     (a)   (b)   (c)    &lt;-classified as\n##    ----  ----  ----\n##      50                (a): class setosa\n##            47     3    (b): class versicolor\n##             1    49    (c): class virginica\n## \n## \n##  Attribute usage:\n## \n##  100.00% Petal.Length\n##   66.67% Petal.Width\n## \n## \n## Time: 0.0 secs\nSuppose that we are predicting the sample in row 130 with a petal length of 5.8 and a petal width of 1.6. From this tree, the terminal node shows “virginica (6/2)” which means a predicted class of the virginica species with a probability of 4/6 = 0.66667. However, we get a different predicted probability:\npredict(mod, iris[130,], type = \"prob\")\n##         setosa versicolor virginica\n## 130 0.04761905  0.3333333 0.6190476\nWhen we wanted to describe the technical aspects of the C5.0 and cubist models, the main source of information on these models was the raw C source code from the RuleQuest website. For many years, both of these models were proprietary commercial products and we only recently open-sourced. Our intuition is that Quinlan quietly evolved these models from the versions described in the most recent publications to what they are today. For example, it would not be unreasonable to assume that C5.0 uses AdaBoost. From the sources, a similar reweighting scheme is used but it does not appear to be the same.\nFor classifying new samples, the C sources have\nClassNo PredictTreeClassify(DataRec Case, Tree DecisionTree)\n/*      ------------  */\n{\n    ClassNo c, C;\n    double  Prior;\n\n    /*  Save total leaf count in ClassSum[0]  */\n    ForEach(c, 0, MaxClass)\n    {\n        ClassSum[c] = 0;\n    }\n\n    PredictFindLeaf(Case, DecisionTree, Nil, 1.0);\n    C = SelectClassGen(DecisionTree-&gt;Leaf, (Boolean)(MCost != Nil), ClassSum);\n\n    /*  Set all confidence values in ClassSum  */\n    ForEach(c, 1, MaxClass)\n    {\n        Prior = DecisionTree-&gt;ClassDist[c] / DecisionTree-&gt;Cases;\n        ClassSum[c] = (ClassSum[0] * ClassSum[c] + Prior) / (ClassSum[0] + 1);\n    }\n    Confidence = ClassSum[C];\n    return C;\n}\nHere:\n\nThe predicted probability is the “confidence” value\nThe prior is the class probabilities from the training set. For the iris data, this value is 1/3 for each of the classes\nThe array ClassSum is the probabilities of each class in the terminal node although ClassSum[0] is the number of samples in the terminal node (which, if there are missing values, can be fractional).\n\nFor sample 130, the virginica values are:\n  (ClassSum[0] * ClassSum[c] + Prior) / (ClassSum[0] + 1)\n= (          6 *       (4/6) + (1/3)) / (          6 + 1) \n= 0.6190476\nWhy is it doing this? This will tend to avoid class predictions that are absolute zero or one.\nBasically, it can be viewed to be similar to how Bayesian methods operate where the simple probability estimates are “shrunken” towards the prior probabilities. Note that, as the number of samples in the terminal nodes (ClassSum[0]) becomes large, this operation has less effect on the final results. Suppose ClassSum[0] = 10000, then the predicted virginica probability would be 0.6663337, which is closer to the simple estimate.\nThis is very much related to the Laplace Correction. Traditionally, we would add a value of one to the denominator of the simple estimate and add the number of classes to the bottom, resulting in (4+1)/(6+3) = 0.5555556. C5.0 is substituting the prior probabilities and their sum (always one) into this equation instead.\nTo be fair, there are well known Bayesian estimates of the sample proportions under different prior distributions for the two class case. For example, if there were two classes, the estimate of the class probability under a uniform prior would be the same as the basic Laplace correction (using the integers and not the fractions). A more flexible Bayesian approach is the Beta-Binomial model, which uses a Beta prior instead of the uniform. The downside here is that two extra parameters need to be estimated (and it only is defined for two classes)\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/caret-cheatsheet/index.html",
    "href": "posts/caret-cheatsheet/index.html",
    "title": "caret Cheatsheet",
    "section": "",
    "text": "It can be found on the RStudio cheatsheet page. Suggestions and pull requests are always welcome.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/caret-webinar-materials/index.html",
    "href": "posts/caret-webinar-materials/index.html",
    "title": "caret webinar materials",
    "section": "",
    "text": "The webinar was recorded (thanks to Ray DiGiacomo and the Orange County RUG). The slides are here minus a few typos.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/central-iowa-r-user-group-talk-updated/index.html",
    "href": "posts/central-iowa-r-user-group-talk-updated/index.html",
    "title": "Central Iowa R User Group Talk [Updated]",
    "section": "",
    "text": "I’ll be giving a talk (“Applied Predictive Modeling”) to the Central Iowa R User Group on Thursday night at 6:00 PM to 8:00 PM (CST).\nIt looks like it will be broadcast live on YouTube. The link is http://www.youtube.com/watch?v=99lnTku75Pc.\nUpdate: Here are my sldies and code for the session\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/comparing-the-bootstrap-and-crossvalidation/index.html",
    "href": "posts/comparing-the-bootstrap-and-crossvalidation/index.html",
    "title": "Comparing the Bootstrap and Cross-Validation",
    "section": "",
    "text": "This is the second of two posts about the performance characteristics of resampling methods. The first post focused on the cross-validation techniques and this post mostly concerns the bootstrap.\nRecall from the last post: we have some simulations to evaluate the precision and bias of these methods. I simulated some regression data (so that I know the real answers and compute the true estimate of RMSE). The model that I used was random forest with 1000 trees in the forest and the default value of the tuning parameter. I simulated 100 different data sets with 500 training set instances. For each data set, I also used each of the resampling methods listed above 25 times using different random number seeds. In the end, we can compute the precision and average bias of each of these resampling methods.\n\nQuestion 3: How do the variance and bias change in the bootstrap?\nFirst, let’s look at how the precision changes over the amount of data held-out and the training set size. We use the variance of the resampling estimates to measure precision.\n\n\n\n\n\nAgain, there shouldn’t be any real surprise that the variance is decreasing as the number of bootstrap samples increases.\nIt is worth noting that compared to the original motivation for the bootstrap, which as to create confidence intervals for some unknown parameter, this application doesn’t require a large number of replicates. Originally, the bootstrap was used to estimate the tail probabilities of the bootstrap distribution of some parameters. For example, if we want to get a 95% bootstrap confidence interval, we one simple approach is to accurately measure the 2.5% and 97.5% quantiles. Since these are very extremely values, traditional bootstrapping requires a large number of bootstrap samples (at least 1,000). For our purposes, we want a fairly good estimate of the mean of the bootstrap distribution and this shouldn’t require hundreds of resamples.\nFor the bias, it is fairly well-known that the naive bootstrap produces biased estimates. The bootstrap has a hold-out rate of about 63.2%. Although this is a random value in practice and the mean hold-out percentage is not affected by the number of resamples. Our simulation confirms the large bias that doesn’t move around very much (the y-axis scale here is very narrow when compared to the previous post):\n\n\n\n\n\nAgain, no surprises\n\n\nQuestion 4: How does this compare to repeated 10-fold CV?\nWe can compare the sample bootstrap to repeated 10-fold CV. For each method, we have relatively constant hold-out rates and matching configurations for the number of total resamples.\nMy initial thoughts would be that the naive bootstrap is probably more precise than repeated 10-fold CV but has much worse bias. Let’s look at the bias first this time. As predicted, CV is much less biased in comparison:\n\n\n\n\n\nNow for the variance:\n\n\n\n\n\nTo me, this is very unexpected. Based on these simulations, repeated 10-fold CV is superior in both bias and variance.\n\n\nQuestion 5: What about the OOB RMSE estimate?\nSince we used random forests as our learner we have access to yet another resampled estimate of performance. Recall that random forest build a large number of trees and each tree is based on a separate bootstrap sample. As a consequence, each tree has an associated “out-of-bag” (OOB) set of instances that were not used in the tree. The RMSE can be calculated for these trees and averaged to get another bootstrap estimate. Recall from the first post that we used 1,000 trees in the forest so the effective number of resamples is 1,000.\nWait - didn’t I say above that we don’t need very many bootstrap samples? Yes, but that is a different situation. Random forests require a large number of bootstrap samples. The reason is that random forests randomly sample from the predictor set at each split. For this reason, you need a lot of resamples to get stable prediction values. Also, if you have a large number of predictors and a small to medium number of training set instances, the number of resamples should be really large to make sure that each predictor gets a chance to influence the model sufficiently.\nLet’s look at the last two plots and add a line for the value of the OOB estimate. For variance:\n\n\n\n\n\nand bias:\n\n\n\n\n\nthe results look pretty good (especially the bias). The great thing about this is that you get this estimate for free and it works pretty well. This is consistent with my other experiences too. For example, Figure 8.18 shows that the CV and OOB error rates for the usual random forest model track very closely for the solubility data.\nThe bad news is that:\n\nThis estimate may not work very well for other types of models. Figure 8.18 does not show nearly identical performance estimates for random forests based on conditional inference trees.\nThere are a limited number of performance estimates where the OOB estimate can be computed. This is mostly due to software limitations (as opposed to theoretical concerns). For the caret package, we can compute RMSE and R2 for regression and accuracy and the Kappa statistic for classification but this is mostly by taking the output that the randomForest function provides. If you want to get the area under the ROC curve or some other measure, you are out of luck.\nIf you are comparing random forest’s OOB error rate with the CV error rate from another model, it may not be a very fair comparison.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/crossvalidation-pitfalls-when-selecting-and-assessing-regression-and-classification-models/index.html",
    "href": "posts/crossvalidation-pitfalls-when-selecting-and-assessing-regression-and-classification-models/index.html",
    "title": "Cross-validation pitfalls when selecting and assessing regression and classification models",
    "section": "",
    "text": "Damjan Krstajic and friends have a great paper on pitfalls of cross-validation. Although the paper uses chemistry data, the meat of the article is broadly applicable. It does a great job of illustrating different resampling approaches and I learned more about double and nested cross-validation.\nFigure 10 surprised me; I assumed that the precision in resampled estimates is mostly driven by the number of resamples. For example, a resampled estimate of the RMSE using 64 resamples has a standard error of sd/8 which is twice as good as one using 16 resamples (i.e. sd/4). In their work, the variation in 50 repeats of 10-fold CV are much better than 50 repeats of 10-fold nested CV.\nFinally, the article has an excellent historical summary of the pivotal papers on this subject and does a great job of labeling and articulating the different goals that one might have when resampling predictive models.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/do-resampling-estimates-have-low-correlation-to-the-truth-the-answer-may-shock-you/index.html",
    "href": "posts/do-resampling-estimates-have-low-correlation-to-the-truth-the-answer-may-shock-you/index.html",
    "title": "Do Resampling Estimates Have Low Correlation to the Truth? The Answer May Shock You.",
    "section": "",
    "text": "One criticism that is often leveled against using resampling methods (such as cross-validation) to measure model performance is that there is no correlation between the CV results and the true error rate.\nLet’s look at this with some simulated data. While this assertion is often correct, there are a few reasons why you shouldn’t care.\n\nThe Setup\nFirst, I simulated some 2-class data using this simulation system. There are 15 predictors in the data set. Many nonlinear classification models can achieve an area under the ROC curve in the low 0.90’s on these data. The training set contained 500 samples and a 125 sample test set was also simulated.\nI used a radial basis function support vector machine to model the data with a single estimate of the kernel parameter sigma and 10 values of the SVM cost parameter (on the log2 scale). The code for this set of simulations can be found here so that you can reproduce the results.\nModels were fit for each of the 10 submodels and five repeats of 10-fold cross-validation were used to measure the areas under the ROC curve. The test set results were also calculated as well as a large sample test set that approximates the truth (and is labeled as such below). All the results were calculated for all of the 10 SVM submodels (over cost). This simulation was conducted 50 times. Here is one example of how the cost parameter relates to the area under the ROC curve:\n\n\n\n\n\n\n\nThe Bad News\nWhen you look at the results, there is little to no correlation between the resampling ROC estimates and the true area under that curve:\n\n\n\n\n\nThe correlations were highest (0.54) when the cost values were low (which is also where the model performed poorly). Under the best cost value, the correlation was even worse (0.01).\nHowever, note that the 125 sample test set estimates do not appear to have a high fidelity to the true values either:\n\n\n\n\n\n\n\nThe Good News\nWe really shouldn’t care about this, or at least we are measuring the effectiveness in the wrong way. High correlation would be nice but could result in a strong relationship that does not reflect accuracy of the resampling procedure. This is basically the same argument that we make against using R2.\nLet’s look at the root mean squared error (RMSE) instead. The RMSE can be decomposed into two quantities:\n\nthe bias reflects how far the resampling estimate is from the true value (which we can measure in our simulations).\nthe variance of the resampling estimate\n\nRMSE is mostly the squared bias plus the variance.\nTwo things can be seem in the bias graph below. First, the bias is getting better as cost increases. This shouldn’t be a surprise since increasing the cost value coerces the SVM model to be more adaptive to the (training) data. Second, the bias scale is exceedingly small (since the area under the ROC curve is typically between 0.50 and 1.00). This is true even at its worst.\n\n\n\n\n\nThe standard deviation curve below shows that the model noise is minimized when performance is best and resembles an inverted version of the curve shown in the Bad News section. This is because the SVM model is pushing against the best performance. As Tolstoy said, “all good models resemble one another, each crappy model is crappy in its own way.” (actually, he did not say this). However, note the scale again. These are not large numbers.\n\n\n\n\n\nLooking at the RMSE of the model, which is the in the same units as the AUC values, the curve movies around a lot but the magnitude of the values are very low. This can obviously be affected by the size of the training set, but 500 samples is not massive for this particular simulation system.\n\n\n\n\n\nSo the results here indicate that:\n\nyes the correlation is low but\nthe overall RMSE is very good.\n\nAccuracy is arguably a much better quality to have relative to correlation.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/early-draft-of-our-feature-engineering-and-selection-book/index.html",
    "href": "posts/early-draft-of-our-feature-engineering-and-selection-book/index.html",
    "title": "Early draft of our “Feature Engineering and Selection” book",
    "section": "",
    "text": "Kjell and I are writing another book on predictive modeling, this time focused on all the things that you can do with predictors. It’s about 60% done and we’d love to get feedback. You cna take a look at http://feat.engineering and provide feedback at https://github.com/topepo/FES/issues.\nThe current TOC is:\n\nIntroduction\nIllustrative Example: Predicting Risk of Ischemic Stroke\nA Review of the Predictive Modeling Process\nExploratory Visualizations\nEncoding Categorical Predictors\nEngineering Numeric Predictors\nDetecting Interaction Effects (these later chapters are not finished yet)\nFlattening Profile Data\nHandling Missing Data\nFeature Engineering Without Overfitting\nFeature Selection\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/exercise-solutions/index.html",
    "href": "posts/exercise-solutions/index.html",
    "title": "Exercise Solutions",
    "section": "",
    "text": "I’m finally recovering form the summer and will start posing again soon.\nIn the meantime, Kjell and I have made some progress on the exercise solutions. We’d like some feedback from the readers and instructors on how to release them:\n\nHello,\nThank you for contacting Max and me regarding solutions to exercises in Applied Predictive Modeling. We took a needed break after completing the manuscript, and we’re now working on compiling the exercise solutions.\nOver the past year, we have had requests for solutions from practitioners who are applying these techniques and approaches in their work, professors who are using the text in their classes, and students who are taking a course which uses APM as a text.\nGiven that the text is being used in courses and that the exercises may be assigned as part of the coursework, we’d like to be sensitive to those professors and not completely divulge our solutions. At the same time, we would also like to provide as many solutions as possible for practitioners who are working through the exercises to improve their skills. These are clearly two different audiences with different needs.\nThat said, the general nature of predictive modeling and the way we have worded many of the exercises provide the potential for a number of possible “correct” solutions–not just the solutions that we provide.\nSpringer, our publisher, has suggested one possible solution by offering to host a password protected website for exercise solutions. Another potential path would be to provide solutions to select exercises (e.g. odd numbered exercises). At the other extreme, we could post all solutions.\nIn the context of this background, we have some questions for you:\nFor those of you who are professors using APM as a text, what approach to exercise solutions would be most useful to you? Would you be negatively impacted if we published solutions to all exercises?\nFor those of you who are practitioners, what approach would be the best alternative (to publishing all solutions) for you, given that the text is being used in courses?\nWe welcome your feedback and other ideas you have for providing exercise solutions.\nThanks, and we look forward to hearing from you.\n\n\nBest regards,Kjell(kjell@arboranalytics.com) and Max (mxkuhn@gmail.com)\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/feature-engineering-versus-feature-extraction-game-on/index.html",
    "href": "posts/feature-engineering-versus-feature-extraction-game-on/index.html",
    "title": "Feature Engineering versus Feature Extraction: Game On!",
    "section": "",
    "text": "“Feature engineering” is a fancy term for making sure that your predictors are encoded in the model in a manner that makes it as easy as possible for the model to achieve good performance. For example, if your have a date field as a predictor and there are larger differences in response for the weekends versus the weekdays, then encoding the date in this way makes it easier to achieve good results.\nHowever, this depends on a lot of things.\nFirst, it is model-dependent. For example, trees might have trouble with a classification data set if the class boundary is a diagonal line since their class boundaries are made using orthogonal slices of the data (oblique trees excepted).\nSecond, the process of predictor encoding benefits the most from subject-specific knowledge of the problem. In my example above, you need to know the patterns of your data to improve the format of the predictor. Feature engineering is very different in image processing, information retrieval, RNA expressions profiling, etc. You need to know something about the problem and your particular data set to do it well.\nHere is some training set data where two predictors are used to model a two-class system (I’ll unblind the data at the end):\n\n\n\n\n\nThere is also a corresponding test set that we will use below.\nThere are some observations that we can make:\n\nThe data are highly correlated (correlation = 0.85)\nEach predictor appears to be fairly right-skewed\nThey appear to be informative in the sense that you might be able to draw a diagonal line to differentiate the classes\n\nDepending on what model that we might choose to use, the between-predictor correlation might bother us. Also, we should look to see of the individual predictors are important. To measure this, we’ll use the area under the ROC curve on the predictor data directly.\nHere are univariate box-plots of each predictor (on the log scale):\n\n\n\n\n\nThere is some mild differentiation between the classes but a significant amount of overlap in the boxes. The area under the ROC curves for predictor A and B are 0.61 and 0.59, respectively. Not so fantastic.\nWhat can we do? Principal component analysis (PCA) is a pre-processing method that does a rotation of the predictor data in a manner that creates new synthetic predictors (i.e. the principal components or PC’s). This is conducted in a way where the first component accounts for the majority of the (linear) variation or information in the predictor data. The second component does the same for any information in the data that remains after extracting the first component and so on. For these data, there are two possible components (since there are only two predictors). Using PCA in this manner is typically called feature extraction.\nLet’s compute the components:\nlibrary(caret)\nhead(example_train)\n##    PredictorA PredictorB Class\n## 2    3278.726  154.89876   One\n## 3    1727.410   84.56460   Two\n## 4    1194.932  101.09107   One\n## 12   1027.222   68.71062   Two\n## 15   1035.608   73.40559   One\n## 16   1433.918   79.47569   One\npca_pp &lt;- preProcess(example_train[, 1:2],\n                     method = c(\"center\", \"scale\", \"pca\"))\npca_pp\n## Call:\n## preProcess.default(x = example_train[, 1:2], method = c(\"center\",\n##  \"scale\", \"pca\"))\n## \n## Created from 1009 samples and 2 variables\n## Pre-processing: centered, scaled, principal component signal extraction \n## \n## PCA needed 2 components to capture 95 percent of the variance\ntrain_pc &lt;- predict(pca_pp, example_train[, 1:2])\ntest_pc &lt;- predict(pca_pp, example_test[, 1:2])\nhead(test_pc, 4)\n##         PC1         PC2\n## 1 0.8420447  0.07284802\n## 5 0.2189168  0.04568417\n## 6 1.2074404 -0.21040558\n## 7 1.1794578 -0.20980371\nNote that we computed all the necessary information from the training set and apply these calculations to the test set. What do the test set data look like?\n\n\n\n\n\nThese are the test set predictors simply rotated.\nPCA is unsupervised, meaning that the outcome classes are not considered when the calculations are done. Here, the area under the ROC curves for the first component is 0.5 and 0.81 for the second component. These results jive with the plot above; the first component has an random mixture of the classes while the second seems to separate the classes well. Box plots of the two components reflect the same thing:\n\n\n\n\n\nThere is much more separation in the second component.\nThis is interesting. First, despite PCA being unsupervised, it managed to find a new predictor that differentiates the classes. Secondly, it is the last component that is most important to the classes but the least important to the predictors. It is often said that PCA doesn’t guarantee that any of the components will be predictive and this is true. Here, we get lucky and it does produce something good.\nHowever, imagine that there are hundreds of predictors. We may only need to use the first X components to capture the majority of the information in the predictors and, in doing so, discard the later components. In this example, the first component accounts for 92.4% of the variation in the predictors; a similar strategy would probably discard the most effective predictor.\nHow does the idea of feature engineering come into play here? Given these two predictors and seeing the first scatterplot shown above, one of the first things that occurs to me is “there are two correlated, positive, skewed predictors that appear to act in tandem to differentiate the classes”. The second thing that occurs to be is “take the ratio”. What does that data look like?\n\n\n\n\n\nThe corresponding area under the ROC curve is 0.8, which is nearly as good as the second component. A simple transformation based on visually exploring the data can do just as good of a job as an unbiased empirical algorithm.\nThese data are from the cell segmentation experiment of Hill et al, and predictor A is the “surface of a sphere created from by rotating the equivalent circle about its diameter” (labeled as EqSphereAreaCh1 in the data) and predictor B is the perimeter of the cell nucleus (PerimCh1). A specialist in high content screening might naturally take the ratio of these two features of cells because it makes good scientific sense (I am not that person). In the context of the problem, their intuition should drive the feature engineering process.\nHowever, in defense of an algorithm such as PCA, the machine has some benefit. In total, there are almost sixty predictors in these data whose features are just as arcane as EqSphereAreaCh1. My personal favorite is the “Haralick texture measurement of the spatial arrangement of pixels based on the co-occurrence matrix”. Look that one up some time. The point is that there are often too many features to engineer and they might be completely unintuitive from the start.\nAnother plus for feature extraction is related to correlation. The predictors in this particular data set tend to have high between-predictor correlations and for good reasons. For example, there are many different ways to quantify the eccentricity of a cell (i.e. how elongated it is). Also, the size of a cell’s nucleus is probably correlated with the size of the overall cell and so on. PCA can mitigate the effect of these correlations in one fell swoop. An approach of manually taking ratios of many predictors seems less likely to be effective and would take more time.\nLast year, in one of the R&D groups that I support, there was a bit of a war being waged between the scientists who focused on biased analysis (i.e. we model what we know) versus the unbiased crowd (i.e. just let the machine figure it out). I fit somewhere in-between and believe that there is a feedback loop between the two. The machine can flag potentially new and interesting features that, once explored, become part of the standard book of “known stuff”.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/feature-selection-3--swarm-mentality/index.html",
    "href": "posts/feature-selection-3--swarm-mentality/index.html",
    "title": "Feature Selection 3 - Swarm Mentality",
    "section": "",
    "text": "“Bees don’t swarm in a mango grove for nothing. Where can you see a wisp of smoke without a fire?” - Hla Stavhana\nIn the last two posts, genetic algorithms were used as feature wrappers to search for more effective subsets of predictors. Here, I will do the same with another type of search algorithm: particle swarm optimization.\nLike genetic algorithms, this search procedure is motivated by a natural phenomenon, such as the movements of bird flocks. An excellent reference for this technique is Poli et al (2007). The methodology was originally developed for optimizing real valued parameters, but was later adapted for discrete optimization by Kennedy and Eberhart (1997).\nThe optimization is initiated with configurations (i.e. multiple particles). In our case, the particles will be different predictor subsets. For now, let’s stick with the parameters being real-valued variables. A particular value of a particle is taken to be it’s position. In addition to a position, each particle has an associated velocity. For the first iteration, these are based on random numbers.\nEach particle produces a fitness value. As with genetic algorithms, this is some measure of model fit. The next candidate set of predictors that a particle evaluates is based on it’s last position and it’s current velocity.\nA swarm of particle are evaluated at once and the location of the best particle is determined. As the velocity of each particle is updated, the update is a function of the:\n\nprevious velocity,\nlast position and\nthe position of the best particle\n\nThere are other parameters of the search procedure, such as the number of particles or how much relative weight the positions of the individual and best particle are used to determine the next candidate point, but this is the basic algorithm in a nutshell.\nAs an example, consider optimzing the Rosenbrock function with two real-valued variables (A and B):\nfitness = 100*(B - A^2)^2 + (A - 1)^2\nThe best value is at (A = 1, B = 1). The movie below shows a particle swarm optimization using 100 iterations. The predicted best (solid white dot) is consistently in the neighborhood of the optimum value at around 50 iterations. You may need to refresh your browser to re-start the animation.\n\n\n\n\n\nWhen searching for subsets, the quantities that we search over are binary (i.e. the predictor is used or excluded from the model). The description above implies that the position is a real valued quantity. If the positions are centered around zero, Kennedy and Eberhart (1997) suggested using a sigmoidal function to translate this value be between zero and one. A uniform random number is used to determine the binary version of the position that is evaluated. Other strategies have been proposed, including the application of a simple threshold to the translated position (i.e. if the translated position is above 0.5, include the predictor).\nR has the pso package that implements this algorithm. It does not work for discrete optimization that we need for feature selection. Since its licensed under the GPL, I took the code and removed the parts specific to real valued optimization. That code is linked that the bottom of the page. I structured it to be similar to the R code for genetic algorithms. One input into the modified pso function is a list that has modules for fitting the model, generating predictions, evaluating the fitness function and so on. I’ve made some changes so that each particle can return multiple values and will treat the first as the fitness function. I’ll fit the same QDA model as before to the same simulated data set. First, here are the QDA functions:\nqda_pso &lt;- list(\n  fit = function(x, y, ...)\n    {\n    ## Check to see if the subset has no members\n    if(ncol(x) &gt; 0)\n      {\n      mod &lt;- train(x, y, \"qda\", \n                   metric = \"ROC\",\n                   trControl = trainControl(method = \"repeatedcv\", \n                                            repeats = 1,\n                                            summaryFunction = twoClassSummary,\n                                            classProbs = TRUE))\n      } else mod &lt;- nullModel(y = y) ## A model with no predictors \n    mod\n    },\n  fitness = function(object, x, y)\n    {\n    if(ncol(x) &gt; 0)\n      {\n      testROC &lt;- roc(y, predict(object, x, type = \"prob\")[,1], \n                     levels = rev(levels(y)))\n      largeROC &lt;- roc(large$Class, \n                      predict(object, \n                              large[,names(x),drop = FALSE], \n                              type = \"prob\")[,1], \n                      levels = rev(levels(y)))  \n      out &lt;- c(Resampling = caret:::getTrainPerf(object)[, \"TrainROC\"],\n               Test = as.vector(auc(testROC)), \n               Large_Sample = as.vector(auc(largeROC)),\n               Size = ncol(x))\n      } else {\n        out &lt;- c(Resampling = .5,\n                 Test = .5, \n                 Large_Sample = .5,\n                 Size = ncol(x))\n        print(out)\n        }\n    out\n    },\n  predict = function(object, x)\n    {\n    library(caret)\n    predict(object, newdata = x)\n    }\n  )\nHere is the familiar code to generate the simulated data:\nset.seed(468)\ntraining &lt;- twoClassSim(  500, noiseVars = 100, \n                        corrVar = 100, corrValue = .75)\ntesting  &lt;- twoClassSim(  500, noiseVars = 100, \n                        corrVar = 100, corrValue = .75)\nlarge    &lt;- twoClassSim(10000, noiseVars = 100, \n                        corrVar = 100, corrValue = .75)\nrealVars &lt;- names(training)\nrealVars &lt;- realVars[!grepl(\"(Corr)|(Noise)\", realVars)]\ncvIndex &lt;- createMultiFolds(training$Class, times = 2)\nctrl &lt;- trainControl(method = \"repeatedcv\",\n                     repeats = 2,\n                     classProbs = TRUE,\n                     summaryFunction = twoClassSummary,\n                     ## We will parallel process within each PSO\n                     ## iteration, so don't double-up the number\n                     ## of sub-processes\n                     allowParallel = FALSE,\n                     index = cvIndex)\nTo run the optimization, the code will be similar to the GA code used in the last two posts:\nset.seed(235)\npsoModel &lt;- psofs(x = training[,-ncol(training)],\n                  y = training$Class,\n                  iterations = 200,\n                  functions = qda_pso,\n                  verbose = FALSE,\n                  ## The PSO code uses foreach to parallelize\n                  parallel = TRUE,\n                  ## These are passed to the fitness function\n                  tx = testing[,-ncol(testing)],\n                  ty = testing$Class)\nSince this is simulated data, we can evaluate how well the search went using estimates of the fitness (the area under the ROC curve) calculated using different data sets: resampling, a test set of 500 samples and large set of 10,000 samples that we use to approximate the truth.\nThe swarm did not consistently move to smaller subsets and, as with the original GA, it overfits to the predictors. This is demonstrated by the increase in the resampled fitness estimates and mediocre test/large sample estimates:\n\n\n\n\n\nOne tactic that helped the GA was to bias the algorithm towards smaller subsets. For PSO, this can be accomplished during the conversion from real valued positions to binary encodings. The previous code used a value of 1 for a predictor if the “squashed” version (i.e. after applying a sigmoidal function) was greater than 0.5. We can bias the subsets by increasing the threshold. This should start the process with smaller subsets and, since we raise the criteria for activating a predictor, only increase the subset size if there is a considerable increase in the fitness function. Here is the code for that conversion and another run of the PSO:\nsmallerSubsets &lt;- function(x)\n{  \n  ## 'x' is a matrix of positions centered around zero. The\n  ## code below uses a logistic function to \"squash\" then to\n  ## be between (0, 1). \n  binary &lt;- binomial()$linkinv(x)\n  ## We could use ranom numbers to translate between the \n  ## squashed version of the position and the binary version.\n  ## In the last application of PSO, I used a simple threshold of\n  ## 0.5. Now, increase the threshold a little to force the \n  ## subsets to be smaller.\n  binary &lt;- ifelse(binary &gt;= .7, 1, 0)\n  ## 'x' has particles in columns and predicors in rows, \n  ## so use apply() to threshold the positions\n  apply(binary, 2, function(x) which(x == 1))\n}\nset.seed(235)\npsoSmallModel &lt;- psofs(x = training[,-ncol(training)],\n                       y = training$Class,\n                       iterations = 200,\n                       convert = smallerSubsets,\n                       functions = qda_pso,\n                       verbose = FALSE,\n                       parallel = TRUE,\n                       tx = testing[,-ncol(testing)],\n                       ty = testing$Class)\nThe results are much better:\n\n\n\n\n\nThe large-sample and test set fitness values agree with the resampled versions. A smoothed version of the number of predictors over iterations shows that the search is driving down the number of predictors and keeping them low:\n\n\n\n\n\nAnd here are the large-sample ROC curves so far:\n\n\n\n\n\nFor the simulated data, the GA and PSO procedures effectively reduced the number of predictors. After reading the last few posts, one could easily remark that I was only able to do this since I knew what the answers should be. If the optimal subset size was not small, would these approaches have been effective? The next (and final) post in this series will apply these methods to a real data set.\nThe code for these analyses are here and the modified PSO code is here. Thanks to Claus Bendtsen for the original pso code and for answering my email.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/in-search-of/index.html",
    "href": "posts/in-search-of/index.html",
    "title": "In Search Of…",
    "section": "",
    "text": "Rafael Ladeira asked on github:\n\nI was wondering why it doesn’t implement some others algorithms for search for optimal tuning parameters. What would be the caveats of using a [genetic algorithm]http://topepo.github.io/caret/training.html#builtin , for instance, instead of grid or random search? Do you think using some of those powerful optimization algorithms for tuning parameters is a good idea?\n\nYou can read the relatively short discussion yourself. It seems clear that nonlinear programming methods have great potential to find better tuning parameter values. However, there are some nontrivial considerations:\n\nHow can your estimate the fitness value efficiently? If you have a ton of data, a single holdout would not be a bad choice for evaluating whatever metric you have chosen to measure performance (e.g. RMSE, accuracy, etc). If not, resampling is most likely the answer and that might not be very efficient. Direct search methods like genetic algorithms (GA), Nelder-Mead and others can require a lot of function evaluations and that can take a while when combined with resampling. Other metrics (i.e. AIC, adjusted R 2 ) could be used but rely on specifying the number of model parameters and that can be unknown (or greater than the sample size).\nIf you are going to evaluate the same data set a large number of times, even with resampling, there is great potential for overfitting.\nYou probably don’t want to reply on some convergence criteria and, instead, use a fixed number of iterations and use the best solution found during the search. Many models have performance curves that plateau or are very broad. This will lead to convergence issues.\n\nLet’s run a test and see if there is any benefit for an example data set. The caret function SLC14_1 simulates a system from Sapp et al. (2014). All informative predictors are independent Gaussian random variables with mean zero and a variance of 9. The prediction equation is:\nx_1 + sin(x_2) + log(abs(x_3)) + x_4^2 + x_5*x_6 + \nI(x_7*x_8*x_9 &lt; 0) + I(x_10 &gt; 0) + x_11*I(x_11 &gt; 0) + \nsqrt(abs(x_12)) + cos(x_13) + 2*x_14 + abs(x_15) + \nI(x_16 &lt; -1) + x_17*I(x_17 &lt; -1) - 2 * x_18 - x_19*x_20\nThe random error here is also Gaussian with mean zero and a variance of 9. I simulated 500 samples for a training set and use a large number (105) as a test set.\nlibrary(caret)\nlibrary(GA)\nlibrary(kernlab)\nlibrary(pROC)\nlibrary(gbm)\n\nset.seed(17516)\ntraining_data &lt;- SLC14_1(500)\ntesting_data &lt;- SLC14_1(10^5)\nLet’s evaluate a support vector machine and a boosted tree on these data using:\n\nbasic grid search. For the SVM model, we estimate the sigma parameter once using kernlab’s sigest function and use a grid of 10 cost values. For GBM, we tune over 1800 combinations of the the four parameters in the default model code (but only 60 models are actually fit).\nrandom search: here I matched the number of parameter combinations to the number of models fit using grid search (10 for SVM and 60 for GBM).\na genetic algorithm with a population size of 50, 12 generations, elitism of two (meaning the two best solutions are carried forward from a generation), cross-over rate of 0.1.\n\nTo measure performance, basic 10-fold CV was used. The same CV folds were used for grid search, random search, and the final model determined by the GA. However, during the genetic algorithm, random CV folds are used.\n[Here is the code][6] to create the data and run all of the models.\nThe fitness functions takes the parameter combination as an argument and estimate the RMSE using 10-fold CV. The GA package assumes that you want to maximize the outcome, so we return the negative of the RMSE:\nsvm_fit &lt;- function(x) {\n  mod &lt;- train(y ~ ., data = training_data,\n               method = \"svmRadial\",\n               preProc = c(\"center\", \"scale\"),\n               trControl = trainControl(method = \"cv\"),\n               tuneGrid = data.frame(C = 2^x[1], sigma = exp(x[2])))\n  -getTrainPerf(mod)[, \"TrainRMSE\"]\n}\n\ngbm_fit &lt;- function(x) {\n  mod &lt;- train(y ~ ., data = training_data,\n               method = \"gbm\",\n               trControl = trainControl(method = \"cv\", number = 10),\n               tuneGrid = data.frame(n.trees = floor(x[1])+1,\n                                     interaction.depth = floor(x[2])+1,\n                                     shrinkage = x[3],\n                                     n.minobsinnode = floor(x[4])+1),\n               verbose = FALSE)\n  -getTrainPerf(mod)[, \"TrainRMSE\"]\n}\nNow, to run the GA’s:\nsvm_ga_obj &lt;- ga(type = \"real-valued\",\n                 fitness =  svm_fit,\n                 min = c(-5, -5),\n                 max = c(10,  0),\n                 popSize = 50,\n                 maxiter = 12,\n                 seed = 16478,\n                 keepBest = TRUE,\n                 monitor = NULL,\n                 elitism = 2)\n  \ngbm_ga_obj &lt;- ga(type = \"real-valued\",\n                 fitness =  gbm_fit,\n                 min = c(   1,  1, 0.0,  5),\n                 max = c(5000, 11, 0.2, 25),\n                 popSize = 50,\n                 maxiter = 12,\n                 seed = 513,\n                 keepBest = TRUE,\n                 monitor = NULL)  \nThese can be run in parallel at multiple values but, to compare apples-to-apples, I ran everything without parallel processing.\nThe code file linked above provides all of the code for each search type.\n\nSVM Results\nFor the SVM, this plot below shows that the GA focuses in fairly quickly on an area of interest. The x- and y-axes are the tuning parameters and the size of the point is the RMSE (smaller is better).\n\n\n\n\n\nIt has more trouble deciding on a cost value than on a value for the RBF kernel parameter. The final solution used a sigma value of 0.008977 and a cost of 83.71 and used 10 * 50 * 12 = 6000 model fits to get there.\nBelow is a plot of the resampled CV for each unique solution. The color of the points corresponds to the generation and the ordering of the point within a generation is arbitrary. There is an overall decrease in the average RMSE (the blue line) but the best solution doesn’t change that often between generations (the black line). My experience with GA’s is that they can run for quite a long time before finding a new best solution.\n\n\n\n\n\nHere is a plot of the grid search results:\n\n\n\n\n\nNote the plateau after cost values of 10 and above. This indicates why the GA had more issues with focusing in on a good cost value since the solutions were virtually the same. The grid search chose a sigma value of 0.029 and a cost of 16 using a total of 100 model fits.\nRandom search had these results:\n\n\n\n\n\nIt finalized on sigma = 0.021 and a cost of 121 also using a total of 100 model fits.\nI’ll contrast the test set results at the end. In terms of execution time, the GA took 128-fold longer to run compared to grid search and 137-fold longer than random search.\n\n\nGBM Results\nHere is a plot of the path that the GA took through the GBM tuning parameters (RMSE is not shown here).\n\n\n\n\n\nThere were 6000 model fits here too and the final parameters were: n.trees = 3270, interaction.depth = 5, shrinkage = 0.0689, and n.minobsinnode = 9.\nThe relationship between resampled RMSE and the individual model fits is:\n\n\n\n\n\nThe last five generations did not produce an optimal combination.\nUsing grid search, the patterns between the parameters and the resampled RMSE were:\n\n\n\n\n\nUsing the “submodel trick” we evaluated 1600 tuning parameter combinations but only fit 600 models over all of the resamples. The optimal values were n.trees = 3000, interaction.depth = 3, shrinkage = 0.1, and n.minobsinnode = 10.\nRandom search yield optimal settings of n.trees = 2202, interaction.depth = 3, shrinkage = 0.02651, and n.minobsinnode = 9 also using 600 model fits for tuning.\nTime-wise, the GBM GA took 8.46-fold longer to run compared to grid search and 9.95-fold longer than random search.\n\n\nTest Set Results\nHere are the results:\n##                  RMSE  Rsquared\n##  svm_ga      6.055410 0.9239437\n##  svm_grid    8.977140 0.8550142\n##  svm_random  7.877201 0.8853781\n##  gbm_ga     11.178473 0.7242996\n##  gbm_grid   11.223867 0.7145263\n##  gbm_random 11.130803 0.7253648\nFor one of the models, it was worth the trouble and time but not for the other. It is certainly possible that a larger number of random search tests would have achieved the same results and I think that I would put my effort there before using a GA to optimize. However, if I really need stellar performance, the GA is worth doing; the only cost is CPU cycles and that is usually limited by our patience.\nIs this worth putting into caret? Probably but it is the kind of thing that would take some effort (and minor changes) to make it work across all different models. For example, self organizing maps have tuning parameters for the size of the two dimensions as well as the topology type (which is categorical). It doesn’t make sense to evaluate networks with dimensions (3, 4) and also try (4, 3) since they are basically the same model. Also, encoding the categorical tuning parameters are not impossible but it might be tricky to do in an automated way. This would take some thought and, in the meantime, the code shown above works pretty well.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/measuring-associations/index.html",
    "href": "posts/measuring-associations/index.html",
    "title": "Measuring Associations",
    "section": "",
    "text": "In Chapter 18, we discuss a relatively new method for measuring predictor importance called the maximal information coefficient (MIC). The original paper is by Reshef at al (2011).\nA summary of the initial reactions to the MIC are Speed and Tibshirani (and others can be found here). My (minor) beef with it is the lack of a probabilistic motivation. The authors have some general criteria (generality and equitability) and created an algorithm that seems to good in terms of these properties. The MIC clearly works, but what is it really optimizing and why?\nIt reminds me of partial least squares. The algorithm made intuitive sense and obviously worked, but it was a while before anyone actually figured out what mathematical problem it was solving. A similar statement could be made about boosting when it was first developed.\nMurrell et al (2013) (or Murrell3?) has a similar generalized measure of association between continuous variables. There’s is based on a generalized notion of R2 that I’d never heard of. At first glance, it has a lot of attractive properties. One is that is has a probabilistic genesis. Another nice quality is that the association can be computed while controlling for other data. That is a big deal for me, since we often have experiments where we need to control for nuisance factors. For example, if you were trying to measure the relationship between the selling price of a house and the acerage of the lot, you might want to control for other factors, such as the type of terrain or geography (e.g. urban, rural etc).\nDespite the probabilistic motivation, they take a randomization approach to making formal statistical inferences on significance of the measure. The same could be done for the MIC measure (and in the book, we used the same idea for Relief scores). I think a confidence interval would be better for everyone since it directly tells you the uncertainty and the size of the association (that’s another post for another time for a topic that has been discussed quite a bit).\nLet’s look at some data. I like the blood-brain barrier data a lot. It has measurements on 208 drugs to see how much (if at all) they enter the brain. The predictors are molecular descriptors (similar to the solubility example in the book). To get the data:\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\ndata(BloodBrain)\n## remove zero variance columns\nisZV &lt;- apply(bbbDescr, 2, function(x) length(unique(x)) == 1)\nbbbDescr &lt;- bbbDescr[, !isZV]\nncol(bbbDescr)\n\n[1] 134\n\n\nFirst, I’ll measure association using the A measure discussed in Murrell3:\n\nlibrary(matie)\n## Compute the associations across all columns of the predictors\nAvalues &lt;- apply(bbbDescr, 2, function(x, y) ma(cbind(x, y))$A, y = logBBB)\nAvalues &lt;- sort(Avalues, decreasing = TRUE)\nhead(Avalues)\n\n     fnsa3   psa_npsa polar_area       tpsa      scaa3       tcnp \n 0.4381135  0.4140194  0.4136030  0.3847299  0.3766920  0.3691589 \n\n\nSo the best predictor only explains 43.9% of the variation in the outcome. Most of the predictors shown above are related to surface area, which makes sense: the larger the molecule the less likely it is to physically fit through the barrier.\nWhat does MIC tell us?\n\nlibrary(minerva)\n## There are three predictors whose scales have very low variances. We\n## need to reset the threshold that checks for this\nmic &lt;- mine(bbbDescr, logBBB, var.thr = 1e-10)$MIC\nmic &lt;- mic[, \"Y\"]\nnames(mic) &lt;- colnames(bbbDescr)\nmic &lt;- mic[order(mic, decreasing = TRUE)]\nhead(mic)\n\n    fnsa3      rpcg     fpsa3     scaa1     scaa3  psa_npsa \n0.5248916 0.4806339 0.4761844 0.4758696 0.4711854 0.4650277 \n\n\nThere are some differences but the top predictor from A is still at the top. The MIC values is sort of a correlation-like measure and our best value was 0.52.\nI also have a measure of importance that is based on scatterplot smoothers. A loess smoother is fit between the outcome and the predictor and the R2 statistic is calculated for this model against the intercept only null model. I don’t claim that there is any justification (which is why I’ve never published it) for this but it has worked for me in the past. This is similar to my statements about MIC and PLS. I still use them because they tend to work, but I’ve no theoretical leg to stand on.\n\n## A measure based on regression smoothers\ngamImp &lt;- filterVarImp(bbbDescr, logBBB, nonpara = TRUE)\ngamImp &lt;- gamImp[order(gamImp$Overall, decreasing = TRUE), , drop = FALSE]\nhead(gamImp)\n\n             Overall\nfnsa3      0.3970318\npsa_npsa   0.3879014\npolar_area 0.3805523\ntcnp       0.3681280\ntpsa       0.3584280\ntpsa.1     0.3474503\n\n\nFinally, I’ll compute the RRelief scores. We discuss this in the book and the a good reference is here. It uses a nearest-neighbor approach and measures the importance of each predictors simultaneously (all of the other methods show here measure each association in isolation).\n\nlibrary(CORElearn)\n\n## The function only uses the formula interface\nbbbData &lt;- bbbDescr\nbbbData$logBBB &lt;- logBBB\n\nset.seed(10)\nRRelief &lt;- attrEval(logBBB ~ ., data = bbbData, estimator = \"RReliefFbestK\", \n                    ReliefIterations = 100)\nRRelief &lt;- RRelief[order(RRelief, decreasing = TRUE)]\nhead(RRelief)\n\n     smr_vsa7      smr_vsa1 dipole_moment    peoe_vsa.0           pol \n    0.2622103     0.2577087     0.2524930     0.2452446     0.2366486 \n     smr_vsa6 \n    0.2278432 \n\n\nThis score ranks the variables differently than the other methods. This is most likely due to the difference in philosophy in measuring association between this method and the others as well as the high degree of correlation between the predictors in these data.\nOverall, do these metrics correlate?\n\n\n\n\nIf you ignore RRelief, there is some association between measures of association. Interestingly, there are a few predictors that have zero association using the A measure but non-zero correlation using MIC. The variables, and their MIC values are: peoe_vsa.2 (0.22), slogp_vsa4 (0.2), peoe_vsa.0 (0.19), smr_vsa1 (0.18), a_acid (0.18), peoe_vsa.2.1 (0.14) and negative (0.04). What do these look like? Here are scatterplots between these predictions and the outcome (with scatterplot smoother fits):\n\n\n\n\nSeveral of these are “near zero variance” predictors and probably shouldn’t be evaluated. For this image, it is difficult for me to see the association between the response and peoe_vsa.0 (MIC = 0.19) or smr_vsa1 (MIC = 0.18).\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/nested-resampling-with-rsample/index.html",
    "href": "posts/nested-resampling-with-rsample/index.html",
    "title": "Nested Resampling with rsample",
    "section": "",
    "text": "A typical scheme for splitting the data when developing a predictive model is to create an initial split of the data into a training and test set. If resampling is used, it is executed on the training set where a series of binary splits is created. In rsample, we use the term analysis set for the data that are used to fit the model and the assessment set is used to compute performance:\n\n\n\n\n\nA common method for tuning models is grid search where a candidate set of tuning parameters is created. The full set of models for every combination of the tuning parameter grid and the resamples is created. Each time, the assessment data are used to measure performance and the average value is determined for each tuning parameter.\nThe potential problem is, once we pick the tuning parameter associated with the best performance, this value is usually quoted as the performance of the model. There is serious potential for optimization bias since we uses the same data to tune the model and quote performance. This can result in an optimistic estimate of performance.\nNested resampling does an additional layer of resampling that separates the tuning activities from the process used to estimate the efficacy of the model. An outer resampling scheme is used and, for every split in the outer resample, another full set of resampling splits are created on the original analysis set. For example, if 10-fold cross-validation is used on the outside and 5-fold cross-validation on the inside, a total of 500 models will be fit. The parameter tuning will be conducted 10 times and the best parameters are determined from the average of the 5 assessment sets.\nOnce the tuning results are complete, a model is fit to each of the outer resampling splits using the best parameter associated with that resample. The average of the outer method’s assessment sets are a unbiased estimate of the model.\nTo get started, let’s load the packages that will be used in this post.\nlibrary(rsample)   \nlibrary(purrr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(mlbench)\nlibrary(kernlab)\nlibrary(sessioninfo)\ntheme_set(theme_bw())\nWe will simulate some regression data to illustrate the methods. The function mlbench::mlbench.friedman1 can simulate a complex regression data structure from the original MARS publication. A training set size of 100 data points are generated as well as a large set that will be used to characterize how well the resampling procedure performed.\nsim_data &lt;- function(n) {\n  tmp &lt;- mlbench.friedman1(n, sd=1)\n  tmp &lt;- cbind(tmp$x, tmp$y)\n  tmp &lt;- as.data.frame(tmp)\n  names(tmp)[ncol(tmp)] &lt;- \"y\"\n  tmp\n}\n\nset.seed(9815)\ntrain_dat &lt;- sim_data(100)\nlarge_dat &lt;- sim_data(10^5)\nTo get started, the types of resampling methods need to be specified. This isn’t a large data set, so 5 repeats of 10-fold cross validation will be used as the outer resampling method that will be used to generate the estimate of overall performance. To tune the model, it would be good to have precise estimates for each of the values of the tuning parameter so 25 iterations of the bootstrap will be used. This means that there will eventually be 5 * 10 * 25 = 1250 models that are fit to the data per tuning parameter. These will be discarded once the performance of the model has been quantified.\nTo create the tibble with the resampling specifications:\nresults &lt;- nested_cv(train_dat, \n                     outside = vfold_cv(repeats = 5), \n                     inside = bootstraps(25))\nresults\n## # 10-fold cross-validation repeated 5 times \n## # Nested : vfold_cv(repeats = 5) / bootstraps(25) \n## # A tibble: 50 x 4\n##          splits      id    id2   inner_resamples\n##          &lt;list&gt;   &lt;chr&gt;  &lt;chr&gt;            &lt;list&gt;\n##  1 &lt;S3: rsplit&gt; Repeat1 Fold01 &lt;tibble [25 x 2]&gt;\n##  2 &lt;S3: rsplit&gt; Repeat1 Fold02 &lt;tibble [25 x 2]&gt;\n##  3 &lt;S3: rsplit&gt; Repeat1 Fold03 &lt;tibble [25 x 2]&gt;\n##  4 &lt;S3: rsplit&gt; Repeat1 Fold04 &lt;tibble [25 x 2]&gt;\n##  5 &lt;S3: rsplit&gt; Repeat1 Fold05 &lt;tibble [25 x 2]&gt;\n##  6 &lt;S3: rsplit&gt; Repeat1 Fold06 &lt;tibble [25 x 2]&gt;\n##  7 &lt;S3: rsplit&gt; Repeat1 Fold07 &lt;tibble [25 x 2]&gt;\n##  8 &lt;S3: rsplit&gt; Repeat1 Fold08 &lt;tibble [25 x 2]&gt;\n##  9 &lt;S3: rsplit&gt; Repeat1 Fold09 &lt;tibble [25 x 2]&gt;\n## 10 &lt;S3: rsplit&gt; Repeat1 Fold10 &lt;tibble [25 x 2]&gt;\n## # ... with 40 more rows\nThe splitting information for each resample is contained in the split objects. Focusing on the second fold of the first repeat:\nresults$splits[[2]]\n## &lt;90/10/100&gt;\n&lt;90/10/100&gt; indicates the number of data in the analysis set, assessment set, and the original data.\nEach element of inner_resamples has its own tibble with the bootstrapping splits.\nresults$inner_resamples[[5]]\n## # Bootstrap sampling with 25 resamples \n## # A tibble: 25 x 2\n##          splits          id\n##          &lt;list&gt;       &lt;chr&gt;\n##  1 &lt;S3: rsplit&gt; Bootstrap01\n##  2 &lt;S3: rsplit&gt; Bootstrap02\n##  3 &lt;S3: rsplit&gt; Bootstrap03\n##  4 &lt;S3: rsplit&gt; Bootstrap04\n##  5 &lt;S3: rsplit&gt; Bootstrap05\n##  6 &lt;S3: rsplit&gt; Bootstrap06\n##  7 &lt;S3: rsplit&gt; Bootstrap07\n##  8 &lt;S3: rsplit&gt; Bootstrap08\n##  9 &lt;S3: rsplit&gt; Bootstrap09\n## 10 &lt;S3: rsplit&gt; Bootstrap10\n## # ... with 15 more rows\nThese are self-contained, meaning that the bootstrap sample is aware that it is a sample of a specific 90% of the data:\nresults$inner_resamples[[5]]$splits[[1]]\n## &lt;90/37/90&gt;\nTo start, we need to define how the model will be created and measured. For our example, a radial basis support vector machine model will be created using the function kernlab::ksvm. This model is generally thought of as having two tuning parameters: the SVM cost value and the kernel parameter sigma. For illustration, only the cost value will be tuned and the function kernlab::sigest will be used to estimate sigma during each model fit. This is automatically done by ksvm.\nAfter the model is fit to the analysis set, the root-mean squared error (RMSE) is computed on the assessment set. One important note: for this model, it is critical to center and scale the predictors before computing dot products. We don’t do this operation here because mlbench.friedman1 simulates all of the predictors to be standard uniform random variables.\nOur function to fit a single model and compute the RMSE is:\n# `object` will be an `rsplit` object from our `results` tibble\n# `cost` is the tuning parameter\nsvm_rmse &lt;- function(object, cost = 1) {\n  y_col &lt;- ncol(object$data)\n  mod &lt;- ksvm(y ~ ., data = analysis(object),  C = cost)\n  holdout_pred &lt;- predict(mod, assessment(object)[-y_col])\n  rmse &lt;- sqrt(mean((assessment(object)$y - holdout_pred)^2, na.rm = TRUE))\n  rmse\n}\n\n# In some case, we want to parameterize the function over the tuning parameter:\nrmse_wrapper &lt;- function(cost, object) svm_rmse(object, cost)\nFor the nested resampling, a model needs to be fit for each tuning parameter and each bootstrap split. To do this, a wrapper can be created:\n# `object` will be an `rsplit` object for the bootstrap samples\ntune_over_cost &lt;- function(object) {\n  results &lt;- tibble(cost = 2^seq(-2, 8, by = 1))\n  results$RMSE &lt;- map_dbl(results$cost, \n                          rmse_wrapper,\n                          object = object)\n  results\n}\nSince this will be called across the set of outer cross-validation splits, another wrapper is required:\n# `object` is an `rsplit` object in `results$inner_resamples` \nsummarize_tune_results &lt;- function(object) {\n  # Return row-bound tibble that has the 25 bootstrap results\n  map_df(object$splits, tune_over_cost) %&gt;%\n    # For each value of the tuning parameter, compute the \n    # average RMSE which is the inner bootstrap estimate. \n    group_by(cost) %&gt;%\n    summarize(mean_RMSE = mean(RMSE, na.rm = TRUE),\n              n = length(RMSE))\n}\nNow that those functions are defined, we can execute all the inner resampling loops:\ntuning_results &lt;- map(results$inner_resamples, summarize_tune_results) \ntuning_results is a list of data frames for each of the 50 outer resamples.\nLet’s make a plot of the averaged results to see what the relationship is between the RMSE and the tuning parameters for each of the inner bootstrapping operations:\npooled_inner &lt;- tuning_results %&gt;% bind_rows\n\nbest_cost &lt;- function(dat) dat[which.min(dat$mean_RMSE),]\n\np &lt;- ggplot(pooled_inner, aes(x = cost, y = mean_RMSE)) + \n  scale_x_continuous(trans='log2') +\n  xlab(\"SVM Cost\") + ylab(\"Inner RMSE\")\n\nfor(i in 1:length(tuning_results))\n  p &lt;- p  + \n  geom_line(data = tuning_results[[i]], alpha = .2) + \n  geom_point(data = best_cost(tuning_results[[i]]), pch = 16)\n\np &lt;- p + geom_smooth(data = pooled_inner, se = FALSE)\np\n## `geom_smooth()` using method = 'loess'\n\n\n\n\n\nEach grey line is a separate bootstrap resampling curve created from a different 90% of the data. The blue line is a loess smooth of all the results pooled together.\nTo determine the best parameter estimate for each of the outer resampling iterations:\ncost_vals &lt;- tuning_results %&gt;% map_df(best_cost) %&gt;% select(cost) \nresults &lt;- bind_cols(results, cost_vals)\n\nggplot(results, aes(x = factor(cost))) + geom_bar() + xlab(\"SVM Cost\")\n\n\n\n\n\nMost of the resamples produced an optimal cost values of 2.0 but the distribution is right-skewed due to the flat trend in the resampling profile once the cost value becomes 10 or larger.\nNow that we have these estimates, we can compute the outer resampling results for each of the 50 splits using the corresponding tuning parameter value:\nresults$RMSE &lt;- map2_dbl(results$splits, results$cost, svm_rmse)\nsummary(results$RMSE)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   1.086   2.183   2.562   2.689   3.191   4.222\nThe RMSE estimate using nested resampling is 2.69.\nWhat is the RMSE estimate for the non-nested procedure when only the outer resampling method is used? For each cost value in the tuning grid, 50 SVM models are fit and their RMSE values are averaged. The table of cost values and mean RMSE estimates is used to determine the best cost value. The associated RMSE is the biased estimate.\nnot_nested &lt;- map(results$splits, tune_over_cost) %&gt;%\n  bind_rows\n\nouter_summary &lt;- not_nested %&gt;% \n  group_by(cost) %&gt;% \n  summarize(outer_RMSE = mean(RMSE),\n            n = length(RMSE))\nouter_summary\n## # A tibble: 11 x 3\n##      cost outer_RMSE     n\n##     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n##  1   0.25   3.565595    50\n##  2   0.50   3.119439    50\n##  3   1.00   2.775602    50\n##  4   2.00   2.609950    50\n##  5   4.00   2.639033    50\n##  6   8.00   2.755651    50\n##  7  16.00   2.831902    50\n##  8  32.00   2.840183    50\n##  9  64.00   2.833896    50\n## 10 128.00   2.831717    50\n## 11 256.00   2.836863    50\n\nggplot(outer_summary, aes(x = cost, y = outer_RMSE)) + \n  geom_point() + \n  geom_line() + \n  scale_x_continuous(trans='log2') +\n  xlab(\"SVM Cost\") + ylab(\"Inner RMSE\")\n\n\n\n\n\nThe non-nested procedure estimates the RMSE to be 2.61. Both estimates are fairly close and would end up choosing a cost parameter value of 2.0.\nThe approximately true RMSE for an SVM model with a cost value of 2.0 and be estimated with the large sample that was simulated at the beginning.\nfinalModel &lt;- ksvm(y ~ ., data = train_dat, C = 2)\nlarge_pred &lt;- predict(finalModel, large_dat[, -ncol(large_dat)])\nsqrt(mean((large_dat$y - large_pred)^2, na.rm = TRUE))\n## [1] 2.696096\nThe nested procedure produces a closer estimate to the approximate truth but the non-nested estimate is very similar. There is some optimization bias here but it is very small (for these data and this model).\nThe R markdown document used to create this post can be found here\nThe session information is:\nsession_info()\n## ─ Session info ──────────────────────────────────────────────────────────\n##  setting  value                       \n##  version  R version 3.3.3 (2017-03-06)\n##  os       macOS Sierra 10.12.6        \n##  system   x86_64, darwin13.4.0        \n##  ui       X11                         \n##  language (EN)                        \n##  collate  en_US.UTF-8                 \n##  tz       America/New_York            \n##  date     2017-09-03                  \n## \n## ─ Packages ──────────────────────────────────────────────────────────────\n##  package     * version  date       source         \n##  assertthat    0.2.0    2017-04-11 CRAN (R 3.3.2) \n##  bindr         0.1      2016-11-13 CRAN (R 3.3.2) \n##  bindrcpp    * 0.2      2017-06-17 cran (@0.2)    \n##  broom       * 0.4.2    2017-02-13 CRAN (R 3.3.2) \n##  clisymbols    1.2.0    2017-05-21 CRAN (R 3.3.2) \n##  colorspace    1.3-2    2016-12-14 CRAN (R 3.3.2) \n##  dplyr       * 0.7.2    2017-07-20 cran (@0.7.2)  \n##  evaluate      0.10.1   2017-06-24 CRAN (R 3.3.2) \n##  foreign       0.8-67   2016-09-13 CRAN (R 3.3.3) \n##  ggplot2     * 2.2.1    2016-12-30 CRAN (R 3.3.2) \n##  glue          1.1.1    2017-06-21 CRAN (R 3.3.2) \n##  gtable        0.2.0    2016-02-26 CRAN (R 3.3.0) \n##  highr         0.6      2016-05-09 CRAN (R 3.3.0) \n##  kernlab     * 0.9-25   2016-10-03 CRAN (R 3.3.0) \n##  knitr       * 1.17     2017-08-10 CRAN (R 3.3.2) \n##  labeling      0.3      2014-08-23 CRAN (R 3.3.0) \n##  lattice       0.20-35  2017-03-25 CRAN (R 3.3.3) \n##  lazyeval      0.2.0    2016-06-12 CRAN (R 3.3.0) \n##  magrittr      1.5      2014-11-22 CRAN (R 3.3.0) \n##  mlbench     * 2.1-1    2012-07-10 CRAN (R 3.3.0) \n##  mnormt        1.5-5    2016-10-15 CRAN (R 3.3.0) \n##  munsell       0.4.3    2016-02-13 CRAN (R 3.3.0) \n##  nlme          3.1-131  2017-02-06 CRAN (R 3.3.3) \n##  pkgconfig     2.0.1    2017-03-21 cran (@2.0.1)  \n##  plyr          1.8.4    2016-06-08 CRAN (R 3.3.0) \n##  psych         1.7.3.21 2017-03-22 CRAN (R 3.3.2) \n##  purrr       * 0.2.3    2017-08-02 cran (@0.2.3)  \n##  R6            2.2.2    2017-06-17 cran (@2.2.2)  \n##  Rcpp          0.12.12  2017-07-15 cran (@0.12.12)\n##  reshape2      1.4.2    2016-10-22 CRAN (R 3.3.3) \n##  rlang         0.1.2    2017-08-09 cran (@0.1.2)  \n##  rsample     * 0.0.1    2017-07-08 CRAN (R 3.3.3) \n##  scales      * 0.5.0    2017-08-24 CRAN (R 3.3.2) \n##  sessioninfo * 1.0.0    2017-06-21 CRAN (R 3.3.2) \n##  stringi       1.1.5    2017-04-07 CRAN (R 3.3.2) \n##  stringr       1.2.0    2017-02-18 CRAN (R 3.3.2) \n##  tibble        1.3.4    2017-08-22 cran (@1.3.4)  \n##  tidyr         0.7.0    2017-08-16 cran (@0.7.0)  \n##  withr         2.0.0    2017-07-28 CRAN (R 3.3.2)\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/new-caret-version-with-adaptive-resampling/index.html",
    "href": "posts/new-caret-version-with-adaptive-resampling/index.html",
    "title": "New caret version with adaptive resampling",
    "section": "",
    "text": "A new version of caret is on CRAN now.\nThere are a number of bug fixes:\n\nA man page with the list of models available via train was added back into the package. See ?models.\nThoralf Mildenberger found and fixed a bug in the variable importance calculation for neural network models.\nThe output of varImp for pamr models was updated to clarify the ordering of the importance scores.\ngetModelInfo was updated to generate a more informative error message if the user looks for a model that is not in the package’s model library.\nA bug was fixed related to how seeds were set inside of train.\nThe model “parRF” (parallel random forest) was added back into the library.\nWhen case weights are specified in train, the hold-out weights are exposed when computing the summary function.\nA check was made to convert a data.table given to train to a data frame (see this post).\n\nOne big new feature is that adaptive resampling can be used. I’ll be speaking about this at useR! this year. Also, while I’m submitting a manuscript, a pre-print is available at arxiv.\nBasically, after a minimum number of resamples have been processed, all tuning parameter values are not treated equally. Some that are unlikely to be optimal are ignored as resampling proceeds. There can be substantial speed-ups in doing so and there is a low probability that a poor model will be found. Here is a plot of the median speed-up (y axis) versus the estimated probability of model at least as good as the one found using all the resamples will occur.\n\n\n\n\n\nThe manuscript has more details about the other factors in the graph. One nice property of this methodology is that, when combined with parallel processing, the speed-ups could be as high as 30-fold (for the simulated example).\nThese features should be considered experimental. Send me any feedback on them that you may have.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/new-version-of-caret-on-cran/index.html",
    "href": "posts/new-version-of-caret-on-cran/index.html",
    "title": "New Version of caret on CRAN",
    "section": "",
    "text": "A new version of caret is on CRAN.\n\n\n\n\n\nSome recent features/changes:\n\nThe license was changed to GPL &gt;= 2 to accommodate new code from the GA package.\nNew feature selection functions gafs and safs were added, along with helper functions and objects, were added. The package HTML was updated to expand more about feature selection. I’ll talk more about these functions in an upcoming blog post.\nA reworked version of nearZerVar based on code from Michael Benesty was added the old version is now called nzv that uses less memory and can be used in parallel.\nsbfControl now has a multivariate option where all the predictors are exposed to the scoring function at once.\nSeveral regression simulation functions were added: SLC14_1, SLC14_2, LPH07_1 and LPH07_2\nFor the input data x to train, we now respect the class of the input value to accommodate other data types (such as sparse matrices).\nA function update.rfe was added.\n\nRecently added models:\n\nFrom the adabag package, two new models were added: AdaBag and AdaBoost.M1.\nWeighted subspace random forests from the wsrf package was added.\nAdditional bagged FDA and MARS models were added (model codes bagFDAGCV and bagEarthGCV) were added that use the GCV statistic to prune the model. This leads to memory reductions during training.\nBrenton Kenkel added ordered logistic or probit regression to train using method = \"polr\" from MASS\nThe adaptive mixture discriminant model from the adaptDA package\nA robust mixture discriminant model from the robustDA package was added.\nThe multi-class discriminant model using binary predictors in the binda package was added.\nEnsembles of partial least squares models (via the enpls package) was added.\nplsRglm was added.\nFrom the kernlab package, SVM models using string kernels were added: svmBoundrangeString, svmExpoString, svmSpectrumString\nThe model code for ada had a bug fix applied and the code was adapted to use the “sub-model trick” so it should train faster.\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/nonclinical-statistician-position-at-pfizer/index.html",
    "href": "posts/nonclinical-statistician-position-at-pfizer/index.html",
    "title": "Nonclinical Statistician Position at Pfizer",
    "section": "",
    "text": "The Research Statistics group collaborates across a wide variety of activities in the early phases of drug discovery. This position is located in Groton CT and has a focus on the optimization of chemical matter and the development of assays to characterize these molecules\nThe successful candidate will:\n\nDemonstrate leadership in influencing and improving drug discovery by identifying, developing, and applying new quantitative methods.\nProactively seek collaborations with scientists and lab heads.\nCollaborate with scientists to plan meaningful studies, statistically analyze, and communicate / document the results.\n\n\nRequirements:\n\nM.S. or Ph.D. in Statistics, Biostatistics, or related field and 2+ years statistical consulting experience in drug discovery and development, preferably in a laboratory science environment.\nThe ability to proactively seek collaborations with scientists and lab heads.\nStrong initiative, excellent interpersonal and communication (written and verbal) skills\nUnderstanding of inference and probability; competence in contemporary linear modeling including mixed models, nonlinear regression; and predictive modeling/machine learning.\nSolid understanding of experimental design\nAn understanding of tools for the analysis of high dimensional data\nStrong computational skills in R\n\n\n\nDesired:\n\nFive or more years experience in the pharmaceutical industry.\nSound understanding and experience of applying Bayesian methods\nFormal training in, or thorough understanding of: human physiology, cell biology, pharmacokinetics and/or pharmaceutical chemistry.\nStrong computing skills in scripting languages, such as perl, python, unix shell scripts or others. SQL and LaTeX skills are also advantageous.\n\n\n\nApplying\nThe position is posted at http://pfizercareers.com/\n\nGo to “Search jobs” on the green tab\nChoose “SEARCH and APPLY for Jobs”\nIn the “Advanced Search” bar, search on Job Id 1024297\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/nonclinical-statistics-position-in-new-england/index.html",
    "href": "posts/nonclinical-statistics-position-in-new-england/index.html",
    "title": "Nonclinical Statistics Position in New England",
    "section": "",
    "text": "I try to limit postings about jobs here, there is an interesting position in pharma for a statistician in New England.\nI’ve had this particular position before as a nonclinical statistician and really loved the work and the scientific collaborations. It is the kind of position that lets you use a wide set of skills; there is very little rote data analysis.\nLink: Nonclinical Statistician\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/optimizing-probability-thresholds-for-class-imbalances/index.html",
    "href": "posts/optimizing-probability-thresholds-for-class-imbalances/index.html",
    "title": "Optimizing Probability Thresholds for Class Imbalances",
    "section": "",
    "text": "One of the toughest problems in predictive model occurs when the classes have a severe imbalance. We spend an entire chapter on this subject itself. One consequence of this is that the performance is generally very biased against the class with the smallest frequencies. For example, if the data have a majority of samples belonging to the first class and very few in the second class, most predictive models will maximize accuracy by predicting everything to be the first class. As a result there’s usually great sensitivity but poor specificity.\nAs a demonstration will use a simulation system described here. By default it has about a 50-50 class frequency but we can change this by altering the function argument called intercept:\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nset.seed(442)\ntraining &lt;- twoClassSim(n = 1000, intercept = -16)\ntesting &lt;- twoClassSim(n = 1000, intercept = -16)\n\nIn the training set the class frequency breakdown looks like this:\n\ntable(training$Class)\n\n\nClass1 Class2 \n   899    101 \n\n## Class1 Class2 \n##    899    101 \n\nThere is almost a 9:1 imbalance in these data.\nLet’s use a standard random forest model with these data using the default value of mtry. We’ll also use 10-fold cross validation to get a sense of performance:\nset.seed(949)\nmod0 &lt;- train(Class ~ ., data = training,\n              method = \"rf\",\n              metric = \"ROC\",\n              tuneGrid = data.frame(mtry = 3),\n              trControl = trainControl(method = \"cv\",\n                                       classProbs = TRUE,\n                                       summaryFunction = twoClassSummary))\ngetTrainPerf(mod0)\n##    TrainROC TrainSens TrainSpec method\n## 1 0.9288911      0.99 0.4736364     rf\nThe area under the ROC curve is very high, indicating that the model has very good predictive power for these data. Here’s a test set ROC curve for this model:\n\n\n\n\nThe plot shows the default probability cut off value of 50%. The sensitivity and specificity values associated with this point indicate that performance is not that good when an actual call needs to be made on a sample.\nOne of the most common ways to deal with this is to determine an alternate probability cut off using the ROC curve. But to do this well, another set of data (not the test set) is needed to set the cut off and the test set is used to validate it. We don’t have a lot of data this is difficult since we will be spending some of our data just to get a single cut off value.\nAlternatively the model can be tuned, using resampling, to determine any model tuning parameters as well as an appropriate cut off for the probabilities.\nThe latest update to the caret package allows users to define their own modeling and prediction components. This also gives us a huge amount of flexibility for creating your own models or doing some things that were originally intended by the package. This page shows a lot of the details for creating custom models.\nSuppose the model has one tuning parameter and we want to look at four candidate values for tuning. Suppose we also want to tune the probability cut off over 20 different thresholds. Now we have to look at 20×4=80 different models (and that is for each resample). One other feature that has been opened up his ability to use sequential parameters: these are tuning parameters that don’t require a completely new model fit to produce predictions. In this case, we can fit one random forest model and get it’s predicted class probabilities and evaluate the candidate probability cutoffs using these same hold-out samples. Again, there’s a lot of details on this page and, without going into them, our code for these analyses can be found here.\nBasically, we define a list of model components (such as the fitting code, the prediction code, etc.) and feed this into the train function instead of using a pre-listed model string (such as method = “rf”). For this model and these data, there was an 8% increase in training time to evaluate 20 additional values of the probability cut off.\nHow do we optimize this model? Normally we might look at the area under the ROC curve as a metric to choose our final values. In this case the ROC curve is independent of the probability threshold so we have to use something else. A common technique to evaluate a candidate threshold is see how close it is to the perfect model where sensitivity and specificity are one. Our code will use the distance between the current model’s performance and the best possible performance and then have train minimize this distance when choosing it’s parameters. Here is the code that we use to calculate this:\nfourStats &lt;- function (data, lev = levels(data$obs), model = NULL) {\n  ## This code will get use the area under the ROC curve and the\n  ## sensitivity and specificity values using the current candidate\n  ## value of the probability threshold.\n  out &lt;- c(twoClassSummary(data, lev = levels(data$obs), model = NULL))\n \n  ## The best possible model has sensitivity of 1 and specifity of 1. \n  ## How far are we from that value?\n  coords &lt;- matrix(c(1, 1, out[\"Spec\"], out[\"Sens\"]), \n                   ncol = 2, \n                   byrow = TRUE)\n  colnames(coords) &lt;- c(\"Spec\", \"Sens\")\n  rownames(coords) &lt;- c(\"Best\", \"Current\")\n  c(out, Dist = dist(coords)[1])\n}\nNow let’s run our random forest model and see what it comes up with for the best possible threshold:\nset.seed(949)\nmod1 &lt;- train(Class ~ ., data = training,\n              ## 'modelInfo' is a list object found in the linked\n              ## source code\n              method = modelInfo,\n              ## Minimize the distance to the perfect model\n              metric = \"Dist\",\n              maximize = FALSE,\n              tuneLength = 20,\n              trControl = trainControl(method = \"cv\",\n                                       classProbs = TRUE,\n                                       summaryFunction = fourStats))\nThe resulting model output notes that:\n## Tuning parameter 'mtry' was held constant at a value of 3\n## Dist was used to select the optimal model using  the smallest value.\n## The final values used for the model were mtry = 3 and threshold = 0.887.\nUsing ggplot(mod1) will show the performance profile. Instead here is a plot of the sensitivity, specificity, and distance to the perfect model:\n\n\n\n\nYou can see that as we increase the probability cut off for the first class it takes more and more evidence for a sample to be predicted as the first class. As a result the sensitivity goes down when the threshold becomes very large. The upside is that we can increase specificity in the same way. The blue curve shows the distance to the perfect model. The value of 0.887 was found to be optimal.\nNow we can use the test set ROC curve to validate the cut off we chose by resampling. Here the cut off closest to the perfect model is 0.881. We were able to find a good probability cut off value without setting aside another set of data for tuning the cut off.\nOne great thing about this code is that it will automatically apply the optimized probability threshold when predicting new samples. Here is an example:\n  Class1 Class2  Class Note\n1  0.874  0.126 Class2    *\n2  1.000  0.000 Class1     \n3  0.930  0.070 Class1     \n4  0.794  0.206 Class2    *\n5  0.836  0.164 Class2    *\n6  0.988  0.012 Class1  \nHowever we should be careful because the probability values are not consistent with our usual notion of a 50-50 cut off.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/projection-pursuit-classification-trees/index.html",
    "href": "posts/projection-pursuit-classification-trees/index.html",
    "title": "Projection Pursuit Classification Trees",
    "section": "",
    "text": "I’ve been looking at this article for a new tree-based method. It uses other classification methods (e.g. LDA) to find a single variable use in the split and builds a tree in that manner. The subtleties of the model are:\n\nThe model does not prune but keeps splitting until achieving purity\nWith more than two classes, it treats the data as a two-class system in some parts of the algorithm (but predictions are still based on the original classes)\n\nIt is similar to oblique trees. These trees look for linear combinations of predictors to use in a split. The similarity between oblique trees and PPtree is the method of finding splits. In each case, a more parametric model can be used for this purpose. Some implementations of oblique trees use PLS, L2 regularization or linear support vector machines to find the optimal combination. Here, the authors use basic discriminant functions but using only a single predictor at a time. This connection wasn’t mentioned in the paper (and comparisons were not made to these methods). They compared to CART and random forests. That’s disappointing because there are a lot of other tree-based models and we have no idea how this model ranks among them (see Hand’s “Classifier Technology and the Illusion of Progress”).\nMy intuition tells me that the PPtree model is somewhat less likely to over-flit the data. While it lacks a pruning algorithm, the nature of the splitting method might make it more robust to small fluctuations in the data. One way to diagnose this is using more comprehensive cross-validation and also assessing whether bagging helps this model. The splitting approach should also reduce the potential problem of bias towards predictors that are more granular. One other consequence of their tree-growing phase is that it eliminates the standard method of generating class probabilities (since it splits until purity).\nPPtrees might do a better job when there are a few linear predictors that drive classification accuracy. This could have been demonstrated using simulation of some sort.\nA lot of tree methods have sub-algorithms for grouping categorical predictors. This model only works with such data as a set of disconnected dummy variables. This isn’t good or bad since I have found a lot of variation in which type of encoding works with different tree methods.\nThe bad news: the method is available in an R package, but there are big implementation issues (to me at least). The package strikes me as a tool for research only (as opposed to software that would enable PPtrees to be used in practice). For example:\n\nIt ignores basic R conventions (like returning factor data for predictions)\nIt also ignores object oriented programming. For example, there is no predict method. That function is named PP.classify.\nSpeaking of PP.classify, you have to trick the code into giving predictions on unknown samples. That is a big red flag to me.\nLittle things are missing (e.g. no print or plot method). They could have used the partykit package to beautifully visualize the tree.\n\nI’ve ranted about these issues before and the package violates most of my checklist. Maybe this is just part of someone’s dissertation and maybe they didn’t know about this list etc. However, most of the items above should have been avoided.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/regression-solutions-available/index.html",
    "href": "posts/regression-solutions-available/index.html",
    "title": "Regression Solutions Available",
    "section": "",
    "text": "The github page for the APM exercises has been updated with three new files for Chapters 6-8 (the section on regression).\nThe classifications section is in-progress.\nHere’s one of our fancy-pants graphs:\n\n\n\n\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/rmedicine-conference/index.html",
    "href": "posts/rmedicine-conference/index.html",
    "title": "R/Medicine conference",
    "section": "",
    "text": "I’ll be giving a talk at the R/Medicine conference on Sept 7th in New Haven CT.\nMy talk is on modeling in the tidyverse but there are some excellent speakers. Rob Tibshirani, Mike Lawrence, Jennifer Thompson, and a bunch of others will be there.\nTake look at the conference website for more details."
  },
  {
    "objectID": "posts/sample-mislabeling-and-boosted-trees/index.html",
    "href": "posts/sample-mislabeling-and-boosted-trees/index.html",
    "title": "Sample Mislabeling and Boosted Trees",
    "section": "",
    "text": "A while back, I saw this post on StackExchange/Crossvalidated: “Does anyone know how well C5.0 boosting performs in the presence of mislabeled data?” I did some simulations in order to make a comparison with gradient boosting machines (GBM).\nSome publications simulate mislabelling by sampling from distinct multivariate distirbutions for each class. I simulated two class data based on this post. Each simulated sample has an associated probability of being in class #1. A random uniform number is generated to assign each sample and observed class label. To mislabel X% of the data, a random set of samples are selected and the probability of being in class #1 is reversed.\nSimulated data sets were simulated with training set sizes between 100 and 1000. The amount of mislabeled data also varied (at 5%, 10%, 15% and 20%). For each mislabeled data set, there is a matched training set (form the same random number seed) with no intentional mislabeling. For each of these configurations, 500 simulations were conducted.\nModel performance was assessed using the area under the ROC curve. A test set of 10,000 with no mislabeling was used to evaluate performance.\nFor the C5.0 and GBM models, models were tuned using cross-validation. For each technique, a model was trained on the mislabeled data and another on the correctly labeled data. In this way, we have a “control” model that reflects how well the model does for each data set if there were no added noise due to mislabeling. As a result, for C5.0 and GBM, a percentage of performance loss can be calculated against the correctly labelled control set:\npct = (mislabeled - correct)/correct*100). \nThis image contains the distributions of the percent loss across the configurations:\n\n\n\n\n\nSome other observations:\n\nWhen there is no mislabeling, the results are almost identical\nSmall amounts of mislabeling do not hurt either model very much\nThe loss of performance decreases with training set size\nWith gross amounts of mislabeling, the gradient boosting machine is not affected as much as C5.0\nThe effect of mislabeling on C5.0 also impacts the variation in the results. If you compare the columns above, note that the C5.0 distribution does not simply shift to the left with the same level of variation.\n\nI contacted Ross Quinlan about this and his response was:\n\n“I agree with your conclusions for the function that you studied. My experiments with noise and AdaBoost suggested that the effect of noise (mislabeling) varies markedly with different applications. There are some summary results in the first part of the following:\n\n\nhttp://rulequest.com/Personal/q.alt96.ps\n\n\nI have only some vague ideas about why thus might be. For those applications where the classes are well-separated in the attribute space, mislabeling does not seem to alter the class clusters much. Alternatively, for applications where there is a tight boundary between two classes, mislabeling could markedly affect the perceived class divide.”\n\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/slides-from-recent-talks/index.html",
    "href": "posts/slides-from-recent-talks/index.html",
    "title": "Slides from recent talks",
    "section": "",
    "text": "I’ve been buried in work lately but thought I’d share the slides from two recent talks. The first is from the Bay Area RUG. Since someone filmed the talks, I was waiting to post the slides. The video of my talk (“Greatest Hits R Mixtape”) isn’t availible yet, so here are the slides.\nThe second talk is from last week’s Thirteenth Annual ASA CT Chapter Mini-Conference. Here are the slides from “Three Aspects of Predictive Modeling”.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/solutions-on-github/index.html",
    "href": "posts/solutions-on-github/index.html",
    "title": "Solutions on github",
    "section": "",
    "text": "See this page. We’re not done with them all but chapter 3 and 4 are there and the regression chapters are not too far behind.\nThe Rnw files (using knitr LaTeX) are there along with the corresponding pdf files.\nYou may have better solutions than we have here and we would love to see them. You can do so by creating a pull request or, if you are not git-savvy, drop an email to Max (max.kuhn@pfizer.com) and/or Kjell (kjell@arboranalytics.com).\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/the-2014-ziegel-award/index.html",
    "href": "posts/the-2014-ziegel-award/index.html",
    "title": "The 2014 Ziegel Award",
    "section": "",
    "text": "Last night in Seattle, Kjell and I were awarded the American Statistical Association’s Ziegel Award for the best book reviewed in Technometrics during 2014. Technometrics is the journal for the Physical, Chemical, and Engineering Sciences published by the American Statistical Association.\nIt was quite an honor and we would like to thank Ejaz Ahmed (the Review Editor) and David Steinberg (the Chair of the Management committee).\nI love that the award certificate has the famous green color from the journal cover. When I lived in Virginia, I used to get teased about bringing that “weird colored journal thingy” to the Outer Banks to read on vacation.\nIn sadder APM news, Hannah Bracken is leaving Springer. She was invaluable during the process of writing the book as well as after the release. While it is sad news for us, she is going to school in Paris. It is probably more accurate to label us as “jealous”. We wish her the best!\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/tidy-resampling-redux-with-agricultural-economics-data/index.html",
    "href": "posts/tidy-resampling-redux-with-agricultural-economics-data/index.html",
    "title": "Tidy Resampling Redux with Agricultural Economics Data",
    "section": "",
    "text": "(No statistical graphs in this one. This is what my dog Artemis looks like when she wants my attention during work hours.)\nMindy L. Mallory (@ace_prof) wrote a blog post on Machine Learning and Econometrics: Model Selection and Assessment Statistical Learning Style where she has a great description of the variance-bias tradeoff, resampling, and model complexity using some data from agricultural economics. I asked if I could take her code and make a more tidy version of the resampling part. Here it is…\nHere are the R packages that I’ll be using:\nlibrary(tidyverse)\nlibrary(rlang)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(purrr)\nFirst, duplicate the process of reading in the data and adding two new columns:\nstocks  &lt;- read.csv('http://blog.mindymallory.com/wp-content/uploads/2018/02/stocks.csv') %&gt;% \n  as_tibble() %&gt;%\n  mutate(\n    USStockUse = USEndingStocks/USTotalUse, \n    WorldStockUse = ROWEndingStocks/WorldTotalUse\n  )\nstocks\n## # A tibble: 43 x 8\n##     Year USEndingStocks ROWEndingStocks USTotalUse\n##    &lt;int&gt;          &lt;dbl&gt;           &lt;int&gt;      &lt;dbl&gt;\n##  1  1975           633.           36411      5767.\n##  2  1976          1136.           39491      5789.\n##  3  1977          1436.           40833      6207.\n##  4  1978          1710.           47957      6995.\n##  5  1979          2034.           59481      7604.\n##  6  1980          1392.           67180      7282.\n##  7  1981          2537.           62725      6975.\n##  8  1982          3523.           60273      7249.\n##  9  1983          1006.           63421      6693.\n## 10  1984          1648.           76287      7032.\n## # ... with 33 more rows, and 4 more variables:\n## #   WorldTotalUse &lt;int&gt;, PriceRecievedFarmers &lt;dbl&gt;,\n## #   USStockUse &lt;dbl&gt;, WorldStockUse &lt;dbl&gt;\nThe blog post has an excellent description of cross-validation and looked at five different models that encoded the US and World stock-use predictors. Either a log- or inverse-transformation was applied and then polynomial basis functions were used on these features to demonstrate overfitting.\nThe blog post has some for loops to do the resampling and I volunteered to show how to do it with some tidy modeling packages.\n\nTidy Cross-Validation\nFirst, let’s take the easy part. Instead of using for loops, we can use the new infrastructure in the tidyverse to resample the model. The rsample package has some functions for different types of resampling and we will use the same procedure as the original post:\nset.seed(918)\nresamp_info &lt;- vfold_cv(stocks, v = 5)\nresamp_info\n## #  5-fold cross-validation \n## # A tibble: 5 x 2\n##   splits       id   \n##   &lt;list&gt;       &lt;chr&gt;\n## 1 &lt;S3: rsplit&gt; Fold1\n## 2 &lt;S3: rsplit&gt; Fold2\n## 3 &lt;S3: rsplit&gt; Fold3\n## 4 &lt;S3: rsplit&gt; Fold4\n## 5 &lt;S3: rsplit&gt; Fold5\nThe first column in the tibble is a set of “rsplit” objects that define how the data are split for each fold of cross-validation. Each one fully and efficiently encapsulates everything what is needed to get the two divisions of the original data. In rsample, to avoid naming confusion, we label the two resulting data sets as:\n\nThe analysis data are those that we selected in the resample. For a bootstrap, this is the sample with replacement. For 5-fold cross-validation, this is the 80% of the data. These data are often used to fit a model or calculate a statistic in traditional bootstrapping.\nThe assessment data are usually the section of the original data not covered by the analysis set. Again, in 5-fold CV, this is the 20% held out. These data are often used to evaluate the performance of a model that was fit to the analysis data.\n\nTo get these partitions for the first split there are functions analysis and assessment that return the appropriate data frames when given an rsplit:\n# printing just shows the #rows per analysis/assessment/overall\nresamp_info$splits[[1]]\n## &lt;34/9/43&gt;\n# data used for modeling:\nanalysis(resamp_info$splits[[1]])\n## # A tibble: 34 x 8\n##     Year USEndingStocks ROWEndingStocks USTotalUse\n##    &lt;int&gt;          &lt;dbl&gt;           &lt;int&gt;      &lt;dbl&gt;\n##  1  1975           633.           36411      5767.\n##  2  1976          1136.           39491      5789.\n##  3  1977          1436.           40833      6207.\n##  4  1979          2034.           59481      7604.\n##  5  1980          1392.           67180      7282.\n##  6  1981          2537.           62725      6975.\n##  7  1982          3523.           60273      7249.\n##  8  1983          1006.           63421      6693.\n##  9  1984          1648.           76287      7032.\n## 10  1985          4040.           75069      6494.\n## # ... with 24 more rows, and 4 more variables:\n## #   WorldTotalUse &lt;int&gt;, PriceRecievedFarmers &lt;dbl&gt;,\n## #   USStockUse &lt;dbl&gt;, WorldStockUse &lt;dbl&gt;\nThe first model for the data contained the US stock-use data with an inverse transformation. Let’s side-step the polynomial model tuning for now and just fit a quadratic model. To make things easier, I’ll define a function that can be used to fit the model when given an rsplit object and return the holdout mean squared error (MSE):\nglm_results &lt;- function(split, ...) {\n  # Get the data used ot fit the model aka the \"analysis\" set\n  # and fit the model with a formula given in the ...\n  mod &lt;- glm(data = analysis(split), ...)\n  \n  # Get predictions on the other data (aka the \"assessment\" set \n  # and compute some metrics\n  holdout &lt;- assessment(split)\n  \n  # Compute performance using the yardstick package\n  rmse &lt;- holdout %&gt;%\n    mutate(pred = predict(mod, holdout)) %&gt;%\n    rmse(truth = PriceRecievedFarmers, estimate = pred)\n  rmse^2\n}\nWe can use this with any formula since it is just passed to glm using the ellipses. For example to get the holdout MSE estimate for the first fold:\nglm_results(resamp_info$splits[[1]], formula = PriceRecievedFarmers ~ poly(1 / USStockUse, 2))\n## [1] 1.67\nTo get these statistics for all folds, purrr::map_dbl is used to add another column:\nresamp_info &lt;- resamp_info %&gt;%\n  mutate(\n    model_1_deg_2 = \n      map_dbl(\n        splits, \n        glm_results, \n        PriceRecievedFarmers ~ poly(1 / USStockUse, 2)\n      )\n  )\nresamp_info\n## #  5-fold cross-validation \n## # A tibble: 5 x 3\n##   splits       id    model_1_deg_2\n## * &lt;list&gt;       &lt;chr&gt;         &lt;dbl&gt;\n## 1 &lt;S3: rsplit&gt; Fold1         1.67 \n## 2 &lt;S3: rsplit&gt; Fold2         0.558\n## 3 &lt;S3: rsplit&gt; Fold3         0.596\n## 4 &lt;S3: rsplit&gt; Fold4         9.58 \n## 5 &lt;S3: rsplit&gt; Fold5         0.248\nThat’s a lot of variation in the outcome! The mean value is fairly consistent with the blog post though:\nresamp_info %&gt;% select(model_1_deg_2) %&gt;% colMeans()\n## model_1_deg_2 \n##          2.53\n# MSE = 2.675 in the blog post\nK-fold cross-validation is one of the noisiest resampling methods so this difference isn’t too surprising.\nThis same process could be repeated for each polynomial degree to get new columns for this model (we’ll discuss this below). The good things about doing things this way:\n\nIt is a lot cleaner (so far) than doing for loops.\nOther tidyverse infrastructure can be used. For example, tidyposterior is a great way to do model comparisons with resampling and Bayesian analysis.\nIt is simple to change resampling methods. Suppose you wanted to change to a larger number of bootstrap resamples (given the variance shown above). The same infrastructure can be easily exchanged; resample::bootstraps is used in place of rsample::vfold_cv.\n\n\n\nTidy Model Specification (maybe)\nThe model specification part is, for me, a lot more difficult to tidy. It would be good to be able to state what predictors that we want, specify the polynomial degree, and have a function to generate the appropriate formula. The original post sensibly just types the terms out.\nI spent some time thinking about how we could use expressions and tidy evaluation (video from Hadley) to make it a little less script-like. The problem is that the solution took me a while to write and, arguably, it doesn’t really buy you much more than the original code (apart from the potential copy/paste duplication errors).\nIn any case, the function uses rlang to manipulate expressions to make the formula. The inputs are expressions of how the predictors are used (i.e. log, inverse, etc.) and the degree. It captures the expression (without evaluating it), substitutes it into the polynomial function, then creates the formula:\nmake_formula &lt;- function(..., degree = 1) {\n  # Capture the expression so it is not evaluated\n  var_expr &lt;- exprs(...)\n  \n  # Create a template expression\n  inv_poly &lt;- quote(poly(x = x, degree = degree))\n\n  # Use a wrapper around rlang::call_modify to reverse the\n  # order of the arguments so that we can map over the\n  # predictor expressions\n  add_args &lt;- function(arg, call, ...)\n    call_modify(call, x = arg, ...)\n  \n  # Add the variables and the degree into the template\n  poly_expr &lt;- map(var_expr, add_args, call = inv_poly, degree = degree)\n  \n  # Convert to character\n  poly_char &lt;- map_chr(poly_expr, deparse)\n  \n  # Convert to a formula\n  poly_char &lt;- paste(poly_char, collapse = \" + \")\n  as.formula(paste(\"PriceRecievedFarmers ~\", poly_char))\n}\n# For example:\nmake_formula(1/USStockUse, log(WorldStockUse), degree = 3)\n## PriceRecievedFarmers ~ poly(x = 1/USStockUse, degree = 3) + poly(x = log(WorldStockUse), \n##     degree = 3)\n## &lt;environment: 0x7fdf40b64840&gt;\nFrom here, we could use a bunch of mutate commands like the one shown above or write a slightly smaller for loop to work across the polynomial degrees. While the function above works well, the overall approach to working across models isn’t particularly satisfying.\nAny suggestions? I’m on twitter @topepos!\n(edit - fix fixed the formula call!)\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/tidyposterior-slides/index.html",
    "href": "posts/tidyposterior-slides/index.html",
    "title": "tidyposterior slides",
    "section": "",
    "text": "tidyposterior is an R package for comparing models based on their resampling statistics. There are a few case studies on the webpage to illustrate the process.\nI gave a talk at the Open Data Science Conference (ODSC) yesterday. A pdf of the slides are here.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/user-2013-highlights/index.html",
    "href": "posts/user-2013-highlights/index.html",
    "title": "UseR! 2013 Highlights",
    "section": "",
    "text": "The conference was excellent this year. My highlights:\n\nBojan Mihaljevic gave a great presentation on machine learning models built from network models. Their package isn’t on CRAN yet, but I’m really looking forward to it.\nJim Harner’s presentation on K-NN models with feature selection was also very interesting, especially the computational aspects of training the model in parallel.\nTal Galili gave a pretty interesting summary of models on tRNA seqeunces. This was a great case study in cross-validation. A simple CV loop was compared with one that used the two Archaea families. The results were very different.\nThe first session on Computational Challenges in Molecular Biology showed a set of applications of modeling in biology. For me, the presentations of Thomas Poulsen and Insa Winzenborg really stood out (in a positive way).\nHadley Wickham’s talk on big data and R was excellent. His new dplyr package should solve a lot of problems for me. I find his work to be excellent despite one member of R Core remarking to me that caret should not depend on plyr since it was considered by them to be “unreliable”. I feel that nothing is farther form the truth.\nAnother RStudio-ista, Joe Cheng, gave yet another great presentation on shiny.\nFinally, I really enjoy the lightning talks. The presentation was setup so that the speaker could have 15 slides and each slide will be shown for 20 seconds. It is sort of a combination of a talk and a game of boggle.\n\nI’m hoping that the presentations will be posted on the website.\nAlbacete was very nice, as was the food and beer. Next year, the conference will be in Los Angeles. It should be a lot of fun.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/user-slides-for-classification-using-c50/index.html",
    "href": "posts/user-slides-for-classification-using-c50/index.html",
    "title": "UseR! Slides for “Classification Using C5.0”",
    "section": "",
    "text": "I’ve had a lot of requests, so here they are. Hopefully, all of the slides will be posted on the conference website.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/what-was-left-out/index.html",
    "href": "posts/what-was-left-out/index.html",
    "title": "What Was Left Out",
    "section": "",
    "text": "There were a few topics that just couldn’t be added to the book due to time and, especially, space. For a project like this, the old saying is “you’re never done, you just stop working on it”.\nFirst, the generalized additive model is one of my favorites. It is simple, effective and has the added bonus of of giving the user an idea of the functional form of relationship between the predictor and outcome. We describe the same ideas for cubic smoothing splines, but the GAM model is probably more flexible.\nTime series models were also excluded since they are fundamentally different than the models we describe (where the training set points are independent). There is a strong literature on using neural networks for these models. I would recommend Rob Hyndman’s book on this subject.\nFinally, ensembles of different models were not included. For example, for a particular data set, a random forest, support vector machine and naive Bayes might be fit to the training data and their individual predictions could be combined into a single prediction per sample. We do discuss other ensemble models (e.g. boosting, bagging etc). Good resources on this topic and Seni and Elder (2010) and [Kuncheva (2004)](http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471210781.html.\nAt least this gives us some topics for a potential second addition.\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/working-at-rstudio/index.html",
    "href": "posts/working-at-rstudio/index.html",
    "title": "Working at RStudio",
    "section": "",
    "text": "I’ve joined Hadley’s team at RStudio.\nUnsurprisingly, I’ll be working on some modeling related R packages and infrastructure. It is very exciting and I’m looking forward to learning a lot and creating some cool things.\nI’ve had a great time doing drug discovery at Pfizer for the last 12ish years and I’ll miss working with everyone there.\nMy contact info has been changed on my existing R packages and you can find me either using my personal email (mxkuhn@gmail.com), RStudio address (max@rstudio.com), or github page\n(This article was originally posted at http://appliedpredictivemodeling.com)"
  },
  {
    "objectID": "posts/two-new-preprocessing-chapters/index.html",
    "href": "posts/two-new-preprocessing-chapters/index.html",
    "title": "Two New Preprocessing Chapters",
    "section": "",
    "text": "We just released two new chapters: “Transforming Numeric Predictors” and “Working with Categorical Predictors.”\nThe first talks about simple transformations of scale and outlier mitigation. It also discusses the important topic of when and how preprocessors should be trained.\nThe second new chapter introduces basic indicator/dummy variables and more complex encoding methods using hashing and target encodings.\nThe tidymodels code for these chapters will be forthcoming in a few weeks; the tidymodels group has a series of CRAN releases underway, and there are some huge new features that we are documenting and writing technical materials for.\nAlso, we’ve moved some content out of our new chapter four and into an upcoming chapter on embeddings. That will discuss PCA, PLS, multidimensional scaling, and other tools.\nFinally, we are always interested in reviewers. If you are well-versed in a particular subject, let us know and we can add you as a reviewer for pull requests."
  },
  {
    "objectID": "posts/tidymodels-survival-models/index.html",
    "href": "posts/tidymodels-survival-models/index.html",
    "title": "Predictive Survival Analysis",
    "section": "",
    "text": "The tidymodels group just released new versions of the core packages that enable (among other things) models for outcomes with censoring.\nCensoring is a situation, usually seen in time-to-event data, where we have partial information. For example, suppose we order something online that is expected to be delivered in 2 days. After a day, we don’t know the actual delivery time, but we know that the value is at least a day. This data point is right censored at a value of 1 day.\ntidymodels.org has a few articles on modeling these data with the new functionality.\nThe main distinction for these models is how you quantify model performance. Most modern survival models don’t focus on the predicted event time but emphasize predicting the probability of the event not occurring (e.g., surviving) up to time point \\(t_0\\). Because of this, we need to use dynamic performance metrics: these are metrics that judge performance at different points in time. Here’s a plot from an analysis where the Brier statistic is computed over a relevant time range:\n\n\n\n\n\n\n\n\nIn this case, the large Brier score values at the first time point indicates mediocre performance. As the evaluation time progresses, the score becomes smaller (which is good) and the model does very well.\nTo include this type of model, there weren’t many syntax changes:\n\nMany functions now have an eval_time argument to take a vector of time points to evaluate performance measures.\nThere are some new performance statistics.\nBefore modeling, you should probably create a Surv object.\n\nHopefully, we will soon be doing specific tidymodels tutorials on this subject (perhaps at useR). We also have two talks accepted at Posit Conf later this year."
  },
  {
    "objectID": "2024-03-talks/index.html",
    "href": "2024-03-talks/index.html",
    "title": "February 2024 Talks",
    "section": "",
    "text": "Two significant talks last month.\nOn March 19th, I gave the final keynote at the inaugural Pharmaceutical Data Science Conference at the University of Connecticut. The talk was called “What Happens After the Model?”\n\nMachine learning models are everywhere now. We must spend more time on what happens before and after the model fit to build higher-quality algorithms. This talk will describe a set of post-model activities that can improve the fit and also ensure that, when deployed, it is used effectively.\n\nYou can find the slides at topepo.github.io/2024_PharmaDS/ and these contain a link to the sources.\nSecond was tutorial for the Statistical Society of Australia called “Introduction to Machine Learning with tidymodels”. The slides are at topepo.github.io/2024_SSA_SCV/ and the sources are at github.com/topepo/2024_SSA_SCV. A huge thanks go to Simon Couch who write the first (very slick) section of the slides."
  },
  {
    "objectID": "posts/2024-03-talks/index.html",
    "href": "posts/2024-03-talks/index.html",
    "title": "February 2024 Talks",
    "section": "",
    "text": "One keynote and one tutorial last month (both new).\nOn March 19th, I gave the final keynote at the inaugural Pharmaceutical Data Science Conference at the University of Connecticut. The talk was called “What Happens After the Model?”\n\nMachine learning models are everywhere now. We must spend more time on what happens before and after the model fit to build higher-quality algorithms. This talk will describe a set of post-model activities that can improve the fit and also ensure that, when deployed, it is used effectively.\n\nYou can find the slides at topepo.github.io/2024_PharmaDS/ and these contain a link to the sources.\nSecond was tutorial for the Statistical Society of Australia called “Introduction to Machine Learning with tidymodels”. The slides are at topepo.github.io/2024_SSA_SCV/ and the sources are at github.com/topepo/2024_SSA_SCV. A huge thanks go to Simon Couch who write the first (very slick) section of the slides."
  },
  {
    "objectID": "posts/data-usage-for-postprocessors/index.html#a-regression-example",
    "href": "posts/data-usage-for-postprocessors/index.html#a-regression-example",
    "title": "Data Usage for Postprocessors",
    "section": "A Regression Example",
    "text": "A Regression Example\nWe’ll use the food delivery data and start with a three-way split:\n\nlibrary(tidymodels)\nlibrary(bonsai)\nlibrary(probably)\n\n# ------------------------------------------------------------------------------\n\ntidymodels_prefer()\ntheme_set(theme_bw())\noptions(pillar.advice = FALSE, pillar.min_title_chars = Inf)\n\n# ------------------------------------------------------------------------------\n\ndata(deliveries, package = \"modeldata\")\n\nset.seed(991)\ndelivery_split &lt;- initial_validation_split(deliveries, prop = c(0.6, 0.2), \n                                           strata = time_to_delivery)\ndelivery_train &lt;- training(delivery_split)\ndelivery_test  &lt;- testing(delivery_split)\ndelivery_val   &lt;- validation(delivery_split)\n\nLet’s deliberately fit a regression model that has poor predicted values: a boosted tree with only three ensemble members:\n\nbst_wflow &lt;-\n  boost_tree(trees = 3) %&gt;%\n  set_engine(\"lightgbm\") %&gt;%\n  set_mode(\"regression\")\n\nbst_fit &lt;- bst_wflow %&gt;% fit(time_to_delivery ~ ., data = delivery_train)\n\nWe predict the validation set and see how bad things are:\n\nreg_metrics &lt;- metric_set(rmse, rsq)\n\nbst_val_pred &lt;- augment(bst_fit, delivery_val)\nreg_metrics(bst_val_pred, truth = time_to_delivery, estimate = .pred)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       5.46 \n2 rsq     standard       0.850\n\n\nThat R2 looks great! How well is it calibrated?\n\ncal_plot_regression(bst_val_pred, truth = time_to_delivery, estimate = .pred)\n\n\n\n\n\n\n\nNormally, we’d try different models that don’t have such an issue. Let’s assume that, for some reason, this is the model that we will productionize if we can make it well-calibrated.\nWe can fit a model that predicts the true values as a function of the predicted values. In this example, it might be enough to use a linear fit and use the slope and intercept to move the predictions to the diagonal reference line. This wouldn’t affect the R2 much but the RMSE would really improve.\nBut exactly what data should we use?"
  },
  {
    "objectID": "posts/data-usage-for-postprocessors/index.html#data-for-training-postprocessors",
    "href": "posts/data-usage-for-postprocessors/index.html#data-for-training-postprocessors",
    "title": "Data Usage for Postprocessors",
    "section": "Data for Training Postprocessors",
    "text": "Data for Training Postprocessors\nIn this situation, we have three data sets: training, validation, and testing. We’re using the training set to estimate model and preprocessor parameters, the validation set to compute metrics during model development, and a training set to be our final, one-time-only measure of final performance. We absolutely can’t use the testing set. The validation set is available but, if we use it to evaluated the efficacy of the calibration, we run the risk of getting optimistic results since we would be using the same data set directly for estimation and evaluation.\nOne slightly less risky option is to resample any out-of-sample predictions and use those to train the calibration. To demonstrate, let’s do an additional 10-fold cross-validation on our training set to have out-of-sample predictions:\n\nset.seed(104)\nresample_res &lt;- \n  bst_wflow %&gt;% \n  fit_resamples(\n    time_to_delivery ~ ., \n    resamples = vfold_cv(delivery_train),\n    control = control_resamples(save_workflow = TRUE, save_pred = TRUE)\n  )\n\nThe probably package has an API for using the out-of-sample predictions to estimate the calibration model and also to apply it to a set of raw predictions:\n\n# Estimate a linear trend using the holdout predictions in 'resample_res'\ncal_res &lt;- cal_estimate_linear(resample_res, smooth = FALSE)\n\n# Calibrate the existing validation set:\nbst_val_cal_pred &lt;- cal_apply(bst_val_pred, cal_res)\n\n# RMSE gets better!\nbst_val_cal_pred %&gt;% \n  reg_metrics(truth = time_to_delivery, estimate = .pred)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       2.71 \n2 rsq     standard       0.850\n\n# Reassess the results\ncal_plot_regression(bst_val_cal_pred, truth = time_to_delivery, estimate = .pred)\n\n\n\n\n\n\n\nNot perfect but a lot better.\n\n\n\n\n\n\nAn side:\n\n\n\nMy first job was in molecular diagnostics for infectious diseases. At the time, the statistics group produced algorithms for many of our instrumented diagnostics. After multiple clinical trials were used to collect training data, we reserved a specific set of samples to use for post-modeling activities such as threshold optimization and so on. The point is that if calibration or post-model optimization is important to you, you should reserve a specific group of samples and use them for this purpose (and this purpose only)."
  },
  {
    "objectID": "posts/data-usage-for-postprocessors/index.html#how-and-when-to-integrate-calibration-into-our-workflow",
    "href": "posts/data-usage-for-postprocessors/index.html#how-and-when-to-integrate-calibration-into-our-workflow",
    "title": "Data Usage for Postprocessors",
    "section": "How and When to Integrate Calibration into Our Workflow?",
    "text": "How and When to Integrate Calibration into Our Workflow?\nDuring model development, I don’t think I would estimate a calibration model until I have a good idea that it is needed. However, if it is needed, our software should be able to integrate it into our pipeline or workflow.\nThe problem is what data should we use and when? If we have an external set reserved for calibration or threshold optimization, we should base our results on those. If not, we could use the out-of-sample approach shown above. However, most software for resampling or tuning models doesn’t keep the out-of-sample predictions in a pool to be available during resampling/tuning. Here’s some pseduocode for the resampling process (which extends to tuning)\nfor each resample\n     determine the analysis and assessment data\n     fit the model to the analysis set\n     predict the assessment set\n     save performance metrics (and maybe predictions)\nend\naverage the metrics across replicates\nOften, the operations within the for loop are done in parallel, and to reduce computational overhead, the full set of results is not returned at any one time."
  },
  {
    "objectID": "posts/data-usage-for-postprocessors/index.html",
    "href": "posts/data-usage-for-postprocessors/index.html",
    "title": "Data Usage with Postprocessing",
    "section": "",
    "text": "This document is used to discuss and test ideas for how have can estimate and evaluate machine learning models that have three potential components:\nWe’ll call the combination of these three components the model pipeline1:\nThe pipeline includes the model in the least; the pre and post model operations are added as needed.\nWe want feedback, so if you like our recommendations or have alternatives, please comment below.\nLet’s take some time to describe what postprocessors can do."
  },
  {
    "objectID": "posts/data-usage-for-postprocessors/index.html#more-about-postprocessors",
    "href": "posts/data-usage-for-postprocessors/index.html#more-about-postprocessors",
    "title": "Data Usage with Postprocessing",
    "section": "More About Postprocessors",
    "text": "More About Postprocessors\nThe process of postprocessing the predicted values has not been thoroughly discussed, mostly because the software to operationalize the full pipeline process is not comprehensive.\nThe number of potential postprocessors is probably in single digits. Some examples:\n\nSpecifying an optimal probability cutoff (mentioned above).\nRestricting the range of possible predictions (e.g., greater than zero).\nSimple deterministic transformations (e.g., exponentiation of predicted values).\nDisqualifying predictions via an equivocal zone.\nPost-hoc nearest neighbor adjustments such as Quinlan (1993).\n\nCalibration.\n\nEach of these steps can involve tuning parameters that require optimization. For example, we can vary the cutoff value over a range for alternate probability cutoffs and measure the performance change in some statistic that uses the hard class predictions (such as accuracy or Kappa). These types of parameters are estimated indirectly via gris search or some other tuning parameter optimization routine. There is no analytical formula where we plug in our predictor and outcome data to produce a point estimate to plug into the postprocessor.\nHowever, the last two in the list above might also have parameter values that require direct estimation (akin to slope parameters in a linear regression). Of these, let’s focus on model calibration for more discussion.\nModel Calibration\nThis article focuses largely on model calibration. This technique is not particularly significant, but we use it because it is a well-known postprocessing technique that requires direct parameter estimation.\nCalibration is most often applied to classification models. To keep things relatively simple, we’ll demonstrate the problem with a regression model that predicts a numeric outcome.\nWe’ll use a regression data set as a demonstration2. The data were split into a training set (n = 6004), a validation set (n = 2004), and a test set (n = 2004). This data splitting scheme corresponds to Case 5 below. The outcome is numeric and there are 30 predictors.\nLet’s deliberately fit a regression model that has poor predicted values: a boosted tree with only five members. The validation set statistics show an RMSE of 4.71 (units are minutes) and a corresponding R2 of 85.4%. It’s hard to know if the RMSE is acceptable without context but the latter statistic seems pretty good.\nHowever, when we look at the data, the observed and predicted values are well correlated but very poorly calibrated:\n\n\n\n\n\n\n\n\nThe predicted values are not realistic in the sense that they are not consistent with the original data. They should fall along the diagonal green line if the model is performing well.\nThe solid blue curve above shows a linear regression line that is fit to these data. This measures the systematic miscalibration. If we were to use the validation set to quantify the pattern, the intercept was -39.2 minutes and the slope was 2.49 minutes. Now that we know this pattern, it might be possible to remove it from the predictions.\nThe problem, more broadly described below in Case 3, is that we can’t use the validation set to measure this pattern and assess its effectiveness.\nAs an alternative, we used a slightly risky approach described in Case 3 to estimate a slope of -38.7 minutes, and the slope was 2.48 minutes. When these values are used to correct the validation set predictions, RMSE decreases from 4.71 to 2.68 minutes. The R2 stays the same.\nThe plot of the calibrated predictions shows a moderately effective model that has much better calibration:\n\n\n\n\n\n\n\n\nOur Goals Here\nIn this article, we want to examine the nuances of data usage in situations where our pipeline is trained in two stages. To do this, we’ll show some diagrams that illustrate some different strategies. The discussion of different data spending schemes with an initial pool of 100 samples, assumed to be in a random order, is visualized using:\n\n\n\n\n\n\n\n\nLet’s start with the ordinary case where we use a simple two-way training/testing split of the data, then consider different analysis paths."
  },
  {
    "objectID": "posts/data-usage-for-postprocessors/index.html#initial-two-way-split",
    "href": "posts/data-usage-for-postprocessors/index.html#initial-two-way-split",
    "title": "Data Usage with Postprocessing",
    "section": "Initial Two-Way Split",
    "text": "Initial Two-Way Split\nThere are a few different scenarios to consider. The first two are very pedestrian and are only included to contrast with the more complex ones.\nCase 1: No Tuning, No Postprocessing Estimation\nThis is a simple case where a basic model will suffice with tuning parameters and no uncertainty about what predictors should be in the model.\n“No Postprocessing Estimation” means that there might be a postprocessor but it does not require any parameter estimation. For example, it might just change the probability cutoff for a binary classification to be something other than 50%.\nWe split the data into a larger set (training, in orange), and the remainder goes into testing (purple). Any 80/20 split is used to demonstrate:\n\n\n\n\n\n\n\n\nAll of our estimation tasks use the training set and the test set is evaluated only once to quantify the efficacy of the model.\nCase 2: Tuning, No Postprocessing Estimation\nHere, some aspects of the model, preprocessor, or postprocessor required optimization. Any postprocessor does not require estimation (but could require tuning).\nUsing the same initial split from Case 1, we might use some resampling strategy like cross-validation, the bootstrap, or a time-series resampling scheme. Without loss of generalization, we’ll show a 5-fold cross-validation diagram:\n\n\n\n\n\n\n\n\nAs usual, we fit five model pipelines with different tuning parameter candidates. Each model uses 4/5 of the data for estimation and the remaining 1/5 to measure performance. The resulting five performance statistics are averaged into a single value, which is used to guide the user in picking which tuning parameters are optimal (or at least reasonable).\nOnce the optimization phase is finished, the final model uses the optimized tuning parameter values and is fit to the 80 data points of the training set. The other 20 samples in the test set are used to verify performance.\n\n\n\n\n\n\nNote\n\n\n\nNote that we use specific terminology to distinguish between the data used for modeling and evaluation at the two different levels of data partitioning. The initial split creates training and test sets. During resampling, the analogs to these data sets are called the analysis and assessment sets.\n\n\nCase 3: No Tuning, Postprocessing Estimation\nHere our model pipeline requires no tuning but we do need to estimate parameters for our postprocessor.\nFor example, perhaps our ordinary least squares linear regression model has some systematic bias in the predictions (and we have to use this model). We could attach a linear calibrator to the model pipeline that estimates the slope and intercept of a line defined by the observed and predicted outcomes (as shown above).\nWe need data to estimate the slope and intercept. We should not touch the test set. Naively re-predicting the training set is a poor choice; for many black-box models, the fitted values will be unreasonably close to the true values. This means that the systematic bias that we are trying to remove will be less pronounced and the calibration may not help. It also leaves us with no other data to judge how well the model (and calibration) works without using the test set.\nOne possible approach is to resample the model (prior to the calibration set) using the approach in Case 2. This can produce the out-of-sample predictions that were used to produce the resampled performance statistic. These values are not overfit to the training data and should be a reasonable substrate to fit the calibration model. The main downside to this approach is that we are “double dipping” on the training set but using it to\n\nEstimate our model parameters, and\nEstimate the calibration parameters.\n\nThis raises the risk of overfitting and we don’t have a data set to check how well this works until the test set (which should be used to verify performance). This is the approach that was used in the regression calibration example above.\nAnother approach is to use a three-way split at the start instead of a basic training/test set. We could reserve some data strictly for calibration (assuming that we know that calibration is required).\nWe can allocate a small fraction of data for postprocessing estimation. A diagram of this is before with 60% used for training the preprocessor and supervised model, 20% for estimating the postprocessor, and 20% for testing3. In the diagram below, the two shades of brown are meant to reflect that these data are used for estimation and the purple data are used strictly for model evaluation.\n\n\n\n\n\n\n\n\nThe nomenclature is a little bit fuzzy here. For now, we’ll call the darker brown data the training set (no different than before), the purple data the standard test set, and the light brown data the “potato set”4\nThis extra set is a simple solution that avoids potential data leakage but is reducing the amount of data used to train the preprocessors and the supervised model.\nThe next use-case is for situations where the model needs to be resampled for tuning or just to get some estimate of model performance.\nCase 4: Tuning, Postprocessing Estimation\nNow our model and/or preprocessor have unknown parameters that need to be indirectly optimized via grid search, Bayesian optimization, or by some other means. The compare and choose between models, we require an out-of-sample performance estimate, just as in Case 2.\nThe difference here is the existence of a postprocessor that needs estimation.\nOnce we arrive at our final tuning parameter value(s), we still need to perform the “last fit” where we estimate all of the parameters for the entire model pipeline.\nLet’s say we use the three-way data splitting scheme shown above in Case 3. How do we resample the model? We suggest taking all data that are not used for the testing set as the substrate for resampling. Let’s again use 5-fold cross-validation to demonstrate. The 80 samples are allocated to one of five folds.\n\n\n\n\n\n\n\n\nFor the first iteration of cross-validation, we take out the first fold earmarked for performance estimation as the assessment set.\nOrdinarily, the other 4/5 would be used to estimate the preprocessor(s) and the model. However, doing so in this case would result in overly optimistic performance statistics; assuming we always retain 1/5 of this data for model assessment, using the remaining 4/5 to estimate the preprocessor(s) and model would require that we train the postprocessor on predictions generated on that same 4/5 of the fold. Instead, we allot some portion of that 4/5—say, 3/4 of it—to train the preprocessor(s) and model, then generate predictions on the held-out 1/4 from the trained preprocessor(s) and model, then use those predictions to train the postprocessor. Then, the trained preprocessor(s), model, and postprocessor generate predictions on the remaining 1/5 of the fold that are evaluated using performance metrics.\nWe can emulate the same procedure used in our initial three-way split by randomly5 selecting the same proportion of data to estimate the two estimation stages.\nVisually, the scheme for the first iteration of cross-validation is:\n\n\n\n\n\n\n\n\nIn this instance, five preprocessor/model fits are paired with five calibration models, and when combined in sequence, they produce five resampled performance statistics. This is a complete resampling of the process that avoids information leakage."
  },
  {
    "objectID": "posts/data-usage-for-postprocessors/index.html#initial-three-way-split",
    "href": "posts/data-usage-for-postprocessors/index.html#initial-three-way-split",
    "title": "Data Usage with Postprocessing",
    "section": "Initial Three-Way Split",
    "text": "Initial Three-Way Split\nIf we have a lot of data, we might choose to start with three partitions of our data pool. The standard training and test sets would be supplemented by a validation set. This serves as a separate group of points that are used to judge model effectiveness during model development. While we should only look at the test set once, the validation set is designed to be evaluated multiple times so that we can compare and/or optimize our pipeline. Here’s a view of a 65/15/20 split:\n\n\n\n\n\n\n\n\nDuring model development, we can view the validation set as if it were a single iteration of resampling. Let’s look at that first.\nCase 5: No Postprocessing Estimation\nDuring model development, we would train our pipeline on the 65% of the data reserved for that purpose. Assuming that we need to tune or compare models, we would predict the 15% validation set and compute performance statistics.\nEventually, we determine the best model pipeline to finalize. At this point, we already have our model fit since we have been directly training our pipeline on the training set6. From here, we predict the test set and verify that our validation set statistics were accurate.\nCase 6: Postprocessing Estimation\nThe situation here is basically the same as the Case 3; we don’t want to re-use our training set to fit the postprocessor. As with Case 3, we advocate for splitting off a fourth partition of data for that purpose. Here is an example of a 50/15/15/20 split:\n\n\n\n\n\n\n\n\nDuring model development,\n\nThe preprocessor and supervised model are training on the training set (50% here)\nThe predictions on the potato set (15%) are used as inputs to estimate the postprocessing parameters.\nThe completely fitted pipeline predicts the validation set (15%), and those performance statistics guide our decision to finalize the pipeline.\nThe test set (20%) is evaluated once for verification of performance.\n\nThat is a lot of splitting but our pipeline is pretty complex. One might think that we are significantly reducing the amount of data used to fit the supervised models. That’s correct, and hopefully, it emphasizes that as the complexity of our pipeline increases, our data usage strategy needs to be more extensive to avoid subtle overfitting errors."
  },
  {
    "objectID": "posts/data-usage-for-postprocessors/index.html#conclusion",
    "href": "posts/data-usage-for-postprocessors/index.html#conclusion",
    "title": "Data Usage with Postprocessing",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve described how postprocessing operations can fit into the machine learning pipeline and the complications that occur in terms of data usage. From the viewpoint of the data, there are two main phases of pipeline training:\n\n\nStage 1: Use a single set of data to train the preprocessors (if any) and the supervised model.\n\nThe inputs are predictors and outcome data.\nOutputs are model predictions.\n\n\n\nStage 2: Use a separate set of training data to estimate any postprocessors (if any).\n\nInputs are model predictions.\nOutputs are modified model predictions.\n\n\n\nWe recommend using two different partitions for each of these two stages.\nThaks to the tidymodels group for review of this post."
  },
  {
    "objectID": "posts/data-usage-for-postprocessors/index.html#footnotes",
    "href": "posts/data-usage-for-postprocessors/index.html#footnotes",
    "title": "Data Usage with Postprocessing",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis document is intended to be independent of implementations. The phrase model pipeline is based on sci-kit learn’s object type. tidymodels calls it a model workflow and only uses different terminology to avoid confusion with the magrittr pipeline.↩︎\nThese data are originally from the Whole Game chapter.↩︎\nAgain, we don’t have to do this for all postprocessors, just those that require parameters to be estimated↩︎\nObviously, this is not going to be the real name. We need a placeholder until we come up with something that we all like. Potential candidate names are the “reserved data,” auxiliary data,” and “supplemental data. Why potato? Because it is easy to find/replace and you will probably remember it.”↩︎\nMost of the time, this will be done via random sampling. For time-series data, we would emulate the same non-random splitting strategy that does not break the correlation structure of the data. Also, if we are bootstrapping, the proportional splits are conducted on the distinct rows of the non-test data to avoid having some replicates of specific rows falling in both partitions of the data.↩︎\nSince the validation set is no longer required, some people might pool the training and validation set and fit the finalized pipeline on those data (80% in total in our example). I don’t think that it’s a good idea but it does happen.↩︎"
  }
]